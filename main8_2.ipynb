{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FNN : https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "\n",
    "\n",
    "class Net42(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net42, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = sigmoid_func_uniq(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return (x)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        denom = ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        if torch.isinf(denom).sum().item()>0:\n",
    "              output_tensor = input_tensor / torch.sqrt(squared_norm)\n",
    "        else:\n",
    "              output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "    \n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    \n",
    "#https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/    \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.input_d = input_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_d)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class NeurNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(NeurNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,sigmoid = F.log_softmax):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        return sigmoid(x)\n",
    "    \n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, cols, size_hidden, n_output):\n",
    "        super(NetG, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 512*32*2\n",
    "TEST_SIZE = 256*32\n",
    "\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "def test(model, func, data_loader, clas):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            if(clas == 1): #classification\n",
    "                # We set this just for the example to run quickly.\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "            if(clas == 2): #regression\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                from torch.autograd import Variable\n",
    "\n",
    "                X = Variable(torch.FloatTensor(data)) \n",
    "                result = model(X)\n",
    "                pred=result.data[:,0].numpy()\n",
    "                out = target.data[:,0].numpy()\n",
    "                #print( pred)\n",
    "                #print(out)\n",
    "                #pred.fillna(X_train.mean(), inplace=True)\n",
    "                total += target.size(0)\n",
    "                #correct += r2_score(pred,out)\n",
    "                correct+=func(result,target).numpy()   \n",
    "            if(clas == 3): #RNN\n",
    "                val_h = model.init_hidden(50) #batch size\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                #if(train_on_gpu): FALSE\n",
    "                #    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output, val_h = model(data, val_h)\n",
    "                val_loss = func(output.squeeze(), target.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "               # model.train()\n",
    "                correct = np.mean(val_losses)\n",
    "                total = 1;\n",
    "                \n",
    "    return correct / total\n",
    "\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        model = NeurNet(784,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "\n",
    "   # optimizer = optim.SGD(    model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader)\n",
    "        acc = test(model, F.nll_loss, test_loader,1)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision                                                       \n",
    "import torchvision.transforms as transforms\n",
    "#https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html\n",
    "import os.path        \n",
    "\n",
    "def train_fashion_mnist(config):\n",
    "    from keras.datasets import fashion_mnist\n",
    "    dataset_dir = os.path.join(os.path.expanduser(\"~\"), 'Datasets', 'FashionMNIST')\n",
    "    valid_ratio = 0.2  # Going to use 80%/20% split for train/valid\n",
    "\n",
    "    # Load the dataset for the training/validation sets\n",
    "    train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                           train=True,\n",
    "                                           transform= None, #transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "    # Split it into training and validation sets\n",
    "    nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "    nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "\n",
    "\n",
    "    # Load the test set\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                                     transform= None, #transforms.ToTensor(),\n",
    "                                                    train=False)\n",
    "    \n",
    "    class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, base_dataset, transform):\n",
    "            self.base_dataset = base_dataset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img, target = self.base_dataset[index]\n",
    "            return self.transform(img), target\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "\n",
    "\n",
    "    train_dataset = DatasetTransformer(train_dataset, transforms.ToTensor())\n",
    "    valid_dataset = DatasetTransformer(valid_dataset, transforms.ToTensor())\n",
    "    test_dataset  = DatasetTransformer(test_dataset , transforms.ToTensor())\n",
    "    ############################################################################################ Dataloaders\n",
    "    num_threads = 4     # Loading the dataset is using 4 CPU threads\n",
    "    batch_size  = 512*8   # Using minibatches of 128 samples\n",
    "\n",
    "    train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,                # <-- this reshuffles the data at every epoch\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    valid_loader1 = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "\n",
    "    test_loader1 = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        model = NeurNet(784,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "        \n",
    "   # optimizer = optim.SGD(    model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader1)\n",
    "        acc = test(model, F.nll_loss, test_loader1,1)\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_TREC(config):\n",
    "    import torch\n",
    "    from torchtext import data\n",
    "    from torchtext import datasets\n",
    "    import random\n",
    "\n",
    "    SEED = 1234\n",
    "    savedPath = os.getcwd()\n",
    "    os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "    \n",
    "    \n",
    "    #torch.manual_seed(SEED)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    TEXT = data.Field(tokenize = 'spacy')\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    \n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.100d', \n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    os.chdir(savedPath)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                     dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            self.convs = nn.ModuleList([\n",
    "                                        nn.Conv2d(in_channels = 1, \n",
    "                                                  out_channels = n_filters, \n",
    "                                                  kernel_size = (fs, embedding_dim)) \n",
    "                                        for fs in filter_sizes\n",
    "                                        ])\n",
    "\n",
    "            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text):\n",
    "\n",
    "            #text = [sent len, batch size]\n",
    "\n",
    "            text = text.permute(1, 0)\n",
    "\n",
    "            #text = [batch size, sent len]\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "            conved = [sigmoid_func_uniq(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "            #pooled_n = [batch size, n_filters]\n",
    "\n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "            return self.fc(cat)\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    " #   print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    def categorical_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text)\n",
    "\n",
    "                loss = criterion(predictions, batch.label)\n",
    "\n",
    "                acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    import time\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for e in range(ITERATIONS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        tune.report(loss=valid_acc)\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    " #       print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "  #      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_diabetes(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "\n",
    "    from sklearn.datasets import load_diabetes\n",
    "\n",
    "    (X,Y) = load_diabetes( return_X_y=True, as_frame=True)\n",
    "    X = pd.DataFrame(X)\n",
    "    Y = pd.DataFrame(Y)\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1234)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    #print(y_train)\n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "\n",
    "    net = NeurNet(10,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    \n",
    "    net = LogReg(10,1)\n",
    "        \n",
    "   # optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "        \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(ITERATIONS):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        acc = test(net,loss_func ,test_loader, 2)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_boston(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    data = datasets.load_boston()\n",
    "\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    Y = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "    print(sigmoid_func_uniq)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1234)\n",
    "    X_test,X_last,y_test,y_last = train_test_split(X_test, y_test, test_size=0.50, random_state=1234)\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "    x_last = torch.tensor(X_last.values, dtype=torch.float)\n",
    "    y_last = torch.tensor(y_last.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    \n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "       \n",
    "    last_datasets = torch.utils.data.TensorDataset(x_last, y_last)\n",
    "    last_loader = torch.utils.data.DataLoader(last_datasets, batch_size=100, shuffle=True)  \n",
    "    \n",
    "    \n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        net = NeurNet(13,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        net = LogReg(13,1)\n",
    "        \n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "        #                         betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "         #                                eps=config.get(\"eps\", 1e-08),\n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "         #                        betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "          #                               eps=config.get(\"eps\", 1e-08),\n",
    "                                                momentum=config.get(\"sigmoid_func\", 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(ITERATIONS):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        acc = test(net,loss_func ,test_loader, 2)\n",
    "        test1= test(net,loss_func ,last_loader, 2)\n",
    "        # Send the current training result back to Tune\n",
    "        \n",
    "        tune.report(loss=acc,mean_accuracy=test1)\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "        \n",
    "    class SentimentRNN(nn.Module):\n",
    "        def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                 drop_prob, sigmoid , vocab_size):\n",
    "            super(SentimentRNN, self).__init__()\n",
    "            self.output_size = output_size\n",
    "            self.n_layers = n_layers\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                                dropout=drop_prob, batch_first=True)\n",
    "            #self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "            self.sig = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            hidden= tuple([each.data for each in hidden])\n",
    "            batch_size = x.size(0)\n",
    "            embeds = self.embedding(x)\n",
    "            lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "            # stack up lstm outputs\n",
    "            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "            # dropout and fully connected layer\n",
    "            out = self.dropout(lstm_out)\n",
    "            out = self.fc(out)\n",
    "\n",
    "            # sigmoid function\n",
    "            sig_out = self.sig(out)\n",
    "\n",
    "            # reshape to be batch_size first\n",
    "            sig_out = sig_out.view(batch_size, -1)\n",
    "            sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            return hidden\n",
    "        \n",
    "\n",
    "    class SentimentRNN1(nn.Module):\n",
    "        def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                 drop_prob, sigmoid , vocab_size):\n",
    "            super(SentimentRNN1, self).__init__()\n",
    "            self.output_size = output_size\n",
    "            self.n_layers = n_layers\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            #    dropout=drop_prob, batch_first=True)\n",
    "            self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "            self.sig = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            hidden = hidden.data\n",
    "            batch_size = x.size(0)\n",
    "            embeds = self.embedding(x)\n",
    "            lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "            # stack up lstm outputs\n",
    "            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "            # dropout and fully connected layer\n",
    "            out = self.dropout(lstm_out)\n",
    "            out = self.fc(out)\n",
    "\n",
    "            # sigmoid function\n",
    "            sig_out = self.sig(out)\n",
    "\n",
    "            # reshape to be batch_size first\n",
    "            sig_out = sig_out.view(batch_size, -1)\n",
    "            sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            (a,b) = hidden\n",
    "            return a\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = 400 \n",
    "    hidden_dim = int(round(config.get(\"hidden_dim\",64)))\n",
    "    n_layers =  2+ int( round(config.get(\"n_layer\",1)))\n",
    "\n",
    "\n",
    "    \n",
    "    net = SentimentRNN1(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)    \n",
    "    net = SentimentRNN(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)\n",
    "    \n",
    "    # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "   # print(*(n for n in net.parameters()))\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # training params\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 128*32\n",
    "    TEST_SIZE = 64*32\n",
    "\n",
    "    def train_rnn():\n",
    "\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(inputs) > EPOCH_SIZE:\n",
    "                return\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step() \n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        train_rnn()\n",
    "        acc = test(net,criterion,valid_loader,3)\n",
    "\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab:Loading vectors from .vector_cache/glove.6B.100d.txt.pt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Example' object has no attribute 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e1b1a3ff99fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msave_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-fe2617ca8a94>\u001b[0m in \u001b[0;36msave_examples\u001b[0;34m(dataset, savepath)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Save elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Write samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'src'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "savedPath = os.getcwd()\n",
    "os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "\n",
    "\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = 'glove.6B.100d', \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "import dill\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "def save_dataset(dataset, path):\n",
    "    if not isinstance(path, Path):\n",
    "        path = Path(path)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(dataset.examples, path/\"examples.pkl\", pickle_module=dill)\n",
    "    torch.save(dataset.fields, path/\"fields.pkl\", pickle_module=dill)\n",
    "\n",
    "def load_dataset(path):\n",
    "    if not isinstance(path, Path):\n",
    "        path = Path(path)\n",
    "    examples = torch.load(path/\"examples.pkl\", pickle_module=dill)\n",
    "    fields = torch.load(path/\"fields.pkl\", pickle_module=dill)\n",
    "    return Dataset(examples, fields)\n",
    "\n",
    "\n",
    "save_examples(train_data,'sdf')\n",
    "examples = load_examples(filename)\n",
    "examples = [data.Example().fromlist(d, fields) for d in examples]\n",
    "\n",
    "# Build dataset\n",
    "mydataset = Dataset(examples, fields)\n",
    "\n",
    "os.chdir(savedPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_TREC(config):\n",
    "    import torch\n",
    "    from torchtext import data\n",
    "    from torchtext import datasets\n",
    "    import random\n",
    "\n",
    "    SEED = 1234\n",
    "    savedPath = os.getcwd()\n",
    "    os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "    \n",
    "    \n",
    "    #torch.manual_seed(SEED)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    TEXT = data.Field(tokenize = 'spacy')\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    \n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.100d', \n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    os.chdir(savedPath)\n",
    "    \n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                     dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            self.convs = nn.ModuleList([\n",
    "                                        nn.Conv2d(in_channels = 1, \n",
    "                                                  out_channels = n_filters, \n",
    "                                                  kernel_size = (fs, embedding_dim)) \n",
    "                                        for fs in filter_sizes\n",
    "                                        ])\n",
    "\n",
    "            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text):\n",
    "\n",
    "            #text = [sent len, batch size]\n",
    "\n",
    "            text = text.permute(1, 0)\n",
    "\n",
    "            #text = [batch size, sent len]\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "            conved = [sigmoid_func_uniq(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "            #pooled_n = [batch size, n_filters]\n",
    "\n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "            return self.fc(cat)\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    " #   print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    def categorical_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text)\n",
    "\n",
    "                loss = criterion(predictions, batch.label)\n",
    "\n",
    "                acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    import time\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for e in range(ITERATIONS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        tune.report(mean_accuracy=valid_acc)\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    " #       print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "  #      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2811, 0.5970])\n"
     ]
    }
   ],
   "source": [
    "#Configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "        \n",
    "    \n",
    "    \n",
    "experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "#experiment_metrics = dict(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "ITERATIONS = 1\n",
    "NUM_TUNED= 1\n",
    "    \n",
    "\n",
    "\n",
    "#[nn.ReLU, nn.Softmax(), nn.Tanh(),nn.Sigmoid() ]\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if True else 2,\n",
    "    \"config\": {\n",
    "    \"steps\": 3,  # evaluation times\n",
    "     \"lr\":  tune.quniform(1e-10, 0.1,1e-10),\n",
    "    \"b1\": tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"b2\":tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"eps\": tune.uniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.quniform(1e-10, 0.1,1e-10),\n",
    "        \"sigmoid_func\":nn.ReLU()\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-10, 0.1),\n",
    "    \"b1\": tune.loguniform(0.9, 1-1e-10),\n",
    "        \"b2\":tune.loguniform(0.9, 1-1e-10),\n",
    "        \"eps\": tune.loguniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "}\n",
    "   \n",
    "#i is in [0;1]\n",
    "#We want all values between 0 and 1\n",
    "def get_sigmoid_func(i):\n",
    "    if(i<0.33):\n",
    "        return nn.ReLU()\n",
    "    elif(i<0.67):\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "    \n",
    "optimizer_is_adam = True   \n",
    "    \n",
    "f = get_sigmoid_func(3)\n",
    "print(f(torch.randn(2)))\n",
    "import random\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.loguniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.loguniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects<br>Result logdir: /home/antoine/ray_results/hyper<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">    model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  steps</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_TREC_2fb91f8c</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.837611</td><td style=\"text-align: right;\">       0.485956</td><td style=\"text-align: right;\">     61.8488</td><td style=\"text-align: right;\">0.00452707</td><td style=\"text-align: right;\">0.0566794</td><td style=\"text-align: right;\">  2.48344</td><td style=\"text-align: right;\">      0.706041</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     0.0301386</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m /home/antoine/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m /home/antoine/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m /home/antoine/anaconda3/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m <generator object Dataset.__getattr__ at 0x7fcef4279150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 16:47:27,623\tERROR trial_runner.py:567 -- Trial train_TREC_2fb91f8c: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=28808, ip=192.168.1.34)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 459, in _report_thread_runner_error\n",
      "    .format(err_tb_str)))\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=28808, ip=192.168.1.34)\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 290, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-52-5bd636ffe49d>\", line 41, in train_TREC\n",
      "  File \"<__array_function__ internals>\", line 6, in save\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 527, in save\n",
      "    arr = np.asanyarray(arr)\n",
      "  File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 136, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "ValueError: invalid __array_struct__\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m <generator object Dataset.__getattr__ at 0x7fcef4279150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m 2020-11-17 16:47:27,586\tERROR function_runner.py:233 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 290, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     output = train_func(config)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"<ipython-input-52-5bd636ffe49d>\", line 41, in train_TREC\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"<__array_function__ internals>\", line 6, in save\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 527, in save\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     arr = np.asanyarray(arr)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 136, in asanyarray\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     return array(a, dtype, copy=False, order=order, subok=True)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m ValueError: invalid __array_struct__\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 246, in run\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 290, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     output = train_func(config)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"<ipython-input-52-5bd636ffe49d>\", line 41, in train_TREC\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"<__array_function__ internals>\", line 6, in save\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 527, in save\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     arr = np.asanyarray(arr)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m   File \"/home/antoine/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 136, in asanyarray\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m     return array(a, dtype, copy=False, order=order, subok=True)\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m ValueError: invalid __array_struct__\n",
      "\u001b[2m\u001b[36m(pid=28808)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects<br>Result logdir: /home/antoine/ray_results/hyper<br>Number of trials: 1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">    model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  steps</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_TREC_2fb91f8c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.837611</td><td style=\"text-align: right;\">       0.485956</td><td style=\"text-align: right;\">     61.8488</td><td style=\"text-align: right;\">0.00452707</td><td style=\"text-align: right;\">0.0566794</td><td style=\"text-align: right;\">  2.48344</td><td style=\"text-align: right;\">      0.706041</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     0.0301386</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_TREC_2fb91f8c</td><td style=\"text-align: right;\">           1</td><td>/home/antoine/ray_results/hyper/train_TREC_2fb91f8c_1_adam=0.83761,droupout_prob=0.48596,hidden_dim=61.849,lr=0.0045271,model=0.056679,n_layer=2.4834,sigmoid_func_2020-11-17_16-47-24/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_TREC_2fb91f8c])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b7996ede9b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#One shot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf_HyperOpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_TREC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-da598c8e8335>\u001b[0m in \u001b[0;36mf_HyperOpt\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncHyperBandScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexperiment_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbayesopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHyperOptSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexperiment_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtune_kwargs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hyper\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_TREC_2fb91f8c])"
     ]
    }
   ],
   "source": [
    "#One shot\n",
    "\n",
    "f_HyperOpt(train_TREC)\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "for i in range(0,0):\n",
    "    for j in range(0,1):\n",
    "        x = x_all[i]\n",
    "        f_HyperOpt(x)\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small budget\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 20\n",
    "\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "optimizer_is_adam = True\n",
    "if(0==1):\n",
    "    for i in range(1,2):\n",
    "        x = train_TREC\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_HyperOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = HyperOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler,  name=\"hyper\", search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BayesOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = BayesOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler, name=\"bayes\",  search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_AX(dataset):\n",
    "    \n",
    "   \n",
    "    if __name__ == \"__main__\":\n",
    "                \n",
    "        algo = AxSearch(\n",
    "            max_concurrent=2, #was working with 2\n",
    "            **experiment_metrics\n",
    "        )\n",
    "        scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "        tune.run(\n",
    "            dataset,       name=\"ax\",\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            **tune_kwargs)\n",
    "\n",
    "        \n",
    "#        algo = AxSearch(\n",
    "#            **experiment_metrics\n",
    "#        )\n",
    "#        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        \n",
    "#        scheduler = AsyncHyperBandScheduler()\n",
    "#        tune.run(\n",
    "#            dataset,\n",
    "#            **experiment_metrics,\n",
    "#            search_alg=algo,\n",
    "#            scheduler=scheduler,\n",
    "#            **tune_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO We are interested in multiple Population based algorithms from nevergrad, and certainly not in OnePlusOne. \n",
    "def f_NeverGrad(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.CMA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"ng\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BOHB(dataset):\n",
    "\n",
    "    bohb_hyperband = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=100,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    bohb_search = TuneBOHB(\n",
    "        # space=config_space, \n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "       # config=config, \n",
    "        scheduler=bohb_hyperband,name=\"bohb\",\n",
    "        search_alg=bohb_search,       \n",
    "         **tune_kwargs)\n",
    "        #num_samples=NUM_TUNED,\n",
    "       # stop={\"training_iteration\": 100})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Random(dataset):\n",
    "    \n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo,   name=\"random\",    \n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ZOOpt(dataset):\n",
    "\n",
    "    dim_dict = {\n",
    "        \"lr\": (ValueType.CONTINUOUS, [0, 1], 1e-2),\n",
    "        \"momentum\": (ValueType.CONTINUOUS, [0,1, 0.9], 1e-2)\n",
    "    }\n",
    "\n",
    "    zoopt_search_config = {\n",
    "        \"parallel_num\": 8,  # how many workers to parallel\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    zoopt_search = ZOOptSearch(\n",
    "    algo=\"Asracos\",  # only support Asracos currently\n",
    "    #dim_dict=dim_dict,\n",
    "    budget=ITERATIONS,\n",
    "    #dim_dict=dim_dict,\n",
    "   #     **zoopt_search_config,\n",
    "    **experiment_metrics)\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "\n",
    "   \n",
    "    tune.run(dataset,\n",
    " #        config = config,\n",
    "    search_alg=zoopt_search,\n",
    "   # num_samples= ITERATIONS,\n",
    "    scheduler=scheduler,       \n",
    "    #         paralell_num=4,\n",
    "    name=\"zoopt_search\", \n",
    "              **tune_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "\n",
    "def GAN_MNIST(SA):\n",
    "    import ray\n",
    "\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.utils.data\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "    import torchvision.utils as vutils\n",
    "    import numpy as np\n",
    "\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.trial import ExportFormat\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    import argparse\n",
    "    import os\n",
    "    from filelock import FileLock\n",
    "    import random\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data\n",
    "    import numpy as np\n",
    "    from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "    from ray.tune.suggest.ax import AxSearch\n",
    "\n",
    "\n",
    "\n",
    "    from torch.autograd import Variable\n",
    "    from torch.nn import functional as F\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "    # Training parameters\n",
    "    dataroot = ray.utils.get_user_temp_dir() + os.sep\n",
    "    workers = 2\n",
    "    batch_size = 64\n",
    "    image_size = 32\n",
    "\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 32\n",
    "\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 32\n",
    "\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # iterations of actual training in each Trainable _train\n",
    "    train_iterations_per_step = 5\n",
    "\n",
    "    MODEL_PATH = os.path.expanduser(\"~/.ray/models/mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "    def get_data_loader():\n",
    "        dataset = dset.MNIST(\n",
    "            root=dataroot,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, ), (0.5, )),\n",
    "            ]))\n",
    "\n",
    "        # Create the dataloader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "    # __GANmodel_begin__\n",
    "    # custom weights initialization called on netG and netD\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find(\"Conv\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find(\"BatchNorm\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "    # Generator Code\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    # __GANmodel_end__\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_begin__\n",
    "    class Net(nn.Module):\n",
    "        \"\"\"\n",
    "        LeNet for MNist classification, used for inception_score\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def inception_score(imgs, mnist_model_ref, batch_size=32, splits=1):\n",
    "        N = len(imgs)\n",
    "        dtype = torch.FloatTensor\n",
    "        dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "        cm = ray.get(mnist_model_ref)  # Get the mnist model from Ray object store.\n",
    "        up = nn.Upsample(size=(28, 28), mode=\"bilinear\").type(dtype)\n",
    "\n",
    "        def get_pred(x):\n",
    "            x = up(x)\n",
    "            x = cm(x)\n",
    "            return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "        preds = np.zeros((N, 10))\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            batch = batch.type(dtype)\n",
    "            batchv = Variable(batch)\n",
    "            batch_size_i = batch.size()[0]\n",
    "            preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "        # Now compute the mean kl-div\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (N // splits):(k + 1) * (N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "        return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_end__\n",
    "\n",
    "\n",
    "    def train(netD, netG, optimG, optimD, criterion, dataloader, iteration, device,\n",
    "              mnist_model_ref):\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            if i >= train_iterations_per_step:\n",
    "                break\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full(\n",
    "                (b_size, ), real_label, dtype=torch.float, device=device)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimG.step()\n",
    "\n",
    "            is_score, is_std = inception_score(fake, mnist_model_ref)\n",
    "\n",
    "            # Output training stats\n",
    "            if iteration % 10 == 0:\n",
    "                print(\"[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z))\"\n",
    "                      \": %.4f / %.4f \\tInception score: %.4f\" %\n",
    "                      (iteration, len(dataloader), errD.item(), errG.item(), D_x,\n",
    "                       D_G_z1, D_G_z2, is_score))\n",
    "\n",
    "        return errG.item(), errD.item(), is_score\n",
    "\n",
    "\n",
    "    def plot_images(dataloader):\n",
    "        # Plot some training images\n",
    "        real_batch = next(iter(dataloader))\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Original Images\")\n",
    "        plt.imshow(\n",
    "            np.transpose(\n",
    "                vutils.make_grid(real_batch[0][:64], padding=2,\n",
    "                                 normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def demo_gan(checkpoint_paths):\n",
    "        img_list = []\n",
    "        fixed_noise = torch.randn(64, nz, 1, 1)\n",
    "        for netG_path in checkpoint_paths:\n",
    "            loadedG = Generator()\n",
    "            loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])\n",
    "            with torch.no_grad():\n",
    "                fake = loadedG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)]\n",
    "               for i in img_list]\n",
    "        ani = animation.ArtistAnimation(\n",
    "            fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "        ani.save(\"./generated.gif\", writer=\"imagemagick\", dpi=72)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # __Trainable_begin__\n",
    "    class PytorchTrainable(tune.Trainable):\n",
    "        def setup(self, config):\n",
    "            use_cuda = config.get(\"use_gpu\") and torch.cuda.is_available()\n",
    "            self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            self.netD = Discriminator().to(self.device)\n",
    "            self.netD.apply(weights_init)\n",
    "            self.netG = Generator().to(self.device)\n",
    "            self.netG.apply(weights_init)\n",
    "            self.criterion = nn.BCELoss()\n",
    "            self.optimizerD = optim.Adam(\n",
    "                self.netD.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            self.optimizerG = optim.Adam(\n",
    "                self.netG.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "                self.dataloader = get_data_loader()\n",
    "            self.mnist_model_ref = c[\"mnist_model_ref\"]\n",
    "\n",
    "        def step(self):\n",
    "            lossG, lossD, is_score = train(self.netD, self.netG, self.optimizerG,\n",
    "                                           self.optimizerD, self.criterion,\n",
    "                                           self.dataloader, self._iteration,\n",
    "                                           self.device, self.mnist_model_ref)\n",
    "            return {\"lossg\": lossG, \"lossd\": lossD, \"is_score\": is_score}\n",
    "\n",
    "        def save_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\n",
    "                \"netDmodel\": self.netD.state_dict(),\n",
    "                \"netGmodel\": self.netG.state_dict(),\n",
    "                \"optimD\": self.optimizerD.state_dict(),\n",
    "                \"optimG\": self.optimizerG.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "            return checkpoint_dir\n",
    "\n",
    "        def load_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            checkpoint = torch.load(path)\n",
    "            self.netD.load_state_dict(checkpoint[\"netDmodel\"])\n",
    "            self.netG.load_state_dict(checkpoint[\"netGmodel\"])\n",
    "            self.optimizerD.load_state_dict(checkpoint[\"optimD\"])\n",
    "            self.optimizerG.load_state_dict(checkpoint[\"optimG\"])\n",
    "\n",
    "        def reset_config(self, new_config):\n",
    "            if \"netD_lr\" in new_config:\n",
    "                for param_group in self.optimizerD.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netD_lr\"]\n",
    "            if \"netG_lr\" in new_config:\n",
    "                for param_group in self.optimizerG.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netG_lr\"]\n",
    "\n",
    "            self.config = new_config\n",
    "            return True\n",
    "\n",
    "        def _export_model(self, export_formats, export_dir):\n",
    "            if export_formats == [ExportFormat.MODEL]:\n",
    "                path = os.path.join(export_dir, \"exported_models\")\n",
    "                torch.save({\n",
    "                    \"netDmodel\": self.netD.state_dict(),\n",
    "                    \"netGmodel\": self.netG.state_dict()\n",
    "                }, path)\n",
    "                return {ExportFormat.MODEL: path}\n",
    "            else:\n",
    "                raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "\n",
    "\n",
    "    import urllib.request\n",
    "    # Download a pre-trained MNIST model for inception score calculation.\n",
    "    # This is a tiny model (<100kb).\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"downloading model\")\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://github.com/ray-project/ray/raw/master/python/ray/tune/\"\n",
    "            \"examples/pbt_dcgan_mnist/mnist_cnn.pt\", MODEL_PATH)\n",
    "\n",
    "    dataloader = get_data_loader()\n",
    "    if not args.smoke_test:\n",
    "        plot_images(dataloader)\n",
    "\n",
    "    # load the pretrained mnist classification model for inception_score\n",
    "    mnist_cnn = Net()\n",
    "    mnist_cnn.load_state_dict(torch.load(MODEL_PATH))\n",
    "    mnist_cnn.eval()\n",
    "    mnist_model_ref = ray.put(mnist_cnn)\n",
    "\n",
    "    # __tune_begin__\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"is_score\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"netG_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "            \"netD_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "        })\n",
    "\n",
    "\n",
    "    experiment_metrics= dict(metric=\"is_score\",\n",
    "        mode=\"max\")\n",
    "\n",
    "   \n",
    "    dim_dict = {\n",
    "        \"netG_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2),\n",
    "        \"netD_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2)\n",
    "    }\n",
    "\n",
    "    config =     {\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "           \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        }\n",
    "\n",
    "    algo = NevergradSearch(optimizer=ng.optimizers.OnePlusOne)\n",
    "\n",
    "    \n",
    "    tune_iter = 5 if args.smoke_test else 1\n",
    "    c={\"mnist_model_ref\" : mnist_model_ref}\n",
    "    \n",
    "    \n",
    "    if(SA==1):\n",
    "        algo =   BayesOptSearch(**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "    if(SA==2):\n",
    "        algo =  AxSearch(\n",
    "            **experiment_metrics) \n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-3, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-3, 0.1)\n",
    "\n",
    "\n",
    "        })\n",
    "        \n",
    "        \n",
    "    if(SA==3):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.OnePlusOne,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "     \n",
    "    if(SA==4):\n",
    "        bohb_hyperband = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=100,\n",
    "            reduction_factor=4,\n",
    "            **experiment_metrics)\n",
    "\n",
    "        bohb_search = TuneBOHB(\n",
    "            # space=config_space, \n",
    "            max_concurrent=4,\n",
    "            **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=bohb_hyperband,\n",
    "        reuse_actors=True,\n",
    "        search_alg=bohb_search,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "    if(SA==5):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "\n",
    "        \n",
    "    if(SA==6):\n",
    "        algo=  ZOOptSearch(\n",
    "                algo=\"Asracos\",  # only support Asracos currently\n",
    "                dim_dict=dim_dict,\n",
    "                budget=10,\n",
    "                #dim_dict=dim_dict,\n",
    "                **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "            PytorchTrainable,\n",
    "            name=\"pbt_dcgan_mnist\",\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            search_alg=algo,\n",
    "            verbose=1,\n",
    "            checkpoint_at_end=True,\n",
    "            stop={\n",
    "                \"training_iteration\": tune_iter,\n",
    "            },\n",
    "            num_samples=8,\n",
    "            export_formats=[ExportFormat.MODEL],\n",
    "            config=dim_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#Classification:\n",
    "\n",
    "metrics.accuracy_score()\n",
    "metrics.f1_score\n",
    "metrics.log_loss\n",
    "metrics.precision_score\n",
    "metrics.recall_score\n",
    "\n",
    "#Regression\n",
    "mean_absolute_error\n",
    "mean_squared_error\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'demo_gan' from 'common' (/home/antoine/anaconda3/lib/python3.7/site-packages/common/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e5111971917a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdemo_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'demo_gan' from 'common' (/home/antoine/anaconda3/lib/python3.7/site-packages/common/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Example of training DCGAN on MNIST using PBT with Tune's function API.\n",
    "\"\"\"\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from filelock import FileLock\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "from common import demo_gan, get_data_loader, plot_images, train, weights_init\n",
    "from common import Discriminator, Generator, Net\n",
    "\n",
    "\n",
    "# __Train_begin__\n",
    "def dcgan_train(config, checkpoint_dir=None):\n",
    "    step = 0\n",
    "    use_cuda = config.get(\"use_gpu\") and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    netD = Discriminator().to(device)\n",
    "    netD.apply(weights_init)\n",
    "    netG = Generator().to(device)\n",
    "    netG.apply(weights_init)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(\n",
    "        netD.parameters(), lr=config.get(\"lr\", 0.01), betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(\n",
    "        netG.parameters(), lr=config.get(\"lr\", 0.01), betas=(beta1, 0.999))\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        dataloader = get_data_loader()\n",
    "\n",
    "    if checkpoint_dir is not None:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        netD.load_state_dict(checkpoint[\"netDmodel\"])\n",
    "        netG.load_state_dict(checkpoint[\"netGmodel\"])\n",
    "        optimizerD.load_state_dict(checkpoint[\"optimD\"])\n",
    "        optimizerG.load_state_dict(checkpoint[\"optimG\"])\n",
    "        step = checkpoint[\"step\"]\n",
    "\n",
    "        if \"netD_lr\" in config:\n",
    "            for param_group in optimizerD.param_groups:\n",
    "                param_group[\"lr\"] = config[\"netD_lr\"]\n",
    "        if \"netG_lr\" in config:\n",
    "            for param_group in optimizerG.param_groups:\n",
    "                param_group[\"lr\"] = config[\"netG_lr\"]\n",
    "\n",
    "    while True:\n",
    "        lossG, lossD, is_score = train(netD, netG, optimizerG, optimizerD,\n",
    "                                       criterion, dataloader, step, device,\n",
    "                                       config[\"mnist_model_ref\"])\n",
    "        step += 1\n",
    "        with tune.checkpoint_dir(step=step) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\n",
    "                \"netDmodel\": netD.state_dict(),\n",
    "                \"netGmodel\": netG.state_dict(),\n",
    "                \"optimD\": optimizerD.state_dict(),\n",
    "                \"optimG\": optimizerG.state_dict(),\n",
    "                \"step\": step,\n",
    "            }, path)\n",
    "        tune.report(lossg=lossG, lossd=lossD, is_score=is_score)\n",
    "\n",
    "\n",
    "# __Train_end__\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    ray.init()\n",
    "\n",
    "    import urllib.request\n",
    "    # Download a pre-trained MNIST model for inception score calculation.\n",
    "    # This is a tiny model (<100kb).\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"downloading model\")\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://github.com/ray-project/ray/raw/master/python/ray/tune/\"\n",
    "            \"examples/pbt_dcgan_mnist/mnist_cnn.pt\", MODEL_PATH)\n",
    "\n",
    "    dataloader = get_data_loader()\n",
    "    if not args.smoke_test:\n",
    "        plot_images(dataloader)\n",
    "\n",
    "    # __tune_begin__\n",
    "\n",
    "    # load the pretrained mnist classification model for inception_score\n",
    "    mnist_cnn = Net()\n",
    "    mnist_cnn.load_state_dict(torch.load(MODEL_PATH))\n",
    "    mnist_cnn.eval()\n",
    "    # Put the model in Ray object store.\n",
    "    mnist_model_ref = ray.put(mnist_cnn)\n",
    "\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"netG_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "            \"netD_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "        })\n",
    "\n",
    "    tune_iter = 5 if args.smoke_test else 300\n",
    "    analysis = tune.run(\n",
    "        dcgan_train,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        verbose=1,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        metric=\"is_score\",\n",
    "        mode=\"max\",\n",
    "        num_samples=8,\n",
    "        config={\n",
    "            \"netG_lr\": tune.choice([0.0001, 0.0002, 0.0005]),\n",
    "            \"netD_lr\": tune.choice([0.0001, 0.0002, 0.0005]),\n",
    "            \"mnist_model_ref\": mnist_model_ref\n",
    "        })\n",
    "    # __tune_end__\n",
    "\n",
    "    # demo of the trained Generators\n",
    "    if not args.smoke_test:\n",
    "        all_trials = analysis.trials\n",
    "        checkpoint_paths = [\n",
    "            os.path.join(analysis.get_best_checkpoint(t), \"checkpoint\")\n",
    "            for t in all_trials\n",
    "        ]\n",
    "        demo_gan(analysis, checkpoint_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
