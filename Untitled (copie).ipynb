{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/anaconda3/lib/python3.7/site-packages/dragonfly/utils/oper_utils.py:30: UserWarning:\n",
      "\n",
      "cannot import name 'direct' from 'dragonfly.utils.direct_fortran' (/home/antoine/anaconda3/lib/python3.7/site-packages/dragonfly/utils/direct_fortran/__init__.py)\n",
      "Could not import Fortran direct library. Dragonfly can still be used, but might be slightly slower. To get rid of this warning, install a numpy compatible Fortran compiler (e.g. gfortran) and the python-dev package and reinstall Dragonfly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf\n",
    "\n",
    "def GAN_MNIST(SA):\n",
    "    import ray\n",
    "\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.utils.data\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "    import torchvision.utils as vutils\n",
    "    import numpy as np\n",
    "\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.trial import ExportFormat\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    import argparse\n",
    "    import os\n",
    "    from filelock import FileLock\n",
    "    import random\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data\n",
    "    import numpy as np\n",
    "    from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "    from ray.tune.suggest.ax import AxSearch\n",
    "\n",
    "\n",
    "\n",
    "    from torch.autograd import Variable\n",
    "    from torch.nn import functional as F\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    # Training parameters\n",
    "    dataroot = ray.utils.get_user_temp_dir() + os.sep\n",
    "    workers = 2\n",
    "    batch_size = 64\n",
    "    image_size = 32\n",
    "\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 32\n",
    "\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 32\n",
    "\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # iterations of actual training in each Trainable _train\n",
    "    train_iterations_per_step = 5\n",
    "\n",
    "    MODEL_PATH = os.path.expanduser(\"~/.ray/models/mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "    def get_data_loader():\n",
    "        dataset = dset.MNIST(\n",
    "            root=dataroot,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, ), (0.5, )),\n",
    "            ]))\n",
    "\n",
    "        # Create the dataloader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "    # __GANmodel_begin__\n",
    "    # custom weights initialization called on netG and netD\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find(\"Conv\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find(\"BatchNorm\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "    # Generator Code\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    # __GANmodel_end__\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_begin__\n",
    "    class Net(nn.Module):\n",
    "        \"\"\"\n",
    "        LeNet for MNist classification, used for inception_score\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def inception_score(imgs, mnist_model_ref, batch_size=32, splits=1):\n",
    "        N = len(imgs)\n",
    "        dtype = torch.FloatTensor\n",
    "        dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "        cm = ray.get(mnist_model_ref)  # Get the mnist model from Ray object store.\n",
    "        up = nn.Upsample(size=(28, 28), mode=\"bilinear\").type(dtype)\n",
    "\n",
    "        def get_pred(x):\n",
    "            x = up(x)\n",
    "            x = cm(x)\n",
    "            return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "        preds = np.zeros((N, 10))\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            batch = batch.type(dtype)\n",
    "            batchv = Variable(batch)\n",
    "            batch_size_i = batch.size()[0]\n",
    "            preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "        # Now compute the mean kl-div\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (N // splits):(k + 1) * (N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "        return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_end__\n",
    "\n",
    "\n",
    "    def train(netD, netG, optimG, optimD, criterion, dataloader, iteration, device,\n",
    "              mnist_model_ref):\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            if i >= train_iterations_per_step:\n",
    "                break\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full(\n",
    "                (b_size, ), real_label, dtype=torch.float, device=device)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimG.step()\n",
    "\n",
    "            is_score, is_std = inception_score(fake, mnist_model_ref)\n",
    "\n",
    "            # Output training stats\n",
    "           # if iteration % 10 == 0:\n",
    "            #    print(\"[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z))\"\n",
    "             #         \": %.4f / %.4f \\tInception score: %.4f\" %\n",
    "              #        (iteration, len(dataloader), errD.item(), errG.item(), D_x,\n",
    "               #        D_G_z1, D_G_z2, is_score))\n",
    "\n",
    "        return errG.item(), errD.item(), is_score\n",
    "\n",
    "\n",
    "    def plot_images(dataloader):\n",
    "        # Plot some training images\n",
    "        if 1==0:\n",
    "            real_batch = next(iter(dataloader))\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Original Images\")\n",
    "            plt.imshow(\n",
    "                np.transpose(\n",
    "                    vutils.make_grid(real_batch[0][:64], padding=2,\n",
    "                                     normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def demo_gan(checkpoint_paths):\n",
    "        img_list = []\n",
    "        fixed_noise = torch.randn(64, nz, 1, 1)\n",
    "        for netG_path in checkpoint_paths:\n",
    "            loadedG = Generator()\n",
    "            loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])\n",
    "            with torch.no_grad():\n",
    "                fake = loadedG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)]\n",
    "               for i in img_list]\n",
    "        ani = animation.ArtistAnimation(\n",
    "            fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "        ani.save(\"./generated.gif\", writer=\"imagemagick\", dpi=72)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # __Trainable_begin__\n",
    "    class PytorchTrainable(tune.Trainable):\n",
    "        def setup(self, config):\n",
    "            use_cuda = config.get(\"use_gpu\") and torch.cuda.is_available()\n",
    "            self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            self.netD = Discriminator().to(self.device)\n",
    "            self.netD.apply(weights_init)\n",
    "            self.netG = Generator().to(self.device)\n",
    "            self.netG.apply(weights_init)\n",
    "            self.criterion = nn.BCELoss()\n",
    "            self.config=config\n",
    "            a=10**(-config.get(\"lr1\", 0.01))\n",
    "            b=10**(-config.get(\"lr2\", 0.01))\n",
    "            c1=10**(-config.get(\"weight_decay1\", 0.01))\n",
    "            d=10**(-config.get(\"weight_decay2\", 0.01))\n",
    "            e=1- 5*10**(-config.get(\"beta1\", 0.01))\n",
    "            f=1- 5*10**(-config.get(\"beta2\", 0.01))\n",
    "            if(config.get(\"adam1\",1) >= 0.5):\n",
    "                self.optimizerD = torch.optim.Adam(self.netD.parameters(), lr=a, \n",
    "                                     betas=(e, 0.999),\n",
    "                                                   weight_decay=c1 ,\n",
    "                                         amsgrad=True)\n",
    "            else: \n",
    "                self.optimizerD = adabelief_pytorch.AdaBelief(self.netD.parameters(), lr=a, \n",
    "                                               betas=(e, 0.999),\n",
    "                                                              weight_decay=c1)\n",
    "            if(config.get(\"adam2\",1) >= 0.5):\n",
    "                self.optimizerG = torch.optim.Adam(self.netG.parameters(), lr=b, \n",
    "                                     betas=(f, 0.999),\n",
    "                                                   weight_decay=d, \n",
    "                                         amsgrad=True)\n",
    "            else: \n",
    "                self.optimizerG = adabelief_pytorch.AdaBelief(self.netG.parameters(), lr=b, \n",
    "                                              betas=(f, 0.999),\n",
    "                                                              weight_decay=d)\n",
    "        \n",
    "\n",
    "            with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "                self.dataloader = get_data_loader()\n",
    "            self.mnist_model_ref = c[\"mnist_model_ref\"]\n",
    "\n",
    "        def step(self):\n",
    "            lossG, lossD, is_score = train(self.netD, self.netG, self.optimizerG,\n",
    "                                           self.optimizerD, self.criterion,\n",
    "                                           self.dataloader, self._iteration,\n",
    "                                           self.device, self.mnist_model_ref)\n",
    "            return {\"lossg\": lossG, \"lossd\": lossD, \"is_score\": is_score}\n",
    "\n",
    "        def save_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\n",
    "                \"netDmodel\": self.netD.state_dict(),\n",
    "                \"netGmodel\": self.netG.state_dict(),\n",
    "                \"optimD\": self.optimizerD.state_dict(),\n",
    "                \"optimG\": self.optimizerG.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "            return checkpoint_dir\n",
    "\n",
    "        def load_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            checkpoint = torch.load(path)\n",
    "            self.netD.load_state_dict(checkpoint[\"netDmodel\"])\n",
    "            self.netG.load_state_dict(checkpoint[\"netGmodel\"])\n",
    "            self.optimizerD.load_state_dict(checkpoint[\"optimD\"])\n",
    "            self.optimizerG.load_state_dict(checkpoint[\"optimG\"])\n",
    "\n",
    "        def reset_config(self, new_config):\n",
    "            if \"netD_lr\" in new_config:\n",
    "                for param_group in self.optimizerD.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netD_lr\"]\n",
    "            if \"netG_lr\" in new_config:\n",
    "                for param_group in self.optimizerG.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netG_lr\"]\n",
    "\n",
    "            self.config = new_config\n",
    "            return True\n",
    "\n",
    "        def _export_model(self, export_formats, export_dir):\n",
    "            if export_formats == [ExportFormat.MODEL]:\n",
    "                path = os.path.join(export_dir, \"exported_models\")\n",
    "                torch.save({\n",
    "                    \"netDmodel\": self.netD.state_dict(),\n",
    "                    \"netGmodel\": self.netG.state_dict()\n",
    "                }, path)\n",
    "                return {ExportFormat.MODEL: path}\n",
    "            else:\n",
    "                raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    import urllib.request\n",
    "    # Download a pre-trained MNIST model for inception score calculation.\n",
    "    # This is a tiny model (<100kb).\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"downloading model\")\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://github.com/ray-project/ray/raw/master/python/ray/tune/\"\n",
    "            \"examples/pbt_dcgan_mnist/mnist_cnn.pt\", MODEL_PATH)\n",
    "\n",
    "    dataloader = get_data_loader()\n",
    "    if not args.smoke_test:\n",
    "        plot_images(dataloader)\n",
    "\n",
    "    # load the pretrained mnist classification model for inception_score\n",
    "    mnist_cnn = Net()\n",
    "    mnist_cnn.load_state_dict(torch.load(MODEL_PATH))\n",
    "    mnist_cnn.eval()\n",
    "    mnist_model_ref = ray.put(mnist_cnn)\n",
    "\n",
    "    # __tune_begin__\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"is_score\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"netG_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "            \"netD_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "        })\n",
    "    c={\"mnist_model_ref\" : mnist_model_ref}\n",
    "\n",
    "\n",
    "    experiment_metrics= dict(metric=\"is_score\",\n",
    "        mode=\"max\")\n",
    "\n",
    " \n",
    "    \n",
    "    if(SA==0):\n",
    "        algo = HyperOptSearch(**experiment_metrics)\n",
    "        name=\"hypher\"\n",
    "\n",
    "    if(SA==1):\n",
    "        algo =   BayesOptSearch(**experiment_metrics) \n",
    "        name=\"Bayes\"\n",
    "        \n",
    "    if(SA==2):\n",
    "        algo = AxSearch(\n",
    "            max_concurrent=2, #was working with 2\n",
    "            **experiment_metrics\n",
    "        )\n",
    "        name=\"AX\"\n",
    "\n",
    "        \n",
    "        \n",
    "    if(SA==3):\n",
    "        algo = NevergradSearch(\n",
    "            optimizer=ng.optimizers.CMA,**experiment_metrics\n",
    "            # space=space,  # If you want to set the space manually\n",
    "            )\n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "        name=\"NG\"\n",
    "\n",
    "        \n",
    "     \n",
    "    if(SA==4):\n",
    "        return\n",
    "        scheduler = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=100,\n",
    "            reduction_factor=4,\n",
    "            **experiment_metrics)\n",
    "\n",
    "        algo = TuneBOHB(\n",
    "            # space=config_space, \n",
    "            max_concurrent=4,\n",
    "            **experiment_metrics)\n",
    "        name=\"BOHB\"\n",
    "        \n",
    "    if(SA==5):\n",
    "        algo = NevergradSearch(\n",
    "        optimizer=ng.optimizers.RandomSearch,**experiment_metrics\n",
    "        # space=space,  # If you want to set the space manually\n",
    "        )\n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "        name=\"random\"\n",
    "                \n",
    "    if(SA==6):\n",
    "        dim_dict = {\n",
    "        \"netG_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2),\n",
    "        \"netD_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2)\n",
    "        }\n",
    "        algo = ZOOptSearch(\n",
    "        algo=\"Asracos\",  # only support Asracos currently\n",
    "        #dim_dict=dim_dict,\n",
    "      #  dim_dict=dim_dict,\n",
    "\n",
    "        budget=100,\n",
    "            **experiment_metrics\n",
    "        #dim_dict=dim_dict,\n",
    "       #     **zoopt_search_config,\n",
    "        )\n",
    "        name=\"zoo\"\n",
    "\n",
    "\n",
    "    if(SA!=4):\n",
    "        scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "        \n",
    "        \n",
    "        \n",
    "    tune_kwargs = {\n",
    "    \"num_samples\": 128 if args.smoke_test else 128,\n",
    "      \"stop\":{\n",
    "            \"training_iteration\": 200,\n",
    "        },  \n",
    "\n",
    "    \"config\": {\n",
    "     \"lr1\":  tune.uniform(2, 5) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "    , \"lr2\":  tune.uniform(2, 5) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "    ,     \"weight_decay1\":0#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "    ,     \"weight_decay2\":0#tune.uniform(1, 5)#,1e-4), #*10 et 0    \n",
    "    ,     \"beta1\":2 #tune.uniform(1, 5),#,1e-4), #*10 et 0\n",
    "     ,    \"beta2\":1 #tune.uniform(1, 5),#,1e-4), #*10 et 0\n",
    "      ,  \"adam1\":0#tune.uniform(0,1),\n",
    "      ,  \"adam2\":0#tune.uniform(0,1),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    analysis = tune.run(\n",
    "    PytorchTrainable,\n",
    "    name=name,\n",
    "    scheduler=scheduler,\n",
    "    reuse_actors=True,\n",
    "    search_alg=algo,\n",
    "    verbose=1,\n",
    "    checkpoint_at_end=False,\n",
    "   # num_samples=8,\n",
    "    export_formats=[ExportFormat.MODEL],\n",
    "    **tune_kwargs,           loggers=[TestLogger])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:16:51,865\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8270\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.34',\n",
       " 'raylet_ip_address': '192.168.1.34',\n",
       " 'redis_address': '192.168.1.34:10928',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-26_12-16-50_754837_24951/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-26_12-16-50_754837_24951/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8270',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-26_12-16-50_754837_24951',\n",
       " 'metrics_export_port': 62899}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using AsyncHyperBand: num_stopped=41\n",
       "Bracket: Iter 64.000: 1.0000000000000004 | Iter 16.000: 1.0000000000000004 | Iter 4.000: 1.0000000000000004 | Iter 1.000: 1.0000000000000004<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.56 GiB heap, 0.0/1.22 GiB objects<br>Result logdir: /home/antoine/ray_results/hypher<br>Number of trials: 128 (79 PENDING, 8 RUNNING, 41 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  adam1</th><th style=\"text-align: right;\">  adam2</th><th style=\"text-align: right;\">  beta1</th><th style=\"text-align: right;\">  beta2</th><th style=\"text-align: right;\">    lr1</th><th style=\"text-align: right;\">    lr2</th><th style=\"text-align: right;\">  weight_decay1</th><th style=\"text-align: right;\">  weight_decay2</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     lossg</th><th style=\"text-align: right;\">    lossd</th><th style=\"text-align: right;\">  is_score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PytorchTrainable_e4fdf938</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.11514</td><td style=\"text-align: right;\">2.74967</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e5005520</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.06986</td><td style=\"text-align: right;\">4.40873</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e502c9ea</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.69477</td><td style=\"text-align: right;\">3.91541</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e504e342</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.47328</td><td style=\"text-align: right;\">4.59542</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e506b8de</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.14028</td><td style=\"text-align: right;\">4.08734</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e508f84c</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.08847</td><td style=\"text-align: right;\">3.19894</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e50b1802</td><td>PENDING   </td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.86359</td><td style=\"text-align: right;\">3.38553</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>PytorchTrainable_e4e055a4</td><td>RUNNING   </td><td>192.168.1.34:28196</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.83436</td><td style=\"text-align: right;\">4.88248</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         558.418</td><td style=\"text-align: right;\">4.8535    </td><td style=\"text-align: right;\">0.0340454</td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4e4ae88</td><td>RUNNING   </td><td>192.168.1.34:30430</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.02378</td><td style=\"text-align: right;\">3.77575</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         347.932</td><td style=\"text-align: right;\">0.266395  </td><td style=\"text-align: right;\">2.45021  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4e894b2</td><td>RUNNING   </td><td>192.168.1.34:30727</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.69047</td><td style=\"text-align: right;\">2.86682</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         325.416</td><td style=\"text-align: right;\">0.00493047</td><td style=\"text-align: right;\">5.94546  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4ee9416</td><td>RUNNING   </td><td>192.168.1.34:30747</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.27475</td><td style=\"text-align: right;\">3.08241</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         323.805</td><td style=\"text-align: right;\">0.771626  </td><td style=\"text-align: right;\">1.29396  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4f25196</td><td>RUNNING   </td><td>192.168.1.34:30765</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.67536</td><td style=\"text-align: right;\">3.52711</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         326.076</td><td style=\"text-align: right;\">0.788705  </td><td style=\"text-align: right;\">2.27286  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4f53820</td><td>RUNNING   </td><td>192.168.1.34:30749</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.32472</td><td style=\"text-align: right;\">4.20729</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.714</td><td style=\"text-align: right;\">0.616395  </td><td style=\"text-align: right;\">1.03863  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4f7b80c</td><td>RUNNING   </td><td>192.168.1.34:30748</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.29685</td><td style=\"text-align: right;\">4.30566</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          90.012</td><td style=\"text-align: right;\">0.684279  </td><td style=\"text-align: right;\">0.988723 </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4052bd2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.64462</td><td style=\"text-align: right;\">4.75469</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         476.145</td><td style=\"text-align: right;\">1.69237   </td><td style=\"text-align: right;\">0.444805 </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e40852da</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">3.44755</td><td style=\"text-align: right;\">2.85275</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         469.585</td><td style=\"text-align: right;\">0.470705  </td><td style=\"text-align: right;\">1.78481  </td><td style=\"text-align: right;\">   1.00027</td></tr>\n",
       "<tr><td>PytorchTrainable_e40e1102</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.51679</td><td style=\"text-align: right;\">4.43673</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         480.213</td><td style=\"text-align: right;\">0.757592  </td><td style=\"text-align: right;\">1.20519  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e412718e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.64646</td><td style=\"text-align: right;\">3.77091</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         299.437</td><td style=\"text-align: right;\">0.0838801 </td><td style=\"text-align: right;\">3.32767  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e4170924</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.39164</td><td style=\"text-align: right;\">2.93668</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         298.455</td><td style=\"text-align: right;\">0.742186  </td><td style=\"text-align: right;\">1.3528   </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e41bae20</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">2.13893</td><td style=\"text-align: right;\">2.69081</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         299.286</td><td style=\"text-align: right;\">0.97075   </td><td style=\"text-align: right;\">1.31478  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "<tr><td>PytorchTrainable_e41e1746</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">4.71239</td><td style=\"text-align: right;\">4.08307</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         300.297</td><td style=\"text-align: right;\">0.27596   </td><td style=\"text-align: right;\">2.21163  </td><td style=\"text-align: right;\">   1      </td></tr>\n",
       "</tbody>\n",
       "</table><br>... 108 more trials not shown (72 PENDING, 1 RUNNING, 34 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bed3b914c995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msupernombre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mGAN_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ab8ad74c0709>\u001b[0m in \u001b[0;36mGAN_MNIST\u001b[0;34m(SA)\u001b[0m\n\u001b[1;32m    596\u001b[0m    \u001b[0;31m# num_samples=8,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0mexport_formats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExportFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     **tune_kwargs,           loggers=[TestLogger])\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m         )\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import *\n",
    "supernombre=0;\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\"/home/antoine/Projet/NovelTuning/\", str(supernombre)+\".csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        #if \"done\" in tmp:\n",
    "         #   if(tmp[\"done\"] != True):\n",
    "\n",
    "        if \"config\" in tmp:\n",
    "            del tmp[\"config\"]\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            if not self._continuing:\n",
    "                self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "                                         \n",
    "for i in range(0,7):\n",
    "    supernombre=i\n",
    "    GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "        \n",
    "    \n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32  256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0  5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "experiment_metrics= dict(metric=\"is_score\",\n",
    "    mode=\"max\")\n",
    "\n",
    "\n",
    "dim_dict = {\n",
    "    \"netG_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2),\n",
    "    \"netD_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2)\n",
    "}\n",
    "\n",
    "config =     {\n",
    "        \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "       \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "\n",
    "algo = NevergradSearch(optimizer=ng.optimizers.OnePlusOne)\n",
    "\n",
    "\n",
    "tune_iter = 5 if args.smoke_test else 1\n",
    "#c={\"mnist_model_ref\" : mnist_model_ref}\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "\n",
    "\n",
    "ITERATIONS = 10\n",
    "NUM_TUNED= 1\n",
    "    \n",
    "\n",
    "\n",
    "#[nn.ReLU, nn.Softmax(), nn.Tanh(),nn.Sigmoid() ]\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if True else 2,\n",
    "    \"config\": {\n",
    "    \"steps\": 3,  # evaluation times\n",
    "     \"lr\":  tune.quniform(1e-10, 0.1,1e-10),\n",
    "    \"b1\": tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"b2\":tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"eps\": tune.uniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.quniform(1e-10, 0.1,1e-10),\n",
    "        \"sigmoid_func\":nn.ReLU()\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-10, 0.1),\n",
    "    \"b1\": tune.loguniform(0.9, 1-1e-10),\n",
    "        \"b2\":tune.loguniform(0.9, 1-1e-10),\n",
    "        \"eps\": tune.loguniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "}\n",
    "   \n",
    "#i is in [0;1]\n",
    "#We want all values between 0 and 1\n",
    "def get_sigmoid_func(i):\n",
    "    if(i<0.33):\n",
    "        return nn.ReLU()\n",
    "    elif(i<0.67):\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "    \n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))    \n",
    "    \n",
    "def get_loos_func(i):\n",
    "        return nn.MSELoss()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "optimizer_is_adam = True   \n",
    "    \n",
    "f = get_sigmoid_func(3)\n",
    "print(f(torch.randn(2)))\n",
    "import random\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.loguniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.loguniform(32.,256.),#,1), #log de 32  256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0  5     \n",
    "        \"adam\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32  256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0  5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            analysis = tune.run(\n",
    "            PytorchTrainable,\n",
    "            name=\"pbt_dcgan_mnist\",\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            search_alg=algo,\n",
    "            verbose=1,\n",
    "            checkpoint_at_end=True,\n",
    "            stop={\n",
    "                \"training_iteration\": tune_iter,\n",
    "            },\n",
    "            num_samples=8,\n",
    "            export_formats=[ExportFormat.MODEL],\n",
    "            config={\n",
    "                \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "                \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
