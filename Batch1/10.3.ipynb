{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_TREC(config):\n",
    "    import torch\n",
    "    from torchtext import data\n",
    "    from torchtext import datasets\n",
    "    import random\n",
    "\n",
    "    SEED = 1234\n",
    "    savedPath = os.getcwd()\n",
    "    os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "    \n",
    "    \n",
    "    #torch.manual_seed(SEED)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    TEXT = data.Field(tokenize = 'spacy')\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    \n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.100d', \n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    os.chdir(savedPath)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                     dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            self.convs = nn.ModuleList([\n",
    "                                        nn.Conv2d(in_channels = 1, \n",
    "                                                  out_channels = n_filters, \n",
    "                                                  kernel_size = (fs, embedding_dim)) \n",
    "                                        for fs in filter_sizes\n",
    "                                        ])\n",
    "\n",
    "            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text):\n",
    "\n",
    "            #text = [sent len, batch size]\n",
    "\n",
    "            text = text.permute(1, 0)\n",
    "\n",
    "            #text = [batch size, sent len]\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "            conved = [sigmoid_func_uniq(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "            #pooled_n = [batch size, n_filters]\n",
    "\n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "            return self.fc(cat)\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    " #   print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    def categorical_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text)\n",
    "\n",
    "                loss = criterion(predictions, batch.label)\n",
    "\n",
    "                acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    import time\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for e in range(ITERATIONS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        tune.report(loss=valid_loss)\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    " #       print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "  #      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "             drop_prob, sigmoid , vocab_size):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        #self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden= tuple([each.data for each in hidden])\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class SentimentRNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "             drop_prob, sigmoid , vocab_size):\n",
    "        super(SentimentRNN1, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                        #    dropout=drop_prob, batch_first=True)\n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden = hidden.data\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        (a,b) = hidden\n",
    "        return a\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx,sigmoid):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "       # text = text.permute(1, 0)\n",
    "        #We want already have batch size, len for sentiment!!!!!\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "        conved = [self.sigmoid(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        sig_out = self.sig(self.fc(cat))\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(50, -1)\n",
    "        #sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return np.squeeze(sig_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "        \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = int(config.get(\"embedding\",600))\n",
    "    hidden_dim = int(round(config.get(\"hidden_dim\",64)))\n",
    "    n_layers =  2+ int( round(config.get(\"n_layer\",1)))\n",
    "\n",
    "\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "   # OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = 4    \n",
    "    cnn = 0 \n",
    "    if(config.get(\"model\",0)<1/3):\n",
    "\n",
    "        net = SentimentRNN(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)    \n",
    "    elif(config.get(\"model\",0)<2/3):\n",
    "        net = SentimentRNN1(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)\n",
    "    else:\n",
    "        cnn = 1;\n",
    "        net = CNN(vocab_size, embedding_dim, hidden_dim, FILTER_SIZES, \n",
    "              output_size, config.get(\"droupout_prob\",0.1), PAD_IDX,sigmoid_func_uniq)\n",
    "    # loss and optimization functions\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "   # print(*(n for n in net.parameters()))\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # training params\n",
    "    if(config.get(\"adam\",0)>0.5):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "           weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                              weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 32 *4 *8\n",
    "    TEST_SIZE = 32 *2 * 4\n",
    "\n",
    "    def train_rnn():\n",
    "        \n",
    "        h = net.init_hidden(batch_size)\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(inputs) > EPOCH_SIZE:\n",
    "                return\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step() \n",
    "\n",
    "    def train_cnn(model, optimizer ,func ,train_loader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.train()\n",
    "        #for (data, target) in train_loader:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(data) > EPOCH_SIZE:\n",
    "               # print(\"hehe\")\n",
    "                return\n",
    "\n",
    "            # We set this just for the example to run quickly.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = func(output.squeeze(), target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    net.train()\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        if(cnn==0):\n",
    "\n",
    "            train_rnn()\n",
    "        else:\n",
    "            train_cnn(net,optimizer,criterion,train_loader)\n",
    "        \n",
    "        acc = test(net,criterion,valid_loader,3+cnn)\n",
    "\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3456, 0.6481])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_HyperOpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2d6763d37d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m#x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mf_HyperOpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_IMDB1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_HyperOpt' is not defined"
     ]
    }
   ],
   "source": [
    "#Configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "        \n",
    "    \n",
    "    \n",
    "#experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "experiment_metrics = dict(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 5\n",
    "    \n",
    "\n",
    "\n",
    "#[nn.ReLU, nn.Softmax(), nn.Tanh(),nn.Sigmoid() ]\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if True else 2,\n",
    "    \"config\": {\n",
    "    \"steps\": 3,  # evaluation times\n",
    "     \"lr\":  tune.quniform(1e-10, 0.1,1e-10),\n",
    "    \"b1\": tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"b2\":tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"eps\": tune.uniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.quniform(1e-10, 0.1,1e-10),\n",
    "        \"sigmoid_func\":nn.ReLU()\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-10, 0.1),\n",
    "    \"b1\": tune.loguniform(0.9, 1-1e-10),\n",
    "        \"b2\":tune.loguniform(0.9, 1-1e-10),\n",
    "        \"eps\": tune.loguniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "}\n",
    "   \n",
    "#i is in [0;1]\n",
    "#We want all values between 0 and 1\n",
    "def get_sigmoid_func(i):\n",
    "    if(i<0.33):\n",
    "        return nn.ReLU()\n",
    "    elif(i<0.67):\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "    \n",
    "optimizer_is_adam = True   \n",
    "    \n",
    "f = get_sigmoid_func(3)\n",
    "print(f(torch.randn(2)))\n",
    "import random\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.loguniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.loguniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1),\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  \n",
    "     \"lr\": 0.001# tune.uniform(1e-4, 0.1 ),\n",
    "     ,    \"embedding\": 400#tune.uniform(64, 1024),\n",
    "\n",
    "      ,   \"weight_decay\":tune.uniform(1e-4, 0.1),\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":256#tune.uniform(32.,256.),\n",
    "      ,  \"n_layer\":1 #tune.uniform(1,3),\n",
    "      ,  \"droupout_prob\":0.5#tune.uniform(0,0.5),     \n",
    "      ,  \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\n",
    "\n",
    "f_HyperOpt(train_IMDB1)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One shot\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "for i in range(0,1):\n",
    "    for j in range(0,1):\n",
    "        x = x_all[i]\n",
    "        f_HyperOpt(x)\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small budget\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 20\n",
    "\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "optimizer_is_adam = True\n",
    "if(0==1):\n",
    "    for i in range(1,2):\n",
    "        x = train_TREC\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_HyperOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = HyperOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler,  name=\"hyper\", search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BayesOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = BayesOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler, name=\"bayes\",  search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_AX(dataset):\n",
    "    \n",
    "   \n",
    "    if __name__ == \"__main__\":\n",
    "                \n",
    "        algo = AxSearch(\n",
    "            max_concurrent=2, #was working with 2\n",
    "            **experiment_metrics\n",
    "        )\n",
    "        scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "        tune.run(\n",
    "            dataset,       name=\"ax\",\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            **tune_kwargs)\n",
    "\n",
    "        \n",
    "#        algo = AxSearch(\n",
    "#            **experiment_metrics\n",
    "#        )\n",
    "#        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        \n",
    "#        scheduler = AsyncHyperBandScheduler()\n",
    "#        tune.run(\n",
    "#            dataset,\n",
    "#            **experiment_metrics,\n",
    "#            search_alg=algo,\n",
    "#            scheduler=scheduler,\n",
    "#            **tune_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO We are interested in multiple Population based algorithms from nevergrad, and certainly not in OnePlusOne. \n",
    "def f_NeverGrad(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.CMA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"ng\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BOHB(dataset):\n",
    "\n",
    "    bohb_hyperband = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=100,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    bohb_search = TuneBOHB(\n",
    "        # space=config_space, \n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "       # config=config, \n",
    "        scheduler=bohb_hyperband,name=\"bohb\",\n",
    "        search_alg=bohb_search,       \n",
    "         **tune_kwargs)\n",
    "        #num_samples=NUM_TUNED,\n",
    "       # stop={\"training_iteration\": 100})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Random(dataset):\n",
    "    \n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo,   name=\"random\",    \n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ZOOpt(dataset):\n",
    "\n",
    "    dim_dict = {\n",
    "        \"lr\": (ValueType.CONTINUOUS, [0, 1], 1e-2),\n",
    "        \"momentum\": (ValueType.CONTINUOUS, [0,1, 0.9], 1e-2)\n",
    "    }\n",
    "\n",
    "    zoopt_search_config = {\n",
    "        \"parallel_num\": 8,  # how many workers to parallel\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    zoopt_search = ZOOptSearch(\n",
    "    algo=\"Asracos\",  # only support Asracos currently\n",
    "    #dim_dict=dim_dict,\n",
    "    budget=ITERATIONS,\n",
    "    #dim_dict=dim_dict,\n",
    "   #     **zoopt_search_config,\n",
    "    **experiment_metrics)\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "\n",
    "   \n",
    "    tune.run(dataset,\n",
    " #        config = config,\n",
    "    search_alg=zoopt_search,\n",
    "   # num_samples= ITERATIONS,\n",
    "    scheduler=scheduler,       \n",
    "    #         paralell_num=4,\n",
    "    name=\"zoopt_search\", \n",
    "              **tune_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "\n",
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)} \n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "\n",
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])\n",
    "\n",
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n",
    "\n",
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "\n",
    "\n",
    "\n",
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "## get any indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length review with their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))\n",
    "\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    ## getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    ## for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])\n",
    "\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB1(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    print(train_x.shape)\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    #sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = 400 \n",
    "    hidden_dim = 256\n",
    "    n_layers = 2\n",
    "\n",
    "    net = SentimentRNN(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           0.3,\n",
    "                           1, vocab_size)  \n",
    "    print(net)\n",
    "\n",
    "\n",
    "        # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 32 *4 *8\n",
    "    TEST_SIZE = 32 *2 * 4\n",
    "\n",
    "        # training params\n",
    "\n",
    "    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 10\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "\n",
    "\n",
    "train_IMDB1(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if args.smoke_test else 1,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  \n",
    "     \"lr\": 0.001# tune.uniform(1e-4, 0.1 ),\n",
    "     ,    \"embedding\": 400#tune.uniform(64, 1024),\n",
    "\n",
    "      ,   \"weight_decay\":tune.uniform(1e-4, 0.1),\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":256#tune.uniform(32.,256.),\n",
    "      ,  \"n_layer\":1 #tune.uniform(1,3),\n",
    "      ,  \"droupout_prob\":0.5#tune.uniform(0,0.5),     \n",
    "      ,  \"adam\":tune.uniform(0,1),\n",
    "        \"model\":0 #tune.uniform(0,1),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\n",
    "\n",
    "f_HyperOpt(train_IMDB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-7f57bd76a2d0>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-7f57bd76a2d0>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB1(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    print(train_x.shape)\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    #sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = 400 \n",
    "    hidden_dim = 256\n",
    "    n_layers = 2\n",
    "\n",
    "    net = SentimentRNN(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           0.3,\n",
    "                           1, vocab_size)  \n",
    "    print(net)\n",
    "    class SentimentRNN2(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN2, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "    net = SentimentRNN2(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "\n",
    "        # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 32 *4 *8\n",
    "    TEST_SIZE = 32 *2 * 4\n",
    "\n",
    "        # training params\n",
    "\n",
    "    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 10\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "\n",
    "\n",
    "train_IMDB1(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
