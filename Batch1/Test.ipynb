{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FNN : https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "\n",
    "\n",
    "class Net42(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net42, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = sigmoid_func_uniq(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return (x)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        denom = ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        if torch.isinf(denom).sum().item()>0:\n",
    "              output_tensor = input_tensor / torch.sqrt(squared_norm)\n",
    "        else:\n",
    "              output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "    \n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    \n",
    "#https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/    \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.input_d = input_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_d)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class NeurNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(NeurNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,sigmoid = F.log_softmax):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        return sigmoid(x)\n",
    "    \n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, cols, size_hidden, n_output):\n",
    "        super(NetG, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.7375\n",
      "10 0.825\n",
      "15 0.875\n",
      "20 0.8875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 512*32*2\n",
    "TEST_SIZE = 256\n",
    "\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "def test(model, func, data_loader, clas):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            if(clas == 1): #classification\n",
    "                # We set this just for the example to run quickly.\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "            if(clas == 2): #regression\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                from torch.autograd import Variable\n",
    "\n",
    "                X = Variable(torch.FloatTensor(data)) \n",
    "                result = model(X)\n",
    "                pred=result.data[:,0].numpy()\n",
    "                out = target.data[:,0].numpy()\n",
    "                #print( pred)\n",
    "                #print(out)\n",
    "                #pred.fillna(X_train.mean(), inplace=True)\n",
    "                total += target.size(0)\n",
    "                #correct += r2_score(pred,out)\n",
    "                correct+=func(result,target).numpy()   \n",
    "            if(clas == 3): #RNN\n",
    "                val_h = model.init_hidden(50) #batch size\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                #if(train_on_gpu): FALSE\n",
    "                #    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output, val_h = model(data, val_h)\n",
    "                val_loss = func(output.squeeze(), target.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "               # model.train()\n",
    "                correct = np.mean(val_losses)\n",
    "                total = 1;\n",
    "                \n",
    "    return correct / total\n",
    "\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    #optimizer = optim.SGD(        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader)\n",
    "        # Send the current training result back to Tune\n",
    "        #tune.report(mean_accuracy=1-acc)\n",
    "        #test(model, F.nll_loss, test_loader,1)\n",
    "        if(i%5 == 4):\n",
    "            acc = test(model, F.nll_loss, test_loader,1)\n",
    "            print(i+1,acc)\n",
    "        #if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "        #    torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    "\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.048164 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.0071067 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.91821,\n",
    "        \"hidden_dim\":42.053,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.5463,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.20304,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.16959,\n",
    "        \"model\":0.12932\n",
    "    } #Bayes\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0040956 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.019716 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.60423,\n",
    "        \"hidden_dim\":58.111,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.5025,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.13126,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.16959,\n",
    "        \"model\":0.12932\n",
    "    } #AX\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0038906 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.044830 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.34570,\n",
    "        \"hidden_dim\":54.625,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.7659,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.31195,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.16959,\n",
    "        \"model\":0.12932\n",
    "    } #NG\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.057313 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.0025655 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.51317,\n",
    "        \"hidden_dim\":195.35,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.7306,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.060365,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.82592,\n",
    "        \"model\":0.22659\n",
    "    } #BOHB\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.093732 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.078377 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.26150,\n",
    "        \"hidden_dim\":73.424,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.6024,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.17638,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.23131,\n",
    "        \"model\":0.55079\n",
    "    } #Hyper\n",
    "       #0.23131 0.17638 73.424 0.093732 0.55079 1.6024 0.26150 0.078377 0.91875 \n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.054028 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.044236 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.42264,\n",
    "        \"hidden_dim\":195.35,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.2390,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.22878,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.82592,\n",
    "        \"model\":0.22659\n",
    "    } #Random\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.076673 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.027438 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.52229,\n",
    "        \"hidden_dim\":221.48,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.9617,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.15109,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.48009,\n",
    "        \"model\":0.22659\n",
    "    } #Zoopt\n",
    "train_mnist(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision                                                       \n",
    "import torchvision.transforms as transforms\n",
    "#https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html\n",
    "import os.path        \n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.099620 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.015169 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.37375,\n",
    "        \"hidden_dim\":167.23,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.3112,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.34757,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.16959,\n",
    "        \"model\":0.12932\n",
    "    } #Zoopt\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.058366 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.00010000 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.38011,\n",
    "        \"hidden_dim\":255.68,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.4875,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.11918,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #AX\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.068062 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.071767 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.44542,\n",
    "        \"hidden_dim\":202.77,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.3735,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.27348,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #Bayes\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.079296 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.082595 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.41622,\n",
    "        \"hidden_dim\":213.35,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.1988,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.31396,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #NG\n",
    "\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.093228 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.097429 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.49388,\n",
    "        \"hidden_dim\":205.33,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.8039,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.29669,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #HyperOpt\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.084595 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.025274 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.30175,\n",
    "        \"hidden_dim\":86.522,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.4178,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.29720,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "} # BOHB\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.093228 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.097429 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.49388,\n",
    "        \"hidden_dim\":205.33,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.8039,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.29669,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #HyperOpt\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.069708 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.056939 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.37620,\n",
    "        \"hidden_dim\":154.32,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.2358,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.30663,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.39323,\n",
    "        \"model\":0.024937\n",
    "\n",
    "    } #Random\n",
    "\n",
    "def train_fashion_mnist(config):\n",
    "    from keras.datasets import fashion_mnist\n",
    "    dataset_dir = os.path.join(os.path.expanduser(\"~\"), 'Datasets', 'FashionMNIST')\n",
    "    valid_ratio = 0.2  # Going to use 80%/20% split for train/valid\n",
    "\n",
    "    # Load the dataset for the training/validation sets\n",
    "    train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                           train=True,\n",
    "                                           transform= None, #transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "    # Split it into training and validation sets\n",
    "    nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "    nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "\n",
    "\n",
    "    # Load the test set\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                                     transform= None, #transforms.ToTensor(),\n",
    "                                                    train=False)\n",
    "    \n",
    "    class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, base_dataset, transform):\n",
    "            self.base_dataset = base_dataset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img, target = self.base_dataset[index]\n",
    "            return self.transform(img), target\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "\n",
    "\n",
    "    train_dataset = DatasetTransformer(train_dataset, transforms.ToTensor())\n",
    "    valid_dataset = DatasetTransformer(valid_dataset, transforms.ToTensor())\n",
    "    test_dataset  = DatasetTransformer(test_dataset , transforms.ToTensor())\n",
    "    ############################################################################################ Dataloaders\n",
    "    num_threads = 4     # Loading the dataset is using 4 CPU threads\n",
    "    batch_size  = 512*8   # Using minibatches of 128 samples\n",
    "\n",
    "    train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,                # <-- this reshuffles the data at every epoch\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    valid_loader1 = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "\n",
    "    test_loader1 = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        model = NeurNet(784,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "        \n",
    "   # optimizer = optim.SGD(    model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader1)\n",
    "        # Send the current training result back to Tune\n",
    "        #tune.report(mean_accuracy=1-acc)\n",
    "        if(i%5 == 4):\n",
    "            acc = test(model, F.nll_loss, test_loader1,1)\n",
    "            print(i+1,acc)\n",
    "        #if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "        #    torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    "for i in range(20):        \n",
    "    train_fashion_mnist(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_TREC(config):\n",
    "    import torch\n",
    "    from torchtext import data\n",
    "    from torchtext import datasets\n",
    "    import random\n",
    "\n",
    "    SEED = 1234\n",
    "    savedPath = os.getcwd()\n",
    "    os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "    \n",
    "    \n",
    "    #torch.manual_seed(SEED)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    TEXT = data.Field(tokenize = 'spacy')\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    \n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.100d', \n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    os.chdir(savedPath)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                     dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            self.convs = nn.ModuleList([\n",
    "                                        nn.Conv2d(in_channels = 1, \n",
    "                                                  out_channels = n_filters, \n",
    "                                                  kernel_size = (fs, embedding_dim)) \n",
    "                                        for fs in filter_sizes\n",
    "                                        ])\n",
    "\n",
    "            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text):\n",
    "\n",
    "            #text = [sent len, batch size]\n",
    "\n",
    "            text = text.permute(1, 0)\n",
    "\n",
    "            #text = [batch size, sent len]\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "            conved = [sigmoid_func_uniq(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "            #pooled_n = [batch size, n_filters]\n",
    "\n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "            return self.fc(cat)\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    " #   print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    def categorical_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text)\n",
    "\n",
    "                loss = criterion(predictions, batch.label)\n",
    "\n",
    "                acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    import time\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for e in range(ITERATIONS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        tune.report(loss=valid_loss)\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    " #       print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "  #      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.00799585527248597\n",
      "10 0.006927751423267836\n",
      "15 0.00640134664064043\n",
      "20 0.006154981891760666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_diabetes(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "\n",
    "    from sklearn.datasets import load_diabetes\n",
    "\n",
    "    (X,Y) = load_diabetes( return_X_y=True, as_frame=True)\n",
    "    X = pd.DataFrame(X)\n",
    "    Y = pd.DataFrame(Y)\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1234)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    #print(y_train)\n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "    \n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        net = NeurNet(10,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        net = LogReg(10,1)\n",
    "        \n",
    "   # optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "        \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        #acc = test(net,loss_func ,test_loader, 2)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        #tune.report(loss=acc)\n",
    "\n",
    "        if(i%5 == 4):\n",
    "            acc = test(net, loss_func, test_loader,2)\n",
    "            print(i+1,acc)\n",
    "        #if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "        #    torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    "\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0098927 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.020113 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.68115,\n",
    "        \"hidden_dim\":107.92,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.3969,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.27019,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.87860,\n",
    "        \"model\":0.17290\n",
    "    } #Ax\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0061335 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.022963 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.48141,\n",
    "        \"hidden_dim\":162.52,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.0259,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.13237,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.67711,\n",
    "        \"model\":0.27600\n",
    "    } #NG\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0077733 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.024219 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.52676,\n",
    "        \"hidden_dim\":97.465,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.4927,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.085898,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.61162,\n",
    "        \"model\":0.27326\n",
    "    } #Hyper\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0042150 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.015286 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.20187,\n",
    "        \"hidden_dim\":171.08,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.1989,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.14294,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.69149,\n",
    "        \"model\":0.051722\n",
    "    } #Bayes\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.052686 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.037083 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.62686,\n",
    "        \"hidden_dim\":80.466,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.7254,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.41249,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.58182,\n",
    "        \"model\":0.25882\n",
    "    } #Random\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.099015 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.070328 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.034509,\n",
    "        \"hidden_dim\":159.27,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.3311,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.30359,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.21977,\n",
    "        \"model\":0.13229\n",
    "    } #BOHB\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.058505 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.0034227 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.62460,\n",
    "        \"hidden_dim\":56.057,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.9144,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.14703,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.84331,\n",
    "        \"model\":0.86984\n",
    "    } #Zoopt\n",
    "train_diabetes(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh()\n",
      "5 2.3719007454666436\n",
      "10 0.022305953736398734\n",
      "15 0.018134681617512423\n",
      "20 0.015038719364241058\n",
      "Tanh()\n",
      "5 1.8576033349130667\n",
      "10 0.4720433179069968\n",
      "15 0.09468890171425015\n",
      "20 0.08033508062362671\n",
      "Tanh()\n",
      "5 0.9251711602304497\n",
      "10 0.01959119766366248\n",
      "15 0.026950648017958097\n",
      "20 0.01022488996386528\n",
      "Tanh()\n",
      "5 2.3763308805577896\n",
      "10 0.4034341363345875\n",
      "15 0.042285118617263494\n",
      "20 0.019045823344997333\n",
      "Tanh()\n",
      "5 0.2737850955888337\n",
      "10 0.07125686664207309\n",
      "15 0.010260301952560743\n",
      "20 0.010397423497017692\n",
      "Tanh()\n",
      "5 4.660696889839921\n",
      "10 0.23205278434005439\n",
      "15 0.0409171978632609\n",
      "20 0.015736636577867995\n",
      "Tanh()\n",
      "5 2.358195360969095\n",
      "10 0.14951402620941984\n",
      "15 0.03038673716432908\n",
      "20 0.01036935350766369\n",
      "Tanh()\n",
      "5 0.05920960388931574\n",
      "10 1.6313900667078354\n",
      "15 0.045112296646716545\n",
      "20 0.03760210673014323\n",
      "Tanh()\n",
      "5 2.765773623597388\n",
      "10 0.10993408689311907\n",
      "15 0.012394175517792795\n",
      "20 0.035570486503488874\n",
      "Tanh()\n",
      "5 2.7710483775419346\n",
      "10 0.07024472250657923\n",
      "15 0.037646196636499145\n",
      "20 0.011714947209054348\n",
      "Tanh()\n",
      "5 1.2364160126330805\n",
      "10 0.10514309359531776\n",
      "15 0.014803111553192139\n",
      "20 0.013163147019404992\n",
      "Tanh()\n",
      "5 0.19625335581162395\n",
      "10 0.04008598947057537\n",
      "15 0.02155603906687568\n",
      "20 0.04049221674601237\n",
      "Tanh()\n",
      "5 1.096333859013576\n",
      "10 0.28182126026527554\n",
      "15 0.027935489719989253\n",
      "20 0.01318393658627482\n",
      "Tanh()\n",
      "5 1.2767957051595051\n",
      "10 0.0526124028598561\n",
      "15 0.08913741625991523\n",
      "20 0.011458582299597123\n",
      "Tanh()\n",
      "5 1.5089327494303386\n",
      "10 0.4526525478737027\n",
      "15 0.02801877493951835\n",
      "20 0.06060514964309393\n",
      "Tanh()\n",
      "5 1.052407657398897\n",
      "10 0.4536150390026616\n",
      "15 0.037558325365477915\n",
      "20 0.01732540567144158\n",
      "Tanh()\n",
      "5 0.12972165089027554\n",
      "10 0.026862731166914396\n",
      "15 0.03256954588726455\n",
      "20 0.026550758118722952\n",
      "Tanh()\n",
      "5 3.161802179673139\n",
      "10 0.16078810598336013\n",
      "15 0.013245435906391517\n",
      "20 0.012198032993896334\n",
      "Tanh()\n",
      "5 0.15887288354775486\n",
      "10 0.03777085916668761\n",
      "15 0.015297765240949742\n",
      "20 0.015044755795422722\n",
      "Tanh()\n",
      "5 0.17054814918368472\n",
      "10 0.03355282603525648\n",
      "15 0.00912773039411096\n",
      "20 0.027549793907240324\n",
      "Tanh()\n",
      "5 2.3367406059713924\n",
      "10 0.16405112603131464\n",
      "15 0.02785946574865603\n",
      "20 0.04350489611719169\n",
      "Tanh()\n",
      "5 0.292508041157442\n",
      "10 0.06479799513723336\n",
      "15 0.014472410082817078\n",
      "20 0.013617006879226835\n",
      "Tanh()\n",
      "5 2.2707921196432674\n",
      "10 0.03641552083632525\n",
      "15 0.06428852735781203\n",
      "20 0.021212302002252315\n",
      "Tanh()\n",
      "5 1.1286706737443513\n",
      "10 0.3841301039153454\n",
      "15 0.19941460385042079\n",
      "20 0.020034256519055833\n",
      "Tanh()\n",
      "5 1.3111130957509958\n",
      "10 0.055251364614449294\n",
      "15 0.06224932857588226\n",
      "20 0.025136727912753235\n",
      "[1.50990495 0.22125272 0.04065241 0.0242845 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_boston(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    data = datasets.load_boston()\n",
    "\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    Y = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "    print(sigmoid_func_uniq)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1234)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    \n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "    \n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        net = NeurNet(13,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        net = LogReg(13,1)\n",
    "        \n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "        #                         betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "         #                                eps=config.get(\"eps\", 1e-08),\n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "         #                        betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "          #                               eps=config.get(\"eps\", 1e-08),\n",
    "                                                momentum=config.get(\"sigmoid_func\", 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        #acc = test(net,loss_func ,test_loader, 2)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        #tune.report(loss=acc)\n",
    "\n",
    "        if(i%5 == 4):\n",
    "            acc = test(net, loss_func, test_loader,2)\n",
    "            print(i+1,acc)\n",
    "            if(i==4):\n",
    "                vect[0]+=acc\n",
    "            if(i==9):\n",
    "                vect[1]+=acc\n",
    "            if(i==14):\n",
    "                vect[2]+=acc\n",
    "            if(i==19):\n",
    "                vect[3]+=acc\n",
    "            \n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.00010000 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.036228 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.014474,\n",
    "        \"hidden_dim\":217.70,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.6425,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.092147,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.66673,\n",
    "        \"model\":0.48513\n",
    "    } #AX\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.017316 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.045980 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.41114,\n",
    "        \"hidden_dim\":187.56,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.0675,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.17717,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.42023,\n",
    "        \"model\":0.73031\n",
    "    } #NG\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.014947 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.068342 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.39320,\n",
    "        \"hidden_dim\":38.714,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.3011,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.40286,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.42065,\n",
    "        \"model\":0.12685\n",
    "    } #Hyper\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.018554 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.092120 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.23375,\n",
    "        \"hidden_dim\":39.667,#,1), #log de 32 à 256\n",
    "        \"n_layer\":2.3554,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.35617,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.29537,\n",
    "        \"model\":0.34584\n",
    "    } #Bayes\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.037222 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.061591 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.76230,\n",
    "        \"hidden_dim\":122.24,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.9582,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.28435,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.66737,\n",
    "        \"model\":0.78779\n",
    "    } #Random\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0093919 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.092432 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.089576,\n",
    "        \"hidden_dim\":43.284,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.5384,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.49455,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.74291,\n",
    "        \"model\":0.11326\n",
    "    } #bohb\n",
    "\n",
    "\n",
    "config= {\n",
    "    \"steps\": 20,  # evaluation times\n",
    "     \"lr\":  0.0069439 ,#,1e-4), #*10\n",
    "         \"weight_decay\":0.057729 ,#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":0.56127,\n",
    "        \"hidden_dim\":83.163,#,1), #log de 32 à 256\n",
    "        \"n_layer\":1.2594,#,1), #from 1 to 3\n",
    "        \"droupout_prob\":0.46376,#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":0.34461,\n",
    "        \"model\":0.34481\n",
    "    } #bohb\n",
    "\n",
    "vect = np.zeros(4)\n",
    "for i in range(25):\n",
    "    train_boston(config)\n",
    "    \n",
    "print(vect/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "        \n",
    "    class SentimentRNN(nn.Module):\n",
    "        def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                 drop_prob, sigmoid , vocab_size):\n",
    "            super(SentimentRNN, self).__init__()\n",
    "            self.output_size = output_size\n",
    "            self.n_layers = n_layers\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                                dropout=drop_prob, batch_first=True)\n",
    "            #self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "            self.sig = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            hidden= tuple([each.data for each in hidden])\n",
    "            batch_size = x.size(0)\n",
    "            embeds = self.embedding(x)\n",
    "            lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "            # stack up lstm outputs\n",
    "            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "            # dropout and fully connected layer\n",
    "            out = self.dropout(lstm_out)\n",
    "            out = self.fc(out)\n",
    "\n",
    "            # sigmoid function\n",
    "            sig_out = self.sig(out)\n",
    "\n",
    "            # reshape to be batch_size first\n",
    "            sig_out = sig_out.view(batch_size, -1)\n",
    "            sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            return hidden\n",
    "        \n",
    "\n",
    "    class SentimentRNN1(nn.Module):\n",
    "        def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                 drop_prob, sigmoid , vocab_size):\n",
    "            super(SentimentRNN1, self).__init__()\n",
    "            self.output_size = output_size\n",
    "            self.n_layers = n_layers\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            #    dropout=drop_prob, batch_first=True)\n",
    "            self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "            self.sig = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            hidden = hidden.data\n",
    "            batch_size = x.size(0)\n",
    "            embeds = self.embedding(x)\n",
    "            lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "            # stack up lstm outputs\n",
    "            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "            # dropout and fully connected layer\n",
    "            out = self.dropout(lstm_out)\n",
    "            out = self.fc(out)\n",
    "\n",
    "            # sigmoid function\n",
    "            sig_out = self.sig(out)\n",
    "\n",
    "            # reshape to be batch_size first\n",
    "            sig_out = sig_out.view(batch_size, -1)\n",
    "            sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            (a,b) = hidden\n",
    "            return a\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = 400 \n",
    "    hidden_dim = int(round(config.get(\"hidden_dim\",64)))\n",
    "    n_layers =  2+ int( round(config.get(\"n_layer\",1)))\n",
    "\n",
    "\n",
    "    \n",
    "    net = GRUNet(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)    \n",
    "    net = SentimentRNN1(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)\n",
    "    \n",
    "    # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "   # print(*(n for n in net.parameters()))\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # training params\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 128\n",
    "    TEST_SIZE = 64\n",
    "\n",
    "    def train_rnn():\n",
    "\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(inputs) > EPOCH_SIZE:\n",
    "                return\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step() \n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        train_rnn()\n",
    "        acc = test(net,criterion,valid_loader,3)\n",
    "\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2462, 0.5757])\n"
     ]
    }
   ],
   "source": [
    "#Configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "        \n",
    "    \n",
    "    \n",
    "experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"min\")\n",
    "#experiment_metrics = dict(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "ITERATIONS = 4\n",
    "NUM_TUNED= 256\n",
    "    \n",
    "\n",
    "\n",
    "#[nn.ReLU, nn.Softmax(), nn.Tanh(),nn.Sigmoid() ]\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if True else 2,\n",
    "    \"config\": {\n",
    "    \"steps\": 3,  # evaluation times\n",
    "     \"lr\":  tune.quniform(1e-10, 0.1,1e-10),\n",
    "    \"b1\": tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"b2\":tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"eps\": tune.uniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.quniform(1e-10, 0.1,1e-10),\n",
    "        \"sigmoid_func\":nn.ReLU()\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-10, 0.1),\n",
    "    \"b1\": tune.loguniform(0.9, 1-1e-10),\n",
    "        \"b2\":tune.loguniform(0.9, 1-1e-10),\n",
    "        \"eps\": tune.loguniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "}\n",
    "   \n",
    "#i is in [0;1]\n",
    "#We want all values between 0 and 1\n",
    "def get_sigmoid_func(i):\n",
    "    if(i<0.33):\n",
    "        return nn.ReLU()\n",
    "    elif(i<0.67):\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "    \n",
    "optimizer_is_adam = True   \n",
    "    \n",
    "f = get_sigmoid_func(3)\n",
    "print(f(torch.randn(2)))\n",
    "import random\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.loguniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.loguniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One shot\n",
    "\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "if(1==1):\n",
    "    for i in range(5,6):\n",
    "        x = x_all[i]\n",
    "        #f_BayesOpt(x)\n",
    "        #f_AX(x)\n",
    "        #f_NeverGrad(x)\n",
    "        #f_BOHB(x)\n",
    "        #f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small budget\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 20\n",
    "\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "optimizer_is_adam = True\n",
    "if(0==1):\n",
    "    for i in range(1,2):\n",
    "        x = train_TREC\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BayesOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = HyperOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler, search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_AX(dataset):\n",
    "    \n",
    "   \n",
    "    if __name__ == \"__main__\":\n",
    "                \n",
    "        algo = AxSearch(\n",
    "            max_concurrent=2, #was working with 2\n",
    "            **experiment_metrics\n",
    "        )\n",
    "        scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "        tune.run(\n",
    "            dataset,\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            **tune_kwargs)\n",
    "\n",
    "        \n",
    "#        algo = AxSearch(\n",
    "#            **experiment_metrics\n",
    "#        )\n",
    "#        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        \n",
    "#        scheduler = AsyncHyperBandScheduler()\n",
    "#        tune.run(\n",
    "#            dataset,\n",
    "#            **experiment_metrics,\n",
    "#            search_alg=algo,\n",
    "#            scheduler=scheduler,\n",
    "#            **tune_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO We are interested in multiple Population based algorithms from nevergrad, and certainly not in OnePlusOne. \n",
    "def f_NeverGrad(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.CMA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BOHB(dataset):\n",
    "\n",
    "    bohb_hyperband = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=100,\n",
    "        reduction_factor=4,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    bohb_search = TuneBOHB(\n",
    "        # space=config_space, \n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "       # config=config, \n",
    "        scheduler=bohb_hyperband,\n",
    "        search_alg=bohb_search,\n",
    "         **tune_kwargs)\n",
    "        #num_samples=NUM_TUNED,\n",
    "       # stop={\"training_iteration\": 100})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Random(dataset):\n",
    "    \n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ZOOpt(dataset):\n",
    "\n",
    "    dim_dict = {\n",
    "        \"lr\": (ValueType.CONTINUOUS, [0, 1], 1e-2),\n",
    "        \"momentum\": (ValueType.CONTINUOUS, [0,1, 0.9], 1e-2)\n",
    "    }\n",
    "\n",
    "    zoopt_search_config = {\n",
    "        \"parallel_num\": 8,  # how many workers to parallel\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    zoopt_search = ZOOptSearch(\n",
    "    algo=\"Asracos\",  # only support Asracos currently\n",
    "    #dim_dict=dim_dict,\n",
    "    budget=ITERATIONS,\n",
    "    #dim_dict=dim_dict,\n",
    "   #     **zoopt_search_config,\n",
    "    **experiment_metrics)\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "\n",
    "   \n",
    "    tune.run(dataset,\n",
    " #        config = config,\n",
    "    search_alg=zoopt_search,\n",
    "   # num_samples= ITERATIONS,\n",
    "    scheduler=scheduler,\n",
    "    #         paralell_num=4,\n",
    "    name=\"zoopt_search\",\n",
    "              **tune_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "\n",
    "def GAN_MNIST(SA):\n",
    "    import ray\n",
    "\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.utils.data\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "    import torchvision.utils as vutils\n",
    "    import numpy as np\n",
    "\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.trial import ExportFormat\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    import argparse\n",
    "    import os\n",
    "    from filelock import FileLock\n",
    "    import random\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data\n",
    "    import numpy as np\n",
    "    from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "    from ray.tune.suggest.ax import AxSearch\n",
    "\n",
    "\n",
    "\n",
    "    from torch.autograd import Variable\n",
    "    from torch.nn import functional as F\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "    # Training parameters\n",
    "    dataroot = ray.utils.get_user_temp_dir() + os.sep\n",
    "    workers = 2\n",
    "    batch_size = 64\n",
    "    image_size = 32\n",
    "\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 32\n",
    "\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 32\n",
    "\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # iterations of actual training in each Trainable _train\n",
    "    train_iterations_per_step = 5\n",
    "\n",
    "    MODEL_PATH = os.path.expanduser(\"~/.ray/models/mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "    def get_data_loader():\n",
    "        dataset = dset.MNIST(\n",
    "            root=dataroot,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, ), (0.5, )),\n",
    "            ]))\n",
    "\n",
    "        # Create the dataloader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "    # __GANmodel_begin__\n",
    "    # custom weights initialization called on netG and netD\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find(\"Conv\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find(\"BatchNorm\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "    # Generator Code\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    # __GANmodel_end__\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_begin__\n",
    "    class Net(nn.Module):\n",
    "        \"\"\"\n",
    "        LeNet for MNist classification, used for inception_score\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def inception_score(imgs, mnist_model_ref, batch_size=32, splits=1):\n",
    "        N = len(imgs)\n",
    "        dtype = torch.FloatTensor\n",
    "        dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "        cm = ray.get(mnist_model_ref)  # Get the mnist model from Ray object store.\n",
    "        up = nn.Upsample(size=(28, 28), mode=\"bilinear\").type(dtype)\n",
    "\n",
    "        def get_pred(x):\n",
    "            x = up(x)\n",
    "            x = cm(x)\n",
    "            return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "        preds = np.zeros((N, 10))\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            batch = batch.type(dtype)\n",
    "            batchv = Variable(batch)\n",
    "            batch_size_i = batch.size()[0]\n",
    "            preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "        # Now compute the mean kl-div\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (N // splits):(k + 1) * (N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "        return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_end__\n",
    "\n",
    "\n",
    "    def train(netD, netG, optimG, optimD, criterion, dataloader, iteration, device,\n",
    "              mnist_model_ref):\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            if i >= train_iterations_per_step:\n",
    "                break\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full(\n",
    "                (b_size, ), real_label, dtype=torch.float, device=device)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimG.step()\n",
    "\n",
    "            is_score, is_std = inception_score(fake, mnist_model_ref)\n",
    "\n",
    "            # Output training stats\n",
    "            if iteration % 10 == 0:\n",
    "                print(\"[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z))\"\n",
    "                      \": %.4f / %.4f \\tInception score: %.4f\" %\n",
    "                      (iteration, len(dataloader), errD.item(), errG.item(), D_x,\n",
    "                       D_G_z1, D_G_z2, is_score))\n",
    "\n",
    "        return errG.item(), errD.item(), is_score\n",
    "\n",
    "\n",
    "    def plot_images(dataloader):\n",
    "        # Plot some training images\n",
    "        real_batch = next(iter(dataloader))\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Original Images\")\n",
    "        plt.imshow(\n",
    "            np.transpose(\n",
    "                vutils.make_grid(real_batch[0][:64], padding=2,\n",
    "                                 normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def demo_gan(checkpoint_paths):\n",
    "        img_list = []\n",
    "        fixed_noise = torch.randn(64, nz, 1, 1)\n",
    "        for netG_path in checkpoint_paths:\n",
    "            loadedG = Generator()\n",
    "            loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])\n",
    "            with torch.no_grad():\n",
    "                fake = loadedG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)]\n",
    "               for i in img_list]\n",
    "        ani = animation.ArtistAnimation(\n",
    "            fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "        ani.save(\"./generated.gif\", writer=\"imagemagick\", dpi=72)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # __Trainable_begin__\n",
    "    class PytorchTrainable(tune.Trainable):\n",
    "        def setup(self, config):\n",
    "            use_cuda = config.get(\"use_gpu\") and torch.cuda.is_available()\n",
    "            self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            self.netD = Discriminator().to(self.device)\n",
    "            self.netD.apply(weights_init)\n",
    "            self.netG = Generator().to(self.device)\n",
    "            self.netG.apply(weights_init)\n",
    "            self.criterion = nn.BCELoss()\n",
    "            self.optimizerD = optim.Adam(\n",
    "                self.netD.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            self.optimizerG = optim.Adam(\n",
    "                self.netG.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "                self.dataloader = get_data_loader()\n",
    "            self.mnist_model_ref = c[\"mnist_model_ref\"]\n",
    "\n",
    "        def step(self):\n",
    "            lossG, lossD, is_score = train(self.netD, self.netG, self.optimizerG,\n",
    "                                           self.optimizerD, self.criterion,\n",
    "                                           self.dataloader, self._iteration,\n",
    "                                           self.device, self.mnist_model_ref)\n",
    "            return {\"lossg\": lossG, \"lossd\": lossD, \"is_score\": is_score}\n",
    "\n",
    "        def save_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\n",
    "                \"netDmodel\": self.netD.state_dict(),\n",
    "                \"netGmodel\": self.netG.state_dict(),\n",
    "                \"optimD\": self.optimizerD.state_dict(),\n",
    "                \"optimG\": self.optimizerG.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "            return checkpoint_dir\n",
    "\n",
    "        def load_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            checkpoint = torch.load(path)\n",
    "            self.netD.load_state_dict(checkpoint[\"netDmodel\"])\n",
    "            self.netG.load_state_dict(checkpoint[\"netGmodel\"])\n",
    "            self.optimizerD.load_state_dict(checkpoint[\"optimD\"])\n",
    "            self.optimizerG.load_state_dict(checkpoint[\"optimG\"])\n",
    "\n",
    "        def reset_config(self, new_config):\n",
    "            if \"netD_lr\" in new_config:\n",
    "                for param_group in self.optimizerD.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netD_lr\"]\n",
    "            if \"netG_lr\" in new_config:\n",
    "                for param_group in self.optimizerG.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netG_lr\"]\n",
    "\n",
    "            self.config = new_config\n",
    "            return True\n",
    "\n",
    "        def _export_model(self, export_formats, export_dir):\n",
    "            if export_formats == [ExportFormat.MODEL]:\n",
    "                path = os.path.join(export_dir, \"exported_models\")\n",
    "                torch.save({\n",
    "                    \"netDmodel\": self.netD.state_dict(),\n",
    "                    \"netGmodel\": self.netG.state_dict()\n",
    "                }, path)\n",
    "                return {ExportFormat.MODEL: path}\n",
    "            else:\n",
    "                raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "\n",
    "\n",
    "    import urllib.request\n",
    "    # Download a pre-trained MNIST model for inception score calculation.\n",
    "    # This is a tiny model (<100kb).\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"downloading model\")\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://github.com/ray-project/ray/raw/master/python/ray/tune/\"\n",
    "            \"examples/pbt_dcgan_mnist/mnist_cnn.pt\", MODEL_PATH)\n",
    "\n",
    "    dataloader = get_data_loader()\n",
    "    if not args.smoke_test:\n",
    "        plot_images(dataloader)\n",
    "\n",
    "    # load the pretrained mnist classification model for inception_score\n",
    "    mnist_cnn = Net()\n",
    "    mnist_cnn.load_state_dict(torch.load(MODEL_PATH))\n",
    "    mnist_cnn.eval()\n",
    "    mnist_model_ref = ray.put(mnist_cnn)\n",
    "\n",
    "    # __tune_begin__\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"is_score\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"netG_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "            \"netD_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "        })\n",
    "\n",
    "\n",
    "    experiment_metrics= dict(metric=\"is_score\",\n",
    "        mode=\"max\")\n",
    "\n",
    "   \n",
    "    dim_dict = {\n",
    "        \"netG_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2),\n",
    "        \"netD_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2)\n",
    "    }\n",
    "\n",
    "    config =     {\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "           \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        }\n",
    "\n",
    "    algo = NevergradSearch(optimizer=ng.optimizers.OnePlusOne)\n",
    "\n",
    "    \n",
    "    tune_iter = 5 if args.smoke_test else 1\n",
    "    c={\"mnist_model_ref\" : mnist_model_ref}\n",
    "    \n",
    "    \n",
    "    if(SA==1):\n",
    "        algo =   BayesOptSearch(**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "    if(SA==2):\n",
    "        algo =  AxSearch(\n",
    "            **experiment_metrics) \n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-3, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-3, 0.1)\n",
    "\n",
    "\n",
    "        })\n",
    "        \n",
    "        \n",
    "    if(SA==3):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.OnePlusOne,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "     \n",
    "    if(SA==4):\n",
    "        bohb_hyperband = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=100,\n",
    "            reduction_factor=4,\n",
    "            **experiment_metrics)\n",
    "\n",
    "        bohb_search = TuneBOHB(\n",
    "            # space=config_space, \n",
    "            max_concurrent=4,\n",
    "            **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=bohb_hyperband,\n",
    "        reuse_actors=True,\n",
    "        search_alg=bohb_search,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "    if(SA==5):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "\n",
    "        \n",
    "    if(SA==6):\n",
    "        algo=  ZOOptSearch(\n",
    "                algo=\"Asracos\",  # only support Asracos currently\n",
    "                dim_dict=dim_dict,\n",
    "                budget=10,\n",
    "                #dim_dict=dim_dict,\n",
    "                **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "            PytorchTrainable,\n",
    "            name=\"pbt_dcgan_mnist\",\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            search_alg=algo,\n",
    "            verbose=1,\n",
    "            checkpoint_at_end=True,\n",
    "            stop={\n",
    "                \"training_iteration\": tune_iter,\n",
    "            },\n",
    "            num_samples=8,\n",
    "            export_formats=[ExportFormat.MODEL],\n",
    "            config=dim_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#Classification:\n",
    "\n",
    "metrics.accuracy_score()\n",
    "metrics.f1_score\n",
    "metrics.log_loss\n",
    "metrics.precision_score\n",
    "metrics.recall_score\n",
    "\n",
    "#Regression\n",
    "mean_absolute_error\n",
    "mean_squared_error\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
