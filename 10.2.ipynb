{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FNN : https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "\n",
    "\n",
    "class Net42(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net42, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = sigmoid_func_uniq(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return (x)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        denom = ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        if torch.isinf(denom).sum().item()>0:\n",
    "              output_tensor = input_tensor / torch.sqrt(squared_norm)\n",
    "        else:\n",
    "              output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "    \n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    \n",
    "#https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/    \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.input_d = input_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_d)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class NeurNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(NeurNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for _ in range(self.n_layers):\n",
    "            x=self.hidden(x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,sigmoid = F.log_softmax):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        return sigmoid(x)\n",
    "    \n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, cols, size_hidden, n_output):\n",
    "        super(NetG, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 512*32*2*32*32\n",
    "TEST_SIZE = 256*32\n",
    "\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "def test(model, func, data_loader, clas):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if clas == 3:\n",
    "        val_h = model.init_hidden(50) #batch size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            if(clas == 1): #classification\n",
    "                # We set this just for the example to run quickly.\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "            if(clas == 2): #regression\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                from torch.autograd import Variable\n",
    "\n",
    "                X = Variable(torch.FloatTensor(data)) \n",
    "                result = model(X)\n",
    "                pred=result.data[:,0].numpy()\n",
    "                out = target.data[:,0].numpy()\n",
    "                #print( pred)\n",
    "                #print(out)\n",
    "                #pred.fillna(X_train.mean(), inplace=True)\n",
    "                total += target.size(0)\n",
    "                #correct += r2_score(pred,out)\n",
    "                correct+=func(result,target).numpy()   \n",
    "            if(clas == 3): #RNN\n",
    "                val_losses = []\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                #if(train_on_gpu): FALSE\n",
    "                #    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output, val_h = model(data, val_h)\n",
    "                print(output)\n",
    "\n",
    "                val_loss = func(output.squeeze(), target.float())\n",
    "                print(val_loss)\n",
    "                val_losses.append(val_loss.item())\n",
    "                print(val_losses)\n",
    "               # model.train()\n",
    "                correct += np.mean(val_losses)\n",
    "                total += 1;\n",
    "                \n",
    "            if(clas == 4): #binary class\n",
    "                val_losses = []\n",
    "\n",
    "                output = model(data)\n",
    "                print(output)\n",
    "                val_loss = func(output.squeeze(), target.float())\n",
    "                print(val_loss)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "               # model.train()\n",
    "                correct += np.mean(val_losses)\n",
    "                total += 1;\n",
    "                \n",
    "    return correct / total\n",
    "\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        model = NeurNet(784,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "\n",
    "   # optimizer = optim.SGD(    model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader)\n",
    "        acc = test(model, F.nll_loss, test_loader,1)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision                                                       \n",
    "import torchvision.transforms as transforms\n",
    "#https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html\n",
    "import os.path        \n",
    "\n",
    "def train_fashion_mnist(config):\n",
    "    from keras.datasets import fashion_mnist\n",
    "    dataset_dir = os.path.join(os.path.expanduser(\"~\"), 'Datasets', 'FashionMNIST')\n",
    "    valid_ratio = 0.2  # Going to use 80%/20% split for train/valid\n",
    "\n",
    "    # Load the dataset for the training/validation sets\n",
    "    train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                           train=True,\n",
    "                                           transform= None, #transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "    # Split it into training and validation sets\n",
    "    nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "    nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "\n",
    "\n",
    "    # Load the test set\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir,\n",
    "                                                     transform= None, #transforms.ToTensor(),\n",
    "                                                    train=False)\n",
    "    \n",
    "    class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, base_dataset, transform):\n",
    "            self.base_dataset = base_dataset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img, target = self.base_dataset[index]\n",
    "            return self.transform(img), target\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "\n",
    "\n",
    "    train_dataset = DatasetTransformer(train_dataset, transforms.ToTensor())\n",
    "    valid_dataset = DatasetTransformer(valid_dataset, transforms.ToTensor())\n",
    "    test_dataset  = DatasetTransformer(test_dataset , transforms.ToTensor())\n",
    "    ############################################################################################ Dataloaders\n",
    "    num_threads = 4     # Loading the dataset is using 4 CPU threads\n",
    "    batch_size  = 512*8   # Using minibatches of 128 samples\n",
    "\n",
    "    train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,                # <-- this reshuffles the data at every epoch\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    valid_loader1 = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "\n",
    "    test_loader1 = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_threads)\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        model = ConvNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        model = NeurNet(784,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "        \n",
    "   # optimizer = optim.SGD(    model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    if(config.get(\"adam\",1) >= 0.5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    " #                                           eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                             #    betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                  #                       eps=config.get(\"eps\", 1e-08),\n",
    "                                                weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    for i in range(config.get(\"steps\",10)):\n",
    "        train(model, optimizer,F.nll_loss ,train_loader1)\n",
    "        acc = test(model, F.nll_loss, test_loader1,1)\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_TREC(config):\n",
    "    import torch\n",
    "    from torchtext import data\n",
    "    from torchtext import datasets\n",
    "    import random\n",
    "\n",
    "    SEED = 1234\n",
    "    savedPath = os.getcwd()\n",
    "    os.chdir('/home/antoine/Projet/NovelTuning')\n",
    "    \n",
    "    \n",
    "    #torch.manual_seed(SEED)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    TEXT = data.Field(tokenize = 'spacy')\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    train_data, test_data = datasets.TREC.splits(TEXT, LABEL,root='data/trec', fine_grained=False)\n",
    "\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    \n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.100d', \n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    os.chdir(savedPath)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                     dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            self.convs = nn.ModuleList([\n",
    "                                        nn.Conv2d(in_channels = 1, \n",
    "                                                  out_channels = n_filters, \n",
    "                                                  kernel_size = (fs, embedding_dim)) \n",
    "                                        for fs in filter_sizes\n",
    "                                        ])\n",
    "\n",
    "            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text):\n",
    "\n",
    "            #text = [sent len, batch size]\n",
    "\n",
    "            text = text.permute(1, 0)\n",
    "\n",
    "            #text = [batch size, sent len]\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "            conved = [sigmoid_func_uniq(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "            #pooled_n = [batch size, n_filters]\n",
    "\n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "            return self.fc(cat)\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    " #   print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    #optimizer = optim.Adam(model.parameters())\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    def categorical_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text)\n",
    "\n",
    "                loss = criterion(predictions, batch.label)\n",
    "\n",
    "                acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    import time\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for e in range(ITERATIONS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        tune.report(loss=valid_loss)\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    " #       print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "  #      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_diabetes(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "\n",
    "    from sklearn.datasets import load_diabetes\n",
    "\n",
    "    (X,Y) = load_diabetes( return_X_y=True, as_frame=True)\n",
    "    X = pd.DataFrame(X)\n",
    "    Y = pd.DataFrame(Y)\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1234)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    #print(y_train)\n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "\n",
    "    net = NeurNet(10,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    \n",
    "    net = LogReg(10,1)\n",
    "        \n",
    "   # optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                 betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                         eps=config.get(\"eps\", 1e-08), weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "        \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(ITERATIONS):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        acc = test(net,loss_func ,test_loader, 2)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_boston(config):\n",
    "    import numpy as  np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    data = datasets.load_boston()\n",
    "\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    Y = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "\n",
    "    #normalizing\n",
    "    Y= Y.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "    print(sigmoid_func_uniq)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1234)\n",
    "    X_test,X_last,y_test,y_last = train_test_split(X_test, y_test, test_size=0.50, random_state=1234)\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "    x_last = torch.tensor(X_last.values, dtype=torch.float)\n",
    "    y_last = torch.tensor(y_last.values, dtype=torch.float)\n",
    "  #  y_train = y_train.type(torch.LongTensor)\n",
    "  #  y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "    \n",
    "    train_datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=100, shuffle=True)\n",
    "    \n",
    "    test_datasets = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=100, shuffle=True)  \n",
    "       \n",
    "    last_datasets = torch.utils.data.TensorDataset(x_last, y_last)\n",
    "    last_loader = torch.utils.data.DataLoader(last_datasets, batch_size=100, shuffle=True)  \n",
    "    \n",
    "    \n",
    "    if(config.get(\"model\", 0.4)<0.5):\n",
    "        net = NeurNet(13,int(round(config.get(\"hidden_dim\",64))),1,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.1) ,sigmoid_func_uniq)\n",
    "    else:\n",
    "        net = LogReg(13,1)\n",
    "        \n",
    "    if(optimizer_is_adam == True):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "        #                         betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "         #                                eps=config.get(\"eps\", 1e-08),\n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "         #                        betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "          #                               eps=config.get(\"eps\", 1e-08),\n",
    "                                                momentum=config.get(\"sigmoid_func\", 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    for i in range(ITERATIONS):\n",
    "        train(net, optimizer,loss_func, train_loader)\n",
    "        acc = test(net,loss_func ,test_loader, 2)\n",
    "        test1= test(net,loss_func ,last_loader, 2)\n",
    "        # Send the current training result back to Tune\n",
    "        \n",
    "        tune.report(loss=acc,mean_accuracy=test1)\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "             drop_prob, sigmoid , vocab_size):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        #self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden= tuple([each.data for each in hidden])\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class SentimentRNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "             drop_prob, sigmoid , vocab_size):\n",
    "        super(SentimentRNN1, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                        #    dropout=drop_prob, batch_first=True)\n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden = hidden.data\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        (a,b) = hidden\n",
    "        return a\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx,sigmoid):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "       # text = text.permute(1, 0)\n",
    "        #We want already have batch size, len for sentiment!!!!!\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "        conved = [self.sigmoid(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        sig_out = self.sig(self.fc(cat))\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(50, -1)\n",
    "        #sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return np.squeeze(sig_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=AVzirwGqpmva\n",
    "def train_IMDB(config):\n",
    "    train_x = np.load('/home/antoine/Projet/NovelTuning/train_x.npy')\n",
    "    train_y = np.load('/home/antoine/Projet/NovelTuning/train_y.npy')\n",
    "    val_x = np.load('/home/antoine/Projet/NovelTuning/val_x.npy')\n",
    "    val_y = np.load('/home/antoine/Projet/NovelTuning/val_y.npy')\n",
    "    test_x = np.load('/home/antoine/Projet/NovelTuning/test_x.npy')\n",
    "    test_y = np.load('/home/antoine/Projet/NovelTuning/test_y.npy')\n",
    "    len_vocab_to_int = 74072\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    import torch\n",
    "\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    # obtain one batch of training data\n",
    "    dataiter = iter(train_loader)\n",
    "    sample_x, sample_y = dataiter.next()\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    sigmoid_func_uniq = get_sigmoid_func(config.get(\"sigmoid_func\", 0))\n",
    "\n",
    "        \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Instantiate the model w/ hyperparams\n",
    "    vocab_size = len_vocab_to_int + 1 # +1 for zero padding + our word tokens\n",
    "    output_size = 1\n",
    "    embedding_dim = int(config.get(\"embedding\",600))\n",
    "    hidden_dim = int(round(config.get(\"hidden_dim\",64)))\n",
    "    n_layers =  2+ int( round(config.get(\"n_layer\",1)))\n",
    "\n",
    "\n",
    "    INPUT_DIM = 7503\n",
    "    EMBEDDING_DIM = 100\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "   # OUTPUT_DIM = len(LABEL.vocab)\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = 4    \n",
    "    cnn = 0 \n",
    "    if(config.get(\"model\",0)<1/3):\n",
    "\n",
    "        net = SentimentRNN(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)    \n",
    "    elif(config.get(\"model\",0)<2/3):\n",
    "        net = SentimentRNN1(embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                           config.get(\"droupout_prob\",0.1),\n",
    "                           sigmoid_func_uniq, vocab_size)\n",
    "    else:\n",
    "        cnn = 1;\n",
    "        net = CNN(vocab_size, embedding_dim, hidden_dim, FILTER_SIZES, \n",
    "              output_size, config.get(\"droupout_prob\",0.1), PAD_IDX,sigmoid_func_uniq)\n",
    "    # loss and optimization functions\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "   # print(*(n for n in net.parameters()))\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # training params\n",
    "    if(config.get(\"adam\",0)>0.5):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "           weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                         amsgrad=True)\n",
    "    else: \n",
    "        optimizer = adabelief_pytorch.AdaBelief(net.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                              weight_decay=config.get(\"weight_decay\", 0))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    counter = 0\n",
    "    print_every = 1\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    EPOCH_SIZE = 32 *4 *8\n",
    "    TEST_SIZE = 32 *2 * 4\n",
    "\n",
    "    def train_rnn():\n",
    "        \n",
    "        h = net.init_hidden(batch_size)\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(inputs) > EPOCH_SIZE:\n",
    "                return\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step() \n",
    "\n",
    "    def train_cnn(model, optimizer ,func ,train_loader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.train()\n",
    "        #for (data, target) in train_loader:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(data) > EPOCH_SIZE:\n",
    "               # print(\"hehe\")\n",
    "                return\n",
    "\n",
    "            # We set this just for the example to run quickly.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = func(output.squeeze(), target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    net.train()\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        if(cnn==0):\n",
    "\n",
    "            train_rnn()\n",
    "        else:\n",
    "            train_cnn(net,optimizer,criterion,train_loader)\n",
    "        \n",
    "        acc = test(net,criterion,valid_loader,3+cnn)\n",
    "\n",
    "        tune.report(loss=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(net.state_dict(), \"./model.pth\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4240, 0.3995])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 8 (7 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7540e272</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25242)\u001b[0m Sigmoid()\n",
      "Result for train_boston_7540e272:\n",
      "  date: 2020-11-26_12-49-45\n",
      "  done: false\n",
      "  experiment_id: 2dcf7f17539c46758bfe860f00f22186\n",
      "  experiment_tag: 1_adam=0.18842,droupout_prob=0.26055,hidden_dim=166.07,lr=0.054793,model=0.6167,n_layer=2.1625,sigmoid_func=0.72493,steps=20,weight_decay=0.054622\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 28.872250205592106\n",
      "  mean_accuracy: 26.09192697625411\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25242\n",
      "  time_since_restore: 3.132594108581543\n",
      "  time_this_iter_s: 3.132594108581543\n",
      "  time_total_s: 3.132594108581543\n",
      "  timestamp: 1606391385\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7540e272\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -28.872250205592106<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 8 (8 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7540e272</td><td>RUNNING </td><td>192.168.1.34:25242</td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">26.0919</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13259</td><td style=\"text-align: right;\">28.8723</td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25189)\u001b[0m ReLU()\n",
      "Result for train_boston_754e6294:\n",
      "  date: 2020-11-26_12-49-45\n",
      "  done: false\n",
      "  experiment_id: c1d66e1748a54a4abf0f9039ef665d26\n",
      "  experiment_tag: 3_adam=0.25619,droupout_prob=0.26128,hidden_dim=147.65,lr=0.029644,model=0.86544,n_layer=2.0152,sigmoid_func=0.2136,steps=20,weight_decay=0.03492\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 22.665095279091283\n",
      "  mean_accuracy: 24.355457506681745\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25189\n",
      "  time_since_restore: 2.9548025131225586\n",
      "  time_this_iter_s: 2.9548025131225586\n",
      "  time_total_s: 2.9548025131225586\n",
      "  timestamp: 1606391385\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 754e6294\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25240)\u001b[0m Tanh()\n",
      "Result for train_boston_7548ef44:\n",
      "  date: 2020-11-26_12-49-45\n",
      "  done: true\n",
      "  experiment_id: d1550539ac8c4f039abab38c7afdbb34\n",
      "  experiment_tag: 2_adam=0.45412,droupout_prob=0.20246,hidden_dim=128.5,lr=0.038121,model=0.48651,n_layer=2.3725,sigmoid_func=0.60529,steps=20,weight_decay=0.051388\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 272.0439453125\n",
      "  mean_accuracy: 254.4489103618421\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25240\n",
      "  time_since_restore: 3.1920218467712402\n",
      "  time_this_iter_s: 3.1920218467712402\n",
      "  time_total_s: 3.1920218467712402\n",
      "  timestamp: 1606391385\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7548ef44\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25171)\u001b[0m Tanh()\n",
      "Result for train_boston_75511af2:\n",
      "  date: 2020-11-26_12-49-46\n",
      "  done: false\n",
      "  experiment_id: b124b5f4ac9645cda6de83d713a40373\n",
      "  experiment_tag: 4_adam=0.34311,droupout_prob=0.22037,hidden_dim=75.436,lr=0.03328,model=0.53437,n_layer=2.3314,sigmoid_func=0.45671,steps=20,weight_decay=0.044958\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 12.422147248920641\n",
      "  mean_accuracy: 13.35881925884046\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25171\n",
      "  time_since_restore: 2.835685968399048\n",
      "  time_this_iter_s: 2.835685968399048\n",
      "  time_total_s: 2.835685968399048\n",
      "  timestamp: 1606391386\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 75511af2\n",
      "  \n",
      "Result for train_boston_7540e272:\n",
      "  date: 2020-11-26_12-49-47\n",
      "  done: true\n",
      "  experiment_id: 2dcf7f17539c46758bfe860f00f22186\n",
      "  experiment_tag: 1_adam=0.18842,droupout_prob=0.26055,hidden_dim=166.07,lr=0.054793,model=0.6167,n_layer=2.1625,sigmoid_func=0.72493,steps=20,weight_decay=0.054622\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.5626147922716642\n",
      "  mean_accuracy: 0.4694863369590358\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25242\n",
      "  time_since_restore: 5.314145803451538\n",
      "  time_this_iter_s: 0.14194798469543457\n",
      "  time_total_s: 5.314145803451538\n",
      "  timestamp: 1606391387\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: 7540e272\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23250)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=23244)\u001b[0m Tanh()\n",
      "Result for train_boston_755c2078:\n",
      "  date: 2020-11-26_12-49-50\n",
      "  done: true\n",
      "  experiment_id: 3fb84135c79245a1a262a3aed1caf901\n",
      "  experiment_tag: 7_adam=0.55152,droupout_prob=0.2234,hidden_dim=111.73,lr=0.070139,model=0.40159,n_layer=1.7094,sigmoid_func=0.48047,steps=20,weight_decay=0.047603\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 89648.7302631579\n",
      "  mean_accuracy: 83714.96710526316\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23250\n",
      "  time_since_restore: 2.6393826007843018\n",
      "  time_this_iter_s: 2.6393826007843018\n",
      "  time_total_s: 2.6393826007843018\n",
      "  timestamp: 1606391390\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 755c2078\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23246)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=23248)\u001b[0m Tanh()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using AsyncHyperBand: num_stopped=3\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.0731958684168364 | Iter 4.000: -1.6401167417827405 | Iter 1.000: -22.665095279091283<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 13 (8 RUNNING, 5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">          acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">         loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">    0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">    0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">  254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">  272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">    0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">    0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">    0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">    0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\">83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\">89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">             </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_75549844:\n",
      "  date: 2020-11-26_12-49-50\n",
      "  done: true\n",
      "  experiment_id: 9ca6832428094d498f4fa3117c616e61\n",
      "  experiment_tag: 5_adam=0.56584,droupout_prob=0.26292,hidden_dim=158.79,lr=0.053956,model=0.38621,n_layer=2.0102,sigmoid_func=0.5986,steps=20,weight_decay=0.049796\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 304622.0789473684\n",
      "  mean_accuracy: 284562.94736842107\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23246\n",
      "  time_since_restore: 3.1458687782287598\n",
      "  time_this_iter_s: 3.1458687782287598\n",
      "  time_total_s: 3.1458687782287598\n",
      "  timestamp: 1606391390\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '75549844'\n",
      "  \n",
      "Result for train_boston_7558c3a6:\n",
      "  date: 2020-11-26_12-49-50\n",
      "  done: false\n",
      "  experiment_id: 207f7177031742c69140175c858035ab\n",
      "  experiment_tag: 6_adam=0.42081,droupout_prob=0.11936,hidden_dim=134.88,lr=0.026123,model=0.5183,n_layer=1.9877,sigmoid_func=0.38866,steps=20,weight_decay=0.075827\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4.8074597810444075\n",
      "  mean_accuracy: 4.337734423185649\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23244\n",
      "  time_since_restore: 3.030021905899048\n",
      "  time_this_iter_s: 3.030021905899048\n",
      "  time_total_s: 3.030021905899048\n",
      "  timestamp: 1606391390\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7558c3a6\n",
      "  \n",
      "Result for train_boston_755e3c1e:\n",
      "  date: 2020-11-26_12-49-50\n",
      "  done: true\n",
      "  experiment_id: 07c4dd1606b24eaa874b07f8a3d7bd8b\n",
      "  experiment_tag: 8_adam=0.51818,droupout_prob=0.42707,hidden_dim=115.09,lr=0.052682,model=0.4742,n_layer=1.7485,sigmoid_func=0.55107,steps=20,weight_decay=0.094306\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 20156.61019736842\n",
      "  mean_accuracy: 20169.615131578947\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23248\n",
      "  time_since_restore: 3.069889783859253\n",
      "  time_this_iter_s: 3.069889783859253\n",
      "  time_total_s: 3.069889783859253\n",
      "  timestamp: 1606391390\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 755e3c1e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23314)\u001b[0m Tanh()\n",
      "Result for train_boston_7ac548a0:\n",
      "  date: 2020-11-26_12-49-54\n",
      "  done: true\n",
      "  experiment_id: 699c7b3163b448919913b69783dc33d2\n",
      "  experiment_tag: 9_adam=0.55635,droupout_prob=0.26029,hidden_dim=98.747,lr=0.058442,model=0.59121,n_layer=2.0022,sigmoid_func=0.45047,steps=20,weight_decay=0.057159\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 32.11888042249178\n",
      "  mean_accuracy: 28.0843505859375\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23314\n",
      "  time_since_restore: 3.3702893257141113\n",
      "  time_this_iter_s: 3.3702893257141113\n",
      "  time_total_s: 3.3702893257141113\n",
      "  timestamp: 1606391394\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7ac548a0\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23346)\u001b[0m Sigmoid()\n",
      "\u001b[2m\u001b[36m(pid=23329)\u001b[0m Sigmoid()\n",
      "Result for train_boston_7c03b06c:\n",
      "  date: 2020-11-26_12-49-57\n",
      "  done: true\n",
      "  experiment_id: 5ce0a0c303224864baf174dad18899c9\n",
      "  experiment_tag: 11_adam=0.47012,droupout_prob=0.28787,hidden_dim=159.5,lr=0.094885,model=0.6629,n_layer=1.6981,sigmoid_func=0.84562,steps=20,weight_decay=0.027036\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 29.490555612664473\n",
      "  mean_accuracy: 27.959617213199014\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23346\n",
      "  time_since_restore: 3.1395676136016846\n",
      "  time_this_iter_s: 3.1395676136016846\n",
      "  time_total_s: 3.1395676136016846\n",
      "  timestamp: 1606391397\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7c03b06c\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23313)\u001b[0m ReLU()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=7\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.04046080143828141 | Iter 4.000: -1.2858483414900932 | Iter 1.000: -24.21688401071649<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 17 (7 RUNNING, 10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7d80f670</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.475309</td><td style=\"text-align: right;\">       0.306478</td><td style=\"text-align: right;\">    152.23  </td><td style=\"text-align: right;\">0.0375412</td><td style=\"text-align: right;\">0.73664 </td><td style=\"text-align: right;\">  1.73359</td><td style=\"text-align: right;\">      0.77795 </td><td style=\"text-align: right;\">     0.0482804</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7da39392</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.369321</td><td style=\"text-align: right;\">       0.316607</td><td style=\"text-align: right;\">    247.265 </td><td style=\"text-align: right;\">0.0839222</td><td style=\"text-align: right;\">0.224336</td><td style=\"text-align: right;\">  1.52021</td><td style=\"text-align: right;\">      0.929015</td><td style=\"text-align: right;\">     0.0712747</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7eceaf9a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.575254</td><td style=\"text-align: right;\">       0.286455</td><td style=\"text-align: right;\">    136.97  </td><td style=\"text-align: right;\">0.0563368</td><td style=\"text-align: right;\">0.542539</td><td style=\"text-align: right;\">  2.26409</td><td style=\"text-align: right;\">      0.511323</td><td style=\"text-align: right;\">     0.0503148</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_8025da8a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.305203</td><td style=\"text-align: right;\">       0.343845</td><td style=\"text-align: right;\">    179.556 </td><td style=\"text-align: right;\">0.0699987</td><td style=\"text-align: right;\">0.30811 </td><td style=\"text-align: right;\">  1.91827</td><td style=\"text-align: right;\">      0.228793</td><td style=\"text-align: right;\">     0.0543419</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_7bbee518:\n",
      "  date: 2020-11-26_12-49-57\n",
      "  done: true\n",
      "  experiment_id: c1486a276c874aca8c1b865458c5f936\n",
      "  experiment_tag: 10_adam=0.58564,droupout_prob=0.18199,hidden_dim=165.97,lr=0.035222,model=0.29529,n_layer=1.9169,sigmoid_func=0.70654,steps=20,weight_decay=0.052032\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1411.518400493421\n",
      "  mean_accuracy: 1232.4998972039473\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23329\n",
      "  time_since_restore: 3.365123748779297\n",
      "  time_this_iter_s: 3.365123748779297\n",
      "  time_total_s: 3.365123748779297\n",
      "  timestamp: 1606391397\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7bbee518\n",
      "  \n",
      "Result for train_boston_7c69811c:\n",
      "  date: 2020-11-26_12-49-57\n",
      "  done: false\n",
      "  experiment_id: 933ddacd443d4e6489845a991697b037\n",
      "  experiment_tag: 12_adam=0.76925,droupout_prob=0.30648,hidden_dim=145.37,lr=0.053158,model=0.59771,n_layer=1.6639,sigmoid_func=0.26556,steps=20,weight_decay=0.022924\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.6996350539358038\n",
      "  mean_accuracy: 0.876947704114412\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23313\n",
      "  time_since_restore: 3.3887951374053955\n",
      "  time_this_iter_s: 3.3887951374053955\n",
      "  time_total_s: 3.3887951374053955\n",
      "  timestamp: 1606391397\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7c69811c\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23311)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=23331)\u001b[0m Sigmoid()\n",
      "Result for train_boston_7d55830a:\n",
      "  date: 2020-11-26_12-50-00\n",
      "  done: true\n",
      "  experiment_id: fb8c0a9e93524e48b3759be9be40aa22\n",
      "  experiment_tag: 13_adam=0.53522,droupout_prob=0.30424,hidden_dim=204.11,lr=0.059386,model=0.44019,n_layer=2.1483,sigmoid_func=0.49642,steps=20,weight_decay=0.046223\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4130915.789473684\n",
      "  mean_accuracy: 3819093.8947368423\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23311\n",
      "  time_since_restore: 3.328510046005249\n",
      "  time_this_iter_s: 3.328510046005249\n",
      "  time_total_s: 3.328510046005249\n",
      "  timestamp: 1606391400\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7d55830a\n",
      "  \n",
      "Result for train_boston_7d80f670:\n",
      "  date: 2020-11-26_12-50-00\n",
      "  done: false\n",
      "  experiment_id: 0d045ba6eeb84d5d90fc9828335c5f5d\n",
      "  experiment_tag: 14_adam=0.47531,droupout_prob=0.30648,hidden_dim=152.23,lr=0.037541,model=0.73664,n_layer=1.7336,sigmoid_func=0.77795,steps=20,weight_decay=0.04828\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 19.840492650082236\n",
      "  mean_accuracy: 19.515457956414473\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23331\n",
      "  time_since_restore: 3.8091704845428467\n",
      "  time_this_iter_s: 3.8091704845428467\n",
      "  time_total_s: 3.8091704845428467\n",
      "  timestamp: 1606391400\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7d80f670\n",
      "  \n",
      "Result for train_boston_7d80f670:\n",
      "  date: 2020-11-26_12-50-01\n",
      "  done: true\n",
      "  experiment_id: 0d045ba6eeb84d5d90fc9828335c5f5d\n",
      "  experiment_tag: 14_adam=0.47531,droupout_prob=0.30648,hidden_dim=152.23,lr=0.037541,model=0.73664,n_layer=1.7336,sigmoid_func=0.77795,steps=20,weight_decay=0.04828\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.0932095176295231\n",
      "  mean_accuracy: 0.7665709445351049\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23331\n",
      "  time_since_restore: 4.338997840881348\n",
      "  time_this_iter_s: 0.36777782440185547\n",
      "  time_total_s: 4.338997840881348\n",
      "  timestamp: 1606391401\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 7d80f670\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23630)\u001b[0m Sigmoid()\n",
      "Result for train_boston_7da39392:\n",
      "  date: 2020-11-26_12-50-04\n",
      "  done: true\n",
      "  experiment_id: 280bd4d8edaa4c069d7b7016faf2d6b4\n",
      "  experiment_tag: 15_adam=0.36932,droupout_prob=0.31661,hidden_dim=247.26,lr=0.083922,model=0.22434,n_layer=1.5202,sigmoid_func=0.92902,steps=20,weight_decay=0.071275\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 130925864.42105263\n",
      "  mean_accuracy: 127115789.4736842\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23630\n",
      "  time_since_restore: 2.651799440383911\n",
      "  time_this_iter_s: 2.651799440383911\n",
      "  time_total_s: 2.651799440383911\n",
      "  timestamp: 1606391404\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7da39392\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=11\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.033304456033204734 | Iter 4.000: -1.058819795909681 | Iter 1.000: -21.252793964586758<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 22 (7 RUNNING, 15 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">     adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7eceaf9a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.575254 </td><td style=\"text-align: right;\">       0.286455</td><td style=\"text-align: right;\">    136.97  </td><td style=\"text-align: right;\">0.0563368</td><td style=\"text-align: right;\">0.542539</td><td style=\"text-align: right;\">  2.26409</td><td style=\"text-align: right;\">      0.511323</td><td style=\"text-align: right;\">     0.0503148</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8025da8a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.305203 </td><td style=\"text-align: right;\">       0.343845</td><td style=\"text-align: right;\">    179.556 </td><td style=\"text-align: right;\">0.0699987</td><td style=\"text-align: right;\">0.30811 </td><td style=\"text-align: right;\">  1.91827</td><td style=\"text-align: right;\">      0.228793</td><td style=\"text-align: right;\">     0.0543419</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_81ed3c32</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.772075 </td><td style=\"text-align: right;\">       0.380641</td><td style=\"text-align: right;\">    124.213 </td><td style=\"text-align: right;\">0.0506693</td><td style=\"text-align: right;\">0.186222</td><td style=\"text-align: right;\">  1.53622</td><td style=\"text-align: right;\">      0.973479</td><td style=\"text-align: right;\">     0.0520169</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_82083082</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.0505669</td><td style=\"text-align: right;\">       0.453085</td><td style=\"text-align: right;\">    166.969 </td><td style=\"text-align: right;\">0.0517356</td><td style=\"text-align: right;\">0.477148</td><td style=\"text-align: right;\">  2.27609</td><td style=\"text-align: right;\">      0.158823</td><td style=\"text-align: right;\">     0.0566153</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_837b5dc2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.390983 </td><td style=\"text-align: right;\">       0.354028</td><td style=\"text-align: right;\">    159.117 </td><td style=\"text-align: right;\">0.0208905</td><td style=\"text-align: right;\">0.645194</td><td style=\"text-align: right;\">  2.05507</td><td style=\"text-align: right;\">      0.111876</td><td style=\"text-align: right;\">     0.0704925</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_83a3fb2e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.543966 </td><td style=\"text-align: right;\">       0.24523 </td><td style=\"text-align: right;\">    196.352 </td><td style=\"text-align: right;\">0.0581661</td><td style=\"text-align: right;\">0.661737</td><td style=\"text-align: right;\">  1.97642</td><td style=\"text-align: right;\">      0.34117 </td><td style=\"text-align: right;\">     0.0426898</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_840ac264</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.514273 </td><td style=\"text-align: right;\">       0.256305</td><td style=\"text-align: right;\">    170.317 </td><td style=\"text-align: right;\">0.081395 </td><td style=\"text-align: right;\">0.783381</td><td style=\"text-align: right;\">  1.94618</td><td style=\"text-align: right;\">      0.272791</td><td style=\"text-align: right;\">     0.0440964</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419 </td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123 </td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193 </td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114 </td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835 </td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813 </td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524 </td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177 </td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349 </td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636 </td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124 </td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248 </td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224 </td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 2 more trials not shown (2 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23669)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=23646)\u001b[0m Tanh()\n",
      "Result for train_boston_7eceaf9a:\n",
      "  date: 2020-11-26_12-50-05\n",
      "  done: false\n",
      "  experiment_id: cd13ae0198cf4c8283fe8056eca4fe60\n",
      "  experiment_tag: 16_adam=0.57525,droupout_prob=0.28646,hidden_dim=136.97,lr=0.056337,model=0.54254,n_layer=2.2641,sigmoid_func=0.51132,steps=20,weight_decay=0.050315\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 17.160313656455592\n",
      "  mean_accuracy: 21.682997854132402\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23646\n",
      "  time_since_restore: 2.450047492980957\n",
      "  time_this_iter_s: 2.450047492980957\n",
      "  time_total_s: 2.450047492980957\n",
      "  timestamp: 1606391405\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7eceaf9a\n",
      "  \n",
      "Result for train_boston_82083082:\n",
      "  date: 2020-11-26_12-50-05\n",
      "  done: true\n",
      "  experiment_id: 0e19bb1e7bec49b88486f91fad809e38\n",
      "  experiment_tag: 19_adam=0.050567,droupout_prob=0.45308,hidden_dim=166.97,lr=0.051736,model=0.47715,n_layer=2.2761,sigmoid_func=0.15882,steps=20,weight_decay=0.056615\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 121180.94736842105\n",
      "  mean_accuracy: 118713.1447368421\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23669\n",
      "  time_since_restore: 2.303612470626831\n",
      "  time_this_iter_s: 2.303612470626831\n",
      "  time_total_s: 2.303612470626831\n",
      "  timestamp: 1606391405\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '82083082'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23666)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=23665)\u001b[0m Sigmoid()\n",
      "Result for train_boston_7eceaf9a:\n",
      "  date: 2020-11-26_12-50-05\n",
      "  done: true\n",
      "  experiment_id: cd13ae0198cf4c8283fe8056eca4fe60\n",
      "  experiment_tag: 16_adam=0.57525,droupout_prob=0.28646,hidden_dim=136.97,lr=0.056337,model=0.54254,n_layer=2.2641,sigmoid_func=0.51132,steps=20,weight_decay=0.050315\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 27.726048519736842\n",
      "  mean_accuracy: 24.411073383532074\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23646\n",
      "  time_since_restore: 2.8422975540161133\n",
      "  time_this_iter_s: 0.24670028686523438\n",
      "  time_total_s: 2.8422975540161133\n",
      "  timestamp: 1606391405\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 7eceaf9a\n",
      "  \n",
      "Result for train_boston_81ed3c32:\n",
      "  date: 2020-11-26_12-50-05\n",
      "  done: true\n",
      "  experiment_id: 172f7c2763c64ed79cbab0b54e5e2180\n",
      "  experiment_tag: 18_adam=0.77207,droupout_prob=0.38064,hidden_dim=124.21,lr=0.050669,model=0.18622,n_layer=1.5362,sigmoid_func=0.97348,steps=20,weight_decay=0.052017\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 81902.05263157895\n",
      "  mean_accuracy: 73822.63815789473\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23665\n",
      "  time_since_restore: 2.5398337841033936\n",
      "  time_this_iter_s: 2.5398337841033936\n",
      "  time_total_s: 2.5398337841033936\n",
      "  timestamp: 1606391405\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 81ed3c32\n",
      "  \n",
      "Result for train_boston_8025da8a:\n",
      "  date: 2020-11-26_12-50-05\n",
      "  done: true\n",
      "  experiment_id: bec32e73d48a40848e606f5c69a93524\n",
      "  experiment_tag: 17_adam=0.3052,droupout_prob=0.34384,hidden_dim=179.56,lr=0.069999,model=0.30811,n_layer=1.9183,sigmoid_func=0.22879,steps=20,weight_decay=0.054342\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3754645.4736842103\n",
      "  mean_accuracy: 3572711.5789473685\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23666\n",
      "  time_since_restore: 2.4468085765838623\n",
      "  time_this_iter_s: 2.4468085765838623\n",
      "  time_total_s: 2.4468085765838623\n",
      "  timestamp: 1606391405\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8025da8a\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23722)\u001b[0m Tanh()\n",
      "Result for train_boston_83a3fb2e:\n",
      "  date: 2020-11-26_12-50-07\n",
      "  done: false\n",
      "  experiment_id: 2cbf8ddca84546fe98467cefff42f5fe\n",
      "  experiment_tag: 21_adam=0.54397,droupout_prob=0.24523,hidden_dim=196.35,lr=0.058166,model=0.66174,n_layer=1.9764,sigmoid_func=0.34117,steps=20,weight_decay=0.04269\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.162379214638158\n",
      "  mean_accuracy: 2.5913258602744653\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23722\n",
      "  time_since_restore: 2.2571189403533936\n",
      "  time_this_iter_s: 2.2571189403533936\n",
      "  time_total_s: 2.2571189403533936\n",
      "  timestamp: 1606391407\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 83a3fb2e\n",
      "  \n",
      "Result for train_boston_83a3fb2e:\n",
      "  date: 2020-11-26_12-50-07\n",
      "  done: true\n",
      "  experiment_id: 2cbf8ddca84546fe98467cefff42f5fe\n",
      "  experiment_tag: 21_adam=0.54397,droupout_prob=0.24523,hidden_dim=196.35,lr=0.058166,model=0.66174,n_layer=1.9764,sigmoid_func=0.34117,steps=20,weight_decay=0.04269\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.9961467542146383\n",
      "  mean_accuracy: 4.134914598966899\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23722\n",
      "  time_since_restore: 2.5967438220977783\n",
      "  time_this_iter_s: 0.19243621826171875\n",
      "  time_total_s: 2.5967438220977783\n",
      "  timestamp: 1606391407\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 83a3fb2e\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:08,252\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.675966739654541 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23696)\u001b[0m ReLU()\n",
      "Result for train_boston_837b5dc2:\n",
      "  date: 2020-11-26_12-50-08\n",
      "  done: true\n",
      "  experiment_id: ad92428e1fda4a738530fc7bf5af1ba1\n",
      "  experiment_tag: 20_adam=0.39098,droupout_prob=0.35403,hidden_dim=159.12,lr=0.02089,model=0.64519,n_layer=2.0551,sigmoid_func=0.11188,steps=20,weight_decay=0.070493\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 41.07035426089638\n",
      "  mean_accuracy: 40.459116082442435\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23696\n",
      "  time_since_restore: 3.308091402053833\n",
      "  time_this_iter_s: 3.308091402053833\n",
      "  time_total_s: 3.308091402053833\n",
      "  timestamp: 1606391408\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 837b5dc2\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m ReLU()\n",
      "Result for train_boston_840ac264:\n",
      "  date: 2020-11-26_12-50-09\n",
      "  done: true\n",
      "  experiment_id: b009d5cac9874d9d915927787768e69f\n",
      "  experiment_tag: 22_adam=0.51427,droupout_prob=0.25631,hidden_dim=170.32,lr=0.081395,model=0.78338,n_layer=1.9462,sigmoid_func=0.27279,steps=20,weight_decay=0.044096\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 47.33691085012335\n",
      "  mean_accuracy: 42.14870733963816\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23764\n",
      "  time_since_restore: 3.016005754470825\n",
      "  time_this_iter_s: 3.016005754470825\n",
      "  time_total_s: 3.016005754470825\n",
      "  timestamp: 1606391409\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 840ac264\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using AsyncHyperBand: num_stopped=18\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.033304456033204734 | Iter 4.000: -1.0817462770562423 | Iter 1.000: -20.5466433073345<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 29 (7 RUNNING, 22 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_85a94a1e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.580567</td><td style=\"text-align: right;\">       0.173613</td><td style=\"text-align: right;\">     72.5299</td><td style=\"text-align: right;\">0.0461383</td><td style=\"text-align: right;\">0.358077</td><td style=\"text-align: right;\">  1.91745</td><td style=\"text-align: right;\">      0.569655</td><td style=\"text-align: right;\">     0.0698312</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8666208a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.450489</td><td style=\"text-align: right;\">       0.309289</td><td style=\"text-align: right;\">    146.657 </td><td style=\"text-align: right;\">0.043224 </td><td style=\"text-align: right;\">0.37235 </td><td style=\"text-align: right;\">  2.33081</td><td style=\"text-align: right;\">      0.658507</td><td style=\"text-align: right;\">     0.0494516</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8680a8f6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.588127</td><td style=\"text-align: right;\">       0.287602</td><td style=\"text-align: right;\">    158.416 </td><td style=\"text-align: right;\">0.0803231</td><td style=\"text-align: right;\">0.468602</td><td style=\"text-align: right;\">  1.86437</td><td style=\"text-align: right;\">      0.447076</td><td style=\"text-align: right;\">     0.0201235</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_86a573c0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.560147</td><td style=\"text-align: right;\">       0.202155</td><td style=\"text-align: right;\">    196.665 </td><td style=\"text-align: right;\">0.0387627</td><td style=\"text-align: right;\">0.389404</td><td style=\"text-align: right;\">  1.81587</td><td style=\"text-align: right;\">      0.454186</td><td style=\"text-align: right;\">     0.039257 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_86b55cfe</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.232166</td><td style=\"text-align: right;\">       0.183912</td><td style=\"text-align: right;\">    178.755 </td><td style=\"text-align: right;\">0.0347913</td><td style=\"text-align: right;\">0.448565</td><td style=\"text-align: right;\">  2.08806</td><td style=\"text-align: right;\">      0.636718</td><td style=\"text-align: right;\">     0.0440773</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8819ce54</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.4756  </td><td style=\"text-align: right;\">       0.191849</td><td style=\"text-align: right;\">    161.988 </td><td style=\"text-align: right;\">0.0416444</td><td style=\"text-align: right;\">0.439134</td><td style=\"text-align: right;\">  2.06967</td><td style=\"text-align: right;\">      0.760968</td><td style=\"text-align: right;\">     0.0327703</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8878c8c8</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.196616</td><td style=\"text-align: right;\">       0.488324</td><td style=\"text-align: right;\">    137.797 </td><td style=\"text-align: right;\">0.0381632</td><td style=\"text-align: right;\">0.638953</td><td style=\"text-align: right;\">  1.85837</td><td style=\"text-align: right;\">      0.420841</td><td style=\"text-align: right;\">     0.0276748</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 9 more trials not shown (9 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23687)\u001b[0m Tanh()\n",
      "Result for train_boston_85a94a1e:\n",
      "  date: 2020-11-26_12-50-12\n",
      "  done: true\n",
      "  experiment_id: 1564cc9417a5455c9f73aceaa16d4848\n",
      "  experiment_tag: 23_adam=0.58057,droupout_prob=0.17361,hidden_dim=72.53,lr=0.046138,model=0.35808,n_layer=1.9175,sigmoid_func=0.56965,steps=20,weight_decay=0.069831\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 40.41786595394737\n",
      "  mean_accuracy: 33.69994474712171\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23687\n",
      "  time_since_restore: 3.205127000808716\n",
      "  time_this_iter_s: 3.205127000808716\n",
      "  time_total_s: 3.205127000808716\n",
      "  timestamp: 1606391412\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 85a94a1e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=23686)\u001b[0m Tanh()\n",
      "Result for train_boston_8666208a:\n",
      "  date: 2020-11-26_12-50-13\n",
      "  done: true\n",
      "  experiment_id: 5d1cf8e19cd74cbe9456e65f1f74ce5d\n",
      "  experiment_tag: 24_adam=0.45049,droupout_prob=0.30929,hidden_dim=146.66,lr=0.043224,model=0.37235,n_layer=2.3308,sigmoid_func=0.65851,steps=20,weight_decay=0.049452\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 10173.386513157895\n",
      "  mean_accuracy: 9157.474506578947\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 23686\n",
      "  time_since_restore: 2.3916923999786377\n",
      "  time_this_iter_s: 2.3916923999786377\n",
      "  time_total_s: 2.3916923999786377\n",
      "  timestamp: 1606391413\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8666208a\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24021)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24025)\u001b[0m Tanh()\n",
      "Result for train_boston_86a573c0:\n",
      "  date: 2020-11-26_12-50-17\n",
      "  done: true\n",
      "  experiment_id: e3fbc456f7164dee9e0570e97e7092c5\n",
      "  experiment_tag: 26_adam=0.56015,droupout_prob=0.20216,hidden_dim=196.66,lr=0.038763,model=0.3894,n_layer=1.8159,sigmoid_func=0.45419,steps=20,weight_decay=0.039257\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 8281.857730263158\n",
      "  mean_accuracy: 8677.699835526315\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24021\n",
      "  time_since_restore: 2.148348808288574\n",
      "  time_this_iter_s: 2.148348808288574\n",
      "  time_total_s: 2.148348808288574\n",
      "  timestamp: 1606391417\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 86a573c0\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24012)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24026)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24023)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:18,285\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6238348484039307 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24017)\u001b[0m Sigmoid()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using AsyncHyperBand: num_stopped=21\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.033304456033204734 | Iter 4.000: -1.0817462770562423 | Iter 1.000: -22.665095279091283<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 32 (7 RUNNING, 25 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_8680a8f6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.588127</td><td style=\"text-align: right;\">       0.287602</td><td style=\"text-align: right;\">    158.416 </td><td style=\"text-align: right;\">0.0803231</td><td style=\"text-align: right;\">0.468602</td><td style=\"text-align: right;\">  1.86437</td><td style=\"text-align: right;\">      0.447076</td><td style=\"text-align: right;\">    0.0201235 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_86b55cfe</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.232166</td><td style=\"text-align: right;\">       0.183912</td><td style=\"text-align: right;\">    178.755 </td><td style=\"text-align: right;\">0.0347913</td><td style=\"text-align: right;\">0.448565</td><td style=\"text-align: right;\">  2.08806</td><td style=\"text-align: right;\">      0.636718</td><td style=\"text-align: right;\">    0.0440773 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8819ce54</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.4756  </td><td style=\"text-align: right;\">       0.191849</td><td style=\"text-align: right;\">    161.988 </td><td style=\"text-align: right;\">0.0416444</td><td style=\"text-align: right;\">0.439134</td><td style=\"text-align: right;\">  2.06967</td><td style=\"text-align: right;\">      0.760968</td><td style=\"text-align: right;\">    0.0327703 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8878c8c8</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.196616</td><td style=\"text-align: right;\">       0.488324</td><td style=\"text-align: right;\">    137.797 </td><td style=\"text-align: right;\">0.0381632</td><td style=\"text-align: right;\">0.638953</td><td style=\"text-align: right;\">  1.85837</td><td style=\"text-align: right;\">      0.420841</td><td style=\"text-align: right;\">    0.0276748 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_88ba232c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.531391</td><td style=\"text-align: right;\">       0.324633</td><td style=\"text-align: right;\">    149.53  </td><td style=\"text-align: right;\">0.0458652</td><td style=\"text-align: right;\">0.610559</td><td style=\"text-align: right;\">  1.59093</td><td style=\"text-align: right;\">      0.369628</td><td style=\"text-align: right;\">    0.00882965</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8ace59b2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.496993</td><td style=\"text-align: right;\">       0.283734</td><td style=\"text-align: right;\">    167.81  </td><td style=\"text-align: right;\">0.0453557</td><td style=\"text-align: right;\">0.480904</td><td style=\"text-align: right;\">  2.19746</td><td style=\"text-align: right;\">      0.508595</td><td style=\"text-align: right;\">    0.048109  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8b264fa0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.273215</td><td style=\"text-align: right;\">       0.32134 </td><td style=\"text-align: right;\">    185.265 </td><td style=\"text-align: right;\">0.07584  </td><td style=\"text-align: right;\">0.315134</td><td style=\"text-align: right;\">  2.17474</td><td style=\"text-align: right;\">      0.599772</td><td style=\"text-align: right;\">    0.0569899 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">    0.0546216 </td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">    0.0513878 </td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">    0.0349197 </td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">    0.0449583 </td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">    0.0497963 </td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">    0.0758267 </td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">    0.0476026 </td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">    0.0943057 </td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">    0.0571592 </td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">    0.0520324 </td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">    0.0270363 </td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">    0.0229244 </td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">    0.0462234 </td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 12 more trials not shown (12 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_86b55cfe:\n",
      "  date: 2020-11-26_12-50-18\n",
      "  done: true\n",
      "  experiment_id: ab476d944c7c497eaeb5e15bd04f76aa\n",
      "  experiment_tag: 27_adam=0.23217,droupout_prob=0.18391,hidden_dim=178.75,lr=0.034791,model=0.44856,n_layer=2.0881,sigmoid_func=0.63672,steps=20,weight_decay=0.044077\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4420.644325657895\n",
      "  mean_accuracy: 3856.3355263157896\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24023\n",
      "  time_since_restore: 2.5653538703918457\n",
      "  time_this_iter_s: 2.5653538703918457\n",
      "  time_total_s: 2.5653538703918457\n",
      "  timestamp: 1606391418\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 86b55cfe\n",
      "  \n",
      "Result for train_boston_8819ce54:\n",
      "  date: 2020-11-26_12-50-18\n",
      "  done: true\n",
      "  experiment_id: 0a79d7ff83cf4a2c9dd9fa4ccb8251d7\n",
      "  experiment_tag: 28_adam=0.4756,droupout_prob=0.19185,hidden_dim=161.99,lr=0.041644,model=0.43913,n_layer=2.0697,sigmoid_func=0.76097,steps=20,weight_decay=0.03277\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1967.042146381579\n",
      "  mean_accuracy: 1707.6864720394738\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24017\n",
      "  time_since_restore: 2.4840140342712402\n",
      "  time_this_iter_s: 2.4840140342712402\n",
      "  time_total_s: 2.4840140342712402\n",
      "  timestamp: 1606391418\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8819ce54\n",
      "  \n",
      "Result for train_boston_8878c8c8:\n",
      "  date: 2020-11-26_12-50-18\n",
      "  done: true\n",
      "  experiment_id: 8848bf0b97ec4e558965c885d0a761cd\n",
      "  experiment_tag: 29_adam=0.19662,droupout_prob=0.48832,hidden_dim=137.8,lr=0.038163,model=0.63895,n_layer=1.8584,sigmoid_func=0.42084,steps=20,weight_decay=0.027675\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 25.81696841591283\n",
      "  mean_accuracy: 24.70115260074013\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24026\n",
      "  time_since_restore: 2.3600387573242188\n",
      "  time_this_iter_s: 2.3600387573242188\n",
      "  time_total_s: 2.3600387573242188\n",
      "  timestamp: 1606391418\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8878c8c8\n",
      "  \n",
      "Result for train_boston_88ba232c:\n",
      "  date: 2020-11-26_12-50-17\n",
      "  done: false\n",
      "  experiment_id: 672e5f6397c848ec8ff812d35b328435\n",
      "  experiment_tag: 30_adam=0.53139,droupout_prob=0.32463,hidden_dim=149.53,lr=0.045865,model=0.61056,n_layer=1.5909,sigmoid_func=0.36963,steps=20,weight_decay=0.0088297\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.3696988758287931\n",
      "  mean_accuracy: 1.3927729757208573\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24025\n",
      "  time_since_restore: 2.3469698429107666\n",
      "  time_this_iter_s: 2.3469698429107666\n",
      "  time_total_s: 2.3469698429107666\n",
      "  timestamp: 1606391417\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 88ba232c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:19,654\tWARNING util.py:139 -- The `search_alg.on_trial_complete` operation took 0.6892530918121338 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-26 12:50:19,749\tWARNING util.py:139 -- The `process_trial` operation took 0.785416841506958 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_8680a8f6:\n",
      "  date: 2020-11-26_12-50-17\n",
      "  done: true\n",
      "  experiment_id: 302eb90b513b469383998dccb02d315f\n",
      "  experiment_tag: 25_adam=0.58813,droupout_prob=0.2876,hidden_dim=158.42,lr=0.080323,model=0.4686,n_layer=1.8644,sigmoid_func=0.44708,steps=20,weight_decay=0.020124\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1575001.5789473683\n",
      "  mean_accuracy: 1500448.6315789474\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24012\n",
      "  time_since_restore: 2.306689739227295\n",
      "  time_this_iter_s: 2.306689739227295\n",
      "  time_total_s: 2.306689739227295\n",
      "  timestamp: 1606391417\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8680a8f6\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24027)\u001b[0m Tanh()\n",
      "Result for train_boston_88ba232c:\n",
      "  date: 2020-11-26_12-50-20\n",
      "  done: true\n",
      "  experiment_id: 672e5f6397c848ec8ff812d35b328435\n",
      "  experiment_tag: 30_adam=0.53139,droupout_prob=0.32463,hidden_dim=149.53,lr=0.045865,model=0.61056,n_layer=1.5909,sigmoid_func=0.36963,steps=20,weight_decay=0.0088297\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.0966954482229132\n",
      "  mean_accuracy: 1.2563168375115645\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24025\n",
      "  time_since_restore: 5.032533884048462\n",
      "  time_this_iter_s: 0.3800981044769287\n",
      "  time_total_s: 5.032533884048462\n",
      "  timestamp: 1606391420\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 88ba232c\n",
      "  \n",
      "Result for train_boston_8ace59b2:\n",
      "  date: 2020-11-26_12-50-20\n",
      "  done: true\n",
      "  experiment_id: c8d1b687ce1a427f90e1611fbfde61c2\n",
      "  experiment_tag: 31_adam=0.49699,droupout_prob=0.28373,hidden_dim=167.81,lr=0.045356,model=0.4809,n_layer=2.1975,sigmoid_func=0.50859,steps=20,weight_decay=0.048109\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 247562.97368421053\n",
      "  mean_accuracy: 243533.94736842104\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24027\n",
      "  time_since_restore: 2.2544984817504883\n",
      "  time_this_iter_s: 2.2544984817504883\n",
      "  time_total_s: 2.2544984817504883\n",
      "  timestamp: 1606391420\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ace59b2\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24016)\u001b[0m Tanh()\n",
      "Result for train_boston_8b264fa0:\n",
      "  date: 2020-11-26_12-50-21\n",
      "  done: true\n",
      "  experiment_id: 7603a781c484473eb279d339b74cf5b6\n",
      "  experiment_tag: 32_adam=0.27321,droupout_prob=0.32134,hidden_dim=185.27,lr=0.07584,model=0.31513,n_layer=2.1747,sigmoid_func=0.59977,steps=20,weight_decay=0.05699\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3534899.3684210526\n",
      "  mean_accuracy: 3246490.947368421\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24016\n",
      "  time_since_restore: 2.797065258026123\n",
      "  time_this_iter_s: 2.797065258026123\n",
      "  time_total_s: 2.797065258026123\n",
      "  timestamp: 1606391421\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b264fa0\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24157)\u001b[0m Tanh()\n",
      "Result for train_boston_8e1d0a50:\n",
      "  date: 2020-11-26_12-50-30\n",
      "  done: true\n",
      "  experiment_id: 07b7aeb092754837977af40c615743b9\n",
      "  experiment_tag: 33_adam=0.62649,droupout_prob=0.22813,hidden_dim=142.88,lr=0.052697,model=0.6558,n_layer=1.5755,sigmoid_func=0.58942,steps=20,weight_decay=0.045985\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 38.63611161081415\n",
      "  mean_accuracy: 35.36049290707237\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24157\n",
      "  time_since_restore: 6.711483001708984\n",
      "  time_this_iter_s: 6.711483001708984\n",
      "  time_total_s: 6.711483001708984\n",
      "  timestamp: 1606391430\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8e1d0a50\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24142)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24179)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24126)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:31,325\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 1.0654902458190918 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=29\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.033304456033204734 | Iter 4.000: -1.0932095176295231 | Iter 1.000: -25.81696841591283<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 40 (7 RUNNING, 33 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_8e34675e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.508515</td><td style=\"text-align: right;\">       0.161228</td><td style=\"text-align: right;\">    149.042 </td><td style=\"text-align: right;\">0.0607732</td><td style=\"text-align: right;\">0.445264</td><td style=\"text-align: right;\">  1.95511</td><td style=\"text-align: right;\">      0.591311</td><td style=\"text-align: right;\">     0.064355 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8e4d8248</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.553402</td><td style=\"text-align: right;\">       0.272397</td><td style=\"text-align: right;\">    113.694 </td><td style=\"text-align: right;\">0.0590917</td><td style=\"text-align: right;\">0.549114</td><td style=\"text-align: right;\">  2.68141</td><td style=\"text-align: right;\">      0.590711</td><td style=\"text-align: right;\">     0.0340919</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8e6474d0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.186636</td><td style=\"text-align: right;\">       0.264184</td><td style=\"text-align: right;\">    170.633 </td><td style=\"text-align: right;\">0.0382452</td><td style=\"text-align: right;\">0.56633 </td><td style=\"text-align: right;\">  1.61423</td><td style=\"text-align: right;\">      0.626378</td><td style=\"text-align: right;\">     0.0564889</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8ef451ea</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.129275</td><td style=\"text-align: right;\">       0.468378</td><td style=\"text-align: right;\">     66.0149</td><td style=\"text-align: right;\">0.0548892</td><td style=\"text-align: right;\">0.022351</td><td style=\"text-align: right;\">  2.42869</td><td style=\"text-align: right;\">      0.490687</td><td style=\"text-align: right;\">     0.0216447</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8f4cff3e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.368331</td><td style=\"text-align: right;\">       0.259028</td><td style=\"text-align: right;\">     76.1335</td><td style=\"text-align: right;\">0.0318749</td><td style=\"text-align: right;\">0.66818 </td><td style=\"text-align: right;\">  1.93408</td><td style=\"text-align: right;\">      0.326297</td><td style=\"text-align: right;\">     0.052371 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8f640ba2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.570075</td><td style=\"text-align: right;\">       0.262998</td><td style=\"text-align: right;\">    137.453 </td><td style=\"text-align: right;\">0.0426636</td><td style=\"text-align: right;\">0.639005</td><td style=\"text-align: right;\">  2.21432</td><td style=\"text-align: right;\">      0.445552</td><td style=\"text-align: right;\">     0.0633783</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_8ff22c2a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.536743</td><td style=\"text-align: right;\">       0.183535</td><td style=\"text-align: right;\">    160.026 </td><td style=\"text-align: right;\">0.0294304</td><td style=\"text-align: right;\">0.572604</td><td style=\"text-align: right;\">  1.69583</td><td style=\"text-align: right;\">      0.625712</td><td style=\"text-align: right;\">     0.0319367</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 20 more trials not shown (20 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_8e4d8248:\n",
      "  date: 2020-11-26_12-50-31\n",
      "  done: false\n",
      "  experiment_id: 52659a3f44184c07814e6314e2fb5cdb\n",
      "  experiment_tag: 35_adam=0.5534,droupout_prob=0.2724,hidden_dim=113.69,lr=0.059092,model=0.54911,n_layer=2.6814,sigmoid_func=0.59071,steps=20,weight_decay=0.034092\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 19.157591167249176\n",
      "  mean_accuracy: 19.064170435855264\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24179\n",
      "  time_since_restore: 6.623545408248901\n",
      "  time_this_iter_s: 6.623545408248901\n",
      "  time_total_s: 6.623545408248901\n",
      "  timestamp: 1606391431\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8e4d8248\n",
      "  \n",
      "Result for train_boston_8e6474d0:\n",
      "  date: 2020-11-26_12-50-31\n",
      "  done: true\n",
      "  experiment_id: 4646b80f4d3b45e7be97659ca82bc3f3\n",
      "  experiment_tag: 36_adam=0.18664,droupout_prob=0.26418,hidden_dim=170.63,lr=0.038245,model=0.56633,n_layer=1.6142,sigmoid_func=0.62638,steps=20,weight_decay=0.056489\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 55.6976960834704\n",
      "  mean_accuracy: 48.55830142372533\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24126\n",
      "  time_since_restore: 5.753532648086548\n",
      "  time_this_iter_s: 5.753532648086548\n",
      "  time_total_s: 5.753532648086548\n",
      "  timestamp: 1606391431\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8e6474d0\n",
      "  \n",
      "Result for train_boston_8e34675e:\n",
      "  date: 2020-11-26_12-50-30\n",
      "  done: true\n",
      "  experiment_id: 89f6509bfaad4e3380edfcf18a3351c0\n",
      "  experiment_tag: 34_adam=0.50852,droupout_prob=0.16123,hidden_dim=149.04,lr=0.060773,model=0.44526,n_layer=1.9551,sigmoid_func=0.59131,steps=20,weight_decay=0.064355\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 335771.4210526316\n",
      "  mean_accuracy: 306833.7368421053\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24142\n",
      "  time_since_restore: 6.636480093002319\n",
      "  time_this_iter_s: 6.636480093002319\n",
      "  time_total_s: 6.636480093002319\n",
      "  timestamp: 1606391430\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8e34675e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24104)\u001b[0m Tanh()\n",
      "Result for train_boston_8ef451ea:\n",
      "  date: 2020-11-26_12-50-31\n",
      "  done: true\n",
      "  experiment_id: 8e3940698b8148169c6cb3251185eba7\n",
      "  experiment_tag: 37_adam=0.12928,droupout_prob=0.46838,hidden_dim=66.015,lr=0.054889,model=0.022351,n_layer=2.4287,sigmoid_func=0.49069,steps=20,weight_decay=0.021645\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 8560.57072368421\n",
      "  mean_accuracy: 8254.06990131579\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24104\n",
      "  time_since_restore: 2.3881959915161133\n",
      "  time_this_iter_s: 2.3881959915161133\n",
      "  time_total_s: 2.3881959915161133\n",
      "  timestamp: 1606391431\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ef451ea\n",
      "  \n",
      "Result for train_boston_8e4d8248:\n",
      "  date: 2020-11-26_12-50-31\n",
      "  done: true\n",
      "  experiment_id: 52659a3f44184c07814e6314e2fb5cdb\n",
      "  experiment_tag: 35_adam=0.5534,droupout_prob=0.2724,hidden_dim=113.69,lr=0.059092,model=0.54911,n_layer=2.6814,sigmoid_func=0.59071,steps=20,weight_decay=0.034092\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.3116567511307564\n",
      "  mean_accuracy: 3.7155729594983553\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24179\n",
      "  time_since_restore: 7.478436470031738\n",
      "  time_this_iter_s: 0.31949639320373535\n",
      "  time_total_s: 7.478436470031738\n",
      "  timestamp: 1606391431\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 8e4d8248\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24526)\u001b[0m ReLU()\n",
      "Result for train_boston_8f4cff3e:\n",
      "  date: 2020-11-26_12-50-38\n",
      "  done: false\n",
      "  experiment_id: f85d14c916d546209feeb3cf8116859b\n",
      "  experiment_tag: 38_adam=0.36833,droupout_prob=0.25903,hidden_dim=76.134,lr=0.031875,model=0.66818,n_layer=1.9341,sigmoid_func=0.3263,steps=20,weight_decay=0.052371\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3.056709891871402\n",
      "  mean_accuracy: 2.5770341973555717\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24526\n",
      "  time_since_restore: 5.609785318374634\n",
      "  time_this_iter_s: 5.609785318374634\n",
      "  time_total_s: 5.609785318374634\n",
      "  timestamp: 1606391438\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8f4cff3e\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=33\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.033304456033204734 | Iter 4.000: -1.0940810002778707 | Iter 1.000: -23.45306356329667<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 45 (8 RUNNING, 37 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_8f4cff3e</td><td>RUNNING   </td><td>192.168.1.34:24526</td><td style=\"text-align: right;\">0.368331</td><td style=\"text-align: right;\">       0.259028</td><td style=\"text-align: right;\">     76.1335</td><td style=\"text-align: right;\">0.0318749 </td><td style=\"text-align: right;\">0.66818 </td><td style=\"text-align: right;\">  1.93408</td><td style=\"text-align: right;\">      0.326297</td><td style=\"text-align: right;\">     0.052371 </td><td style=\"text-align: right;\">     2.57703  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.60979</td><td style=\"text-align: right;\">     3.05671  </td></tr>\n",
       "<tr><td>train_boston_8f640ba2</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.570075</td><td style=\"text-align: right;\">       0.262998</td><td style=\"text-align: right;\">    137.453 </td><td style=\"text-align: right;\">0.0426636 </td><td style=\"text-align: right;\">0.639005</td><td style=\"text-align: right;\">  2.21432</td><td style=\"text-align: right;\">      0.445552</td><td style=\"text-align: right;\">     0.0633783</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_8ff22c2a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.536743</td><td style=\"text-align: right;\">       0.183535</td><td style=\"text-align: right;\">    160.026 </td><td style=\"text-align: right;\">0.0294304 </td><td style=\"text-align: right;\">0.572604</td><td style=\"text-align: right;\">  1.69583</td><td style=\"text-align: right;\">      0.625712</td><td style=\"text-align: right;\">     0.0319367</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_95e1dacc</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.3026  </td><td style=\"text-align: right;\">       0.347987</td><td style=\"text-align: right;\">    174.771 </td><td style=\"text-align: right;\">0.0179181 </td><td style=\"text-align: right;\">0.929853</td><td style=\"text-align: right;\">  1.37883</td><td style=\"text-align: right;\">      0.519301</td><td style=\"text-align: right;\">     0.0206717</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_95f8ad6a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.416446</td><td style=\"text-align: right;\">       0.20317 </td><td style=\"text-align: right;\">    192.763 </td><td style=\"text-align: right;\">0.00893143</td><td style=\"text-align: right;\">0.565872</td><td style=\"text-align: right;\">  1.32265</td><td style=\"text-align: right;\">      0.448567</td><td style=\"text-align: right;\">     0.056085 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_960b6478</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.531108</td><td style=\"text-align: right;\">       0.226554</td><td style=\"text-align: right;\">    118.425 </td><td style=\"text-align: right;\">0.0508432 </td><td style=\"text-align: right;\">0.472605</td><td style=\"text-align: right;\">  1.91847</td><td style=\"text-align: right;\">      0.490214</td><td style=\"text-align: right;\">     0.0398267</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_9634e06e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.56664 </td><td style=\"text-align: right;\">       0.374291</td><td style=\"text-align: right;\">    113.872 </td><td style=\"text-align: right;\">0.0434054 </td><td style=\"text-align: right;\">0.188433</td><td style=\"text-align: right;\">  1.93065</td><td style=\"text-align: right;\">      0.382412</td><td style=\"text-align: right;\">     0.0544758</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_9655bbcc</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.545211</td><td style=\"text-align: right;\">       0.347435</td><td style=\"text-align: right;\">    192.53  </td><td style=\"text-align: right;\">0.0704904 </td><td style=\"text-align: right;\">0.45929 </td><td style=\"text-align: right;\">  1.44864</td><td style=\"text-align: right;\">      0.644256</td><td style=\"text-align: right;\">     0.0537678</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 25 more trials not shown (25 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24532)\u001b[0m Tanh()\n",
      "Result for train_boston_8ff22c2a:\n",
      "  date: 2020-11-26_12-50-38\n",
      "  done: false\n",
      "  experiment_id: 75f9627e552044ae9fd6a9eb790e8ce4\n",
      "  experiment_tag: 40_adam=0.53674,droupout_prob=0.18353,hidden_dim=160.03,lr=0.02943,model=0.5726,n_layer=1.6958,sigmoid_func=0.62571,steps=20,weight_decay=0.031937\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 9.058738306949014\n",
      "  mean_accuracy: 9.382674367804276\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24532\n",
      "  time_since_restore: 5.243176698684692\n",
      "  time_this_iter_s: 5.243176698684692\n",
      "  time_total_s: 5.243176698684692\n",
      "  timestamp: 1606391438\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8ff22c2a\n",
      "  \n",
      "Result for train_boston_8ff22c2a:\n",
      "  date: 2020-11-26_12-50-38\n",
      "  done: true\n",
      "  experiment_id: 75f9627e552044ae9fd6a9eb790e8ce4\n",
      "  experiment_tag: 40_adam=0.53674,droupout_prob=0.18353,hidden_dim=160.03,lr=0.02943,model=0.5726,n_layer=1.6958,sigmoid_func=0.62571,steps=20,weight_decay=0.031937\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.5755522878546464\n",
      "  mean_accuracy: 2.832161552027652\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24532\n",
      "  time_since_restore: 5.606422185897827\n",
      "  time_this_iter_s: 0.20316791534423828\n",
      "  time_total_s: 5.606422185897827\n",
      "  timestamp: 1606391438\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 8ff22c2a\n",
      "  \n",
      "Result for train_boston_8f4cff3e:\n",
      "  date: 2020-11-26_12-50-39\n",
      "  done: true\n",
      "  experiment_id: f85d14c916d546209feeb3cf8116859b\n",
      "  experiment_tag: 38_adam=0.36833,droupout_prob=0.25903,hidden_dim=76.134,lr=0.031875,model=0.66818,n_layer=1.9341,sigmoid_func=0.3263,steps=20,weight_decay=0.052371\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.22292056836579976\n",
      "  mean_accuracy: 0.18543781732258044\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24526\n",
      "  time_since_restore: 6.808433532714844\n",
      "  time_this_iter_s: 0.0901186466217041\n",
      "  time_total_s: 6.808433532714844\n",
      "  timestamp: 1606391439\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: 8f4cff3e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24524)\u001b[0m Tanh()\n",
      "Result for train_boston_8f640ba2:\n",
      "  date: 2020-11-26_12-50-39\n",
      "  done: true\n",
      "  experiment_id: 5a540682b03f40cca1429144bf2333d2\n",
      "  experiment_tag: 39_adam=0.57008,droupout_prob=0.263,hidden_dim=137.45,lr=0.042664,model=0.639,n_layer=2.2143,sigmoid_func=0.44555,steps=20,weight_decay=0.063378\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 24.015981573807565\n",
      "  mean_accuracy: 21.866363525390625\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24524\n",
      "  time_since_restore: 5.366574287414551\n",
      "  time_this_iter_s: 5.366574287414551\n",
      "  time_total_s: 5.366574287414551\n",
      "  timestamp: 1606391439\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8f640ba2\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24640)\u001b[0m Tanh()\n",
      "Result for train_boston_9655bbcc:\n",
      "  date: 2020-11-26_12-50-42\n",
      "  done: true\n",
      "  experiment_id: ef763541edbb4ac8a252efd25c526604\n",
      "  experiment_tag: 45_adam=0.54521,droupout_prob=0.34744,hidden_dim=192.53,lr=0.07049,model=0.45929,n_layer=1.4486,sigmoid_func=0.64426,steps=20,weight_decay=0.053768\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 218349.31578947368\n",
      "  mean_accuracy: 201967.51315789475\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24640\n",
      "  time_since_restore: 3.6996231079101562\n",
      "  time_this_iter_s: 3.6996231079101562\n",
      "  time_total_s: 3.6996231079101562\n",
      "  timestamp: 1606391442\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9655bbcc\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24553)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24552)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24600)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24550)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:43,841\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.8952434062957764 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=37\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.035926692579921926 | Iter 4.000: -1.0817462770562423 | Iter 1.000: -22.665095279091283<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 48 (7 RUNNING, 41 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_95e1dacc</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.3026  </td><td style=\"text-align: right;\">       0.347987</td><td style=\"text-align: right;\">    174.771 </td><td style=\"text-align: right;\">0.0179181 </td><td style=\"text-align: right;\">0.929853</td><td style=\"text-align: right;\">  1.37883</td><td style=\"text-align: right;\">      0.519301</td><td style=\"text-align: right;\">     0.0206717</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_95f8ad6a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.416446</td><td style=\"text-align: right;\">       0.20317 </td><td style=\"text-align: right;\">    192.763 </td><td style=\"text-align: right;\">0.00893143</td><td style=\"text-align: right;\">0.565872</td><td style=\"text-align: right;\">  1.32265</td><td style=\"text-align: right;\">      0.448567</td><td style=\"text-align: right;\">     0.056085 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_960b6478</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.531108</td><td style=\"text-align: right;\">       0.226554</td><td style=\"text-align: right;\">    118.425 </td><td style=\"text-align: right;\">0.0508432 </td><td style=\"text-align: right;\">0.472605</td><td style=\"text-align: right;\">  1.91847</td><td style=\"text-align: right;\">      0.490214</td><td style=\"text-align: right;\">     0.0398267</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9634e06e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.56664 </td><td style=\"text-align: right;\">       0.374291</td><td style=\"text-align: right;\">    113.872 </td><td style=\"text-align: right;\">0.0434054 </td><td style=\"text-align: right;\">0.188433</td><td style=\"text-align: right;\">  1.93065</td><td style=\"text-align: right;\">      0.382412</td><td style=\"text-align: right;\">     0.0544758</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9a6cbbe8</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.491043</td><td style=\"text-align: right;\">       0.247405</td><td style=\"text-align: right;\">    177.323 </td><td style=\"text-align: right;\">0.0425409 </td><td style=\"text-align: right;\">0.689932</td><td style=\"text-align: right;\">  1.84214</td><td style=\"text-align: right;\">      0.510027</td><td style=\"text-align: right;\">     0.0342212</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9a950a58</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.682544</td><td style=\"text-align: right;\">       0.246236</td><td style=\"text-align: right;\">     89.8635</td><td style=\"text-align: right;\">0.0520941 </td><td style=\"text-align: right;\">0.551439</td><td style=\"text-align: right;\">  1.76615</td><td style=\"text-align: right;\">      0.240776</td><td style=\"text-align: right;\">     0.0249756</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9ab839f6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.800305</td><td style=\"text-align: right;\">       0.408902</td><td style=\"text-align: right;\">    246.36  </td><td style=\"text-align: right;\">0.00496684</td><td style=\"text-align: right;\">0.760581</td><td style=\"text-align: right;\">  2.18419</td><td style=\"text-align: right;\">      0.446059</td><td style=\"text-align: right;\">     0.0819502</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386  </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 28 more trials not shown (28 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_95f8ad6a:\n",
      "  date: 2020-11-26_12-50-43\n",
      "  done: false\n",
      "  experiment_id: 7e8a1dbdc1394061b133cd70a469c5b1\n",
      "  experiment_tag: 42_adam=0.41645,droupout_prob=0.20317,hidden_dim=192.76,lr=0.0089314,model=0.56587,n_layer=1.3227,sigmoid_func=0.44857,steps=20,weight_decay=0.056085\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 8.980585198653372\n",
      "  mean_accuracy: 10.480728149414062\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24553\n",
      "  time_since_restore: 3.968045234680176\n",
      "  time_this_iter_s: 3.968045234680176\n",
      "  time_total_s: 3.968045234680176\n",
      "  timestamp: 1606391443\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 95f8ad6a\n",
      "  \n",
      "Result for train_boston_95e1dacc:\n",
      "  date: 2020-11-26_12-50-43\n",
      "  done: false\n",
      "  experiment_id: 281c872f73dd4ddbbd67f38bedbb88ba\n",
      "  experiment_tag: 41_adam=0.3026,droupout_prob=0.34799,hidden_dim=174.77,lr=0.017918,model=0.92985,n_layer=1.3788,sigmoid_func=0.5193,steps=20,weight_decay=0.020672\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 20.0770263671875\n",
      "  mean_accuracy: 25.77857248406661\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24552\n",
      "  time_since_restore: 4.085174560546875\n",
      "  time_this_iter_s: 4.085174560546875\n",
      "  time_total_s: 4.085174560546875\n",
      "  timestamp: 1606391443\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 95e1dacc\n",
      "  \n",
      "Result for train_boston_9634e06e:\n",
      "  date: 2020-11-26_12-50-43\n",
      "  done: true\n",
      "  experiment_id: 6d480550c7244a7dad33f0aa34468139\n",
      "  experiment_tag: 44_adam=0.56664,droupout_prob=0.37429,hidden_dim=113.87,lr=0.043405,model=0.18843,n_layer=1.9306,sigmoid_func=0.38241,steps=20,weight_decay=0.054476\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 63385.07236842105\n",
      "  mean_accuracy: 58100.89473684211\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24550\n",
      "  time_since_restore: 4.128371477127075\n",
      "  time_this_iter_s: 4.128371477127075\n",
      "  time_total_s: 4.128371477127075\n",
      "  timestamp: 1606391443\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9634e06e\n",
      "  \n",
      "Result for train_boston_960b6478:\n",
      "  date: 2020-11-26_12-50-43\n",
      "  done: true\n",
      "  experiment_id: c363f38c30f44f3783eef219bab1de62\n",
      "  experiment_tag: 43_adam=0.53111,droupout_prob=0.22655,hidden_dim=118.43,lr=0.050843,model=0.47261,n_layer=1.9185,sigmoid_func=0.49021,steps=20,weight_decay=0.039827\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2422.1027960526317\n",
      "  mean_accuracy: 2325.5497532894738\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24600\n",
      "  time_since_restore: 4.220136404037476\n",
      "  time_this_iter_s: 4.220136404037476\n",
      "  time_total_s: 4.220136404037476\n",
      "  timestamp: 1606391443\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 960b6478\n",
      "  \n",
      "Result for train_boston_95f8ad6a:\n",
      "  date: 2020-11-26_12-50-44\n",
      "  done: true\n",
      "  experiment_id: 7e8a1dbdc1394061b133cd70a469c5b1\n",
      "  experiment_tag: 42_adam=0.41645,droupout_prob=0.20317,hidden_dim=192.76,lr=0.0089314,model=0.56587,n_layer=1.3227,sigmoid_func=0.44857,steps=20,weight_decay=0.056085\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 5.020995290655839\n",
      "  mean_accuracy: 6.0771849782843335\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24553\n",
      "  time_since_restore: 5.497896194458008\n",
      "  time_this_iter_s: 0.3296189308166504\n",
      "  time_total_s: 5.497896194458008\n",
      "  timestamp: 1606391444\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 95f8ad6a\n",
      "  \n",
      "Result for train_boston_95e1dacc:\n",
      "  date: 2020-11-26_12-50-44\n",
      "  done: true\n",
      "  experiment_id: 281c872f73dd4ddbbd67f38bedbb88ba\n",
      "  experiment_tag: 41_adam=0.3026,droupout_prob=0.34799,hidden_dim=174.77,lr=0.017918,model=0.92985,n_layer=1.3788,sigmoid_func=0.5193,steps=20,weight_decay=0.020672\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 6.473867315995066\n",
      "  mean_accuracy: 8.142615067331414\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24552\n",
      "  time_since_restore: 5.599334955215454\n",
      "  time_this_iter_s: 0.35465574264526367\n",
      "  time_total_s: 5.599334955215454\n",
      "  timestamp: 1606391444\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 95e1dacc\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24551)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=24528)\u001b[0m ReLU()\n",
      "Result for train_boston_9a6cbbe8:\n",
      "  date: 2020-11-26_12-50-49\n",
      "  done: true\n",
      "  experiment_id: 8f1564f9619448a7ab193c18d55a2c48\n",
      "  experiment_tag: 46_adam=0.49104,droupout_prob=0.24741,hidden_dim=177.32,lr=0.042541,model=0.68993,n_layer=1.8421,sigmoid_func=0.51003,steps=20,weight_decay=0.034221\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 23.11589933696546\n",
      "  mean_accuracy: 23.024389969675166\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24551\n",
      "  time_since_restore: 4.157317638397217\n",
      "  time_this_iter_s: 4.157317638397217\n",
      "  time_total_s: 4.157317638397217\n",
      "  timestamp: 1606391449\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9a6cbbe8\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using AsyncHyperBand: num_stopped=42\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.035926692579921926 | Iter 4.000: -1.0940810002778707 | Iter 1.000: -20.724043595163444<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 53 (7 RUNNING, 46 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_9a950a58</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.682544</td><td style=\"text-align: right;\">       0.246236</td><td style=\"text-align: right;\">     89.8635</td><td style=\"text-align: right;\">0.0520941 </td><td style=\"text-align: right;\">0.551439</td><td style=\"text-align: right;\">  1.76615</td><td style=\"text-align: right;\">      0.240776</td><td style=\"text-align: right;\">     0.0249756</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9ab839f6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.800305</td><td style=\"text-align: right;\">       0.408902</td><td style=\"text-align: right;\">    246.36  </td><td style=\"text-align: right;\">0.00496684</td><td style=\"text-align: right;\">0.760581</td><td style=\"text-align: right;\">  2.18419</td><td style=\"text-align: right;\">      0.446059</td><td style=\"text-align: right;\">     0.0819502</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9d563b72</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.543483</td><td style=\"text-align: right;\">       0.153491</td><td style=\"text-align: right;\">    138.196 </td><td style=\"text-align: right;\">0.0608216 </td><td style=\"text-align: right;\">0.484869</td><td style=\"text-align: right;\">  2.28706</td><td style=\"text-align: right;\">      0.555232</td><td style=\"text-align: right;\">     0.0175784</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9d736dd2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.600616</td><td style=\"text-align: right;\">       0.125398</td><td style=\"text-align: right;\">    150.717 </td><td style=\"text-align: right;\">0.0481093 </td><td style=\"text-align: right;\">0.723921</td><td style=\"text-align: right;\">  2.84673</td><td style=\"text-align: right;\">      0.686256</td><td style=\"text-align: right;\">     0.0452479</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9d86d610</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.198855</td><td style=\"text-align: right;\">       0.16489 </td><td style=\"text-align: right;\">    101.793 </td><td style=\"text-align: right;\">0.0763515 </td><td style=\"text-align: right;\">0.434287</td><td style=\"text-align: right;\">  1.60977</td><td style=\"text-align: right;\">      0.281829</td><td style=\"text-align: right;\">     0.0476293</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9dd334a6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.171393</td><td style=\"text-align: right;\">       0.14411 </td><td style=\"text-align: right;\">    190.224 </td><td style=\"text-align: right;\">0.024465  </td><td style=\"text-align: right;\">0.634095</td><td style=\"text-align: right;\">  1.84269</td><td style=\"text-align: right;\">      0.154571</td><td style=\"text-align: right;\">     0.0576795</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9dec32d0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.247601</td><td style=\"text-align: right;\">       0.189744</td><td style=\"text-align: right;\">    168.904 </td><td style=\"text-align: right;\">0.0208652 </td><td style=\"text-align: right;\">0.700728</td><td style=\"text-align: right;\">  1.87337</td><td style=\"text-align: right;\">      0.483494</td><td style=\"text-align: right;\">     0.0664963</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386  </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 33 more trials not shown (33 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_9a950a58:\n",
      "  date: 2020-11-26_12-50-49\n",
      "  done: false\n",
      "  experiment_id: 23e10809af3945e7b3972ecaa980586b\n",
      "  experiment_tag: 47_adam=0.68254,droupout_prob=0.24624,hidden_dim=89.863,lr=0.052094,model=0.55144,n_layer=1.7662,sigmoid_func=0.24078,steps=20,weight_decay=0.024976\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 16.379222669099505\n",
      "  mean_accuracy: 16.794165360300166\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24528\n",
      "  time_since_restore: 4.310804843902588\n",
      "  time_this_iter_s: 4.310804843902588\n",
      "  time_total_s: 4.310804843902588\n",
      "  timestamp: 1606391449\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9a950a58\n",
      "  \n",
      "Result for train_boston_9a950a58:\n",
      "  date: 2020-11-26_12-50-50\n",
      "  done: true\n",
      "  experiment_id: 23e10809af3945e7b3972ecaa980586b\n",
      "  experiment_tag: 47_adam=0.68254,droupout_prob=0.24624,hidden_dim=89.863,lr=0.052094,model=0.55144,n_layer=1.7662,sigmoid_func=0.24078,steps=20,weight_decay=0.024976\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 4.667704933568051\n",
      "  mean_accuracy: 4.017132006193462\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24528\n",
      "  time_since_restore: 5.055432319641113\n",
      "  time_this_iter_s: 0.23816251754760742\n",
      "  time_total_s: 5.055432319641113\n",
      "  timestamp: 1606391450\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 9a950a58\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:52,373\tWARNING util.py:139 -- The `start_trial` operation took 0.7813611030578613 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24954)\u001b[0m Tanh()\n",
      "Result for train_boston_9ab839f6:\n",
      "  date: 2020-11-26_12-50-57\n",
      "  done: true\n",
      "  experiment_id: 833e1032e61448f79c1adb6a5b36e1b8\n",
      "  experiment_tag: 48_adam=0.8003,droupout_prob=0.4089,hidden_dim=246.36,lr=0.0049668,model=0.76058,n_layer=2.1842,sigmoid_func=0.44606,steps=20,weight_decay=0.08195\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 109.9366519325658\n",
      "  mean_accuracy: 104.87662546258224\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24954\n",
      "  time_since_restore: 4.386369705200195\n",
      "  time_this_iter_s: 4.386369705200195\n",
      "  time_total_s: 4.386369705200195\n",
      "  timestamp: 1606391457\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9ab839f6\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:50:58,486\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.9555270671844482 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=44\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.035926692579921926 | Iter 4.000: -1.0949524829262183 | Iter 1.000: -20.017892937911185<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 55 (7 RUNNING, 48 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_9d563b72</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.543483</td><td style=\"text-align: right;\">       0.153491</td><td style=\"text-align: right;\">    138.196 </td><td style=\"text-align: right;\">0.0608216</td><td style=\"text-align: right;\">0.484869</td><td style=\"text-align: right;\">  2.28706</td><td style=\"text-align: right;\">      0.555232</td><td style=\"text-align: right;\">     0.0175784</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9d736dd2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.600616</td><td style=\"text-align: right;\">       0.125398</td><td style=\"text-align: right;\">    150.717 </td><td style=\"text-align: right;\">0.0481093</td><td style=\"text-align: right;\">0.723921</td><td style=\"text-align: right;\">  2.84673</td><td style=\"text-align: right;\">      0.686256</td><td style=\"text-align: right;\">     0.0452479</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9d86d610</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.198855</td><td style=\"text-align: right;\">       0.16489 </td><td style=\"text-align: right;\">    101.793 </td><td style=\"text-align: right;\">0.0763515</td><td style=\"text-align: right;\">0.434287</td><td style=\"text-align: right;\">  1.60977</td><td style=\"text-align: right;\">      0.281829</td><td style=\"text-align: right;\">     0.0476293</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9dd334a6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.171393</td><td style=\"text-align: right;\">       0.14411 </td><td style=\"text-align: right;\">    190.224 </td><td style=\"text-align: right;\">0.024465 </td><td style=\"text-align: right;\">0.634095</td><td style=\"text-align: right;\">  1.84269</td><td style=\"text-align: right;\">      0.154571</td><td style=\"text-align: right;\">     0.0576795</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_9dec32d0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.247601</td><td style=\"text-align: right;\">       0.189744</td><td style=\"text-align: right;\">    168.904 </td><td style=\"text-align: right;\">0.0208652</td><td style=\"text-align: right;\">0.700728</td><td style=\"text-align: right;\">  1.87337</td><td style=\"text-align: right;\">      0.483494</td><td style=\"text-align: right;\">     0.0664963</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a0c6bc50</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.460592</td><td style=\"text-align: right;\">       0.308467</td><td style=\"text-align: right;\">    219.438 </td><td style=\"text-align: right;\">0.0336784</td><td style=\"text-align: right;\">0.579486</td><td style=\"text-align: right;\">  1.65722</td><td style=\"text-align: right;\">      0.419804</td><td style=\"text-align: right;\">     0.0424805</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a1479262</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.686723</td><td style=\"text-align: right;\">       0.233829</td><td style=\"text-align: right;\">    134.376 </td><td style=\"text-align: right;\">0.0325007</td><td style=\"text-align: right;\">0.614192</td><td style=\"text-align: right;\">  1.91777</td><td style=\"text-align: right;\">      0.5039  </td><td style=\"text-align: right;\">     0.0547438</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 35 more trials not shown (35 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24971)\u001b[0m Tanh()\n",
      "Result for train_boston_9d563b72:\n",
      "  date: 2020-11-26_12-50-58\n",
      "  done: true\n",
      "  experiment_id: 1c91697c1ee54fbfa6b365d8f0dceb42\n",
      "  experiment_tag: 49_adam=0.54348,droupout_prob=0.15349,hidden_dim=138.2,lr=0.060822,model=0.48487,n_layer=2.2871,sigmoid_func=0.55523,steps=20,weight_decay=0.017578\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 322547.05263157893\n",
      "  mean_accuracy: 310103.44736842107\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24971\n",
      "  time_since_restore: 4.129655361175537\n",
      "  time_this_iter_s: 4.129655361175537\n",
      "  time_total_s: 4.129655361175537\n",
      "  timestamp: 1606391458\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9d563b72\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24973)\u001b[0m Sigmoid()\n",
      "\u001b[2m\u001b[36m(pid=24988)\u001b[0m ReLU()\n",
      "Result for train_boston_9d736dd2:\n",
      "  date: 2020-11-26_12-51-01\n",
      "  done: false\n",
      "  experiment_id: ffb2100e2fa14439826143fec6ce66be\n",
      "  experiment_tag: 50_adam=0.60062,droupout_prob=0.1254,hidden_dim=150.72,lr=0.048109,model=0.72392,n_layer=2.8467,sigmoid_func=0.68626,steps=20,weight_decay=0.045248\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.488937578703228\n",
      "  mean_accuracy: 2.1992125260202506\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24973\n",
      "  time_since_restore: 4.363011360168457\n",
      "  time_this_iter_s: 4.363011360168457\n",
      "  time_total_s: 4.363011360168457\n",
      "  timestamp: 1606391461\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9d736dd2\n",
      "  \n",
      "Result for train_boston_9d86d610:\n",
      "  date: 2020-11-26_12-51-01\n",
      "  done: true\n",
      "  experiment_id: 378c4fb9b4a245d7b4fe679c24ba04cc\n",
      "  experiment_tag: 51_adam=0.19886,droupout_prob=0.16489,hidden_dim=101.79,lr=0.076352,model=0.43429,n_layer=1.6098,sigmoid_func=0.28183,steps=20,weight_decay=0.047629\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1288234.3157894737\n",
      "  mean_accuracy: 1185816.2105263157\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24988\n",
      "  time_since_restore: 3.980234146118164\n",
      "  time_this_iter_s: 3.980234146118164\n",
      "  time_total_s: 3.980234146118164\n",
      "  timestamp: 1606391461\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9d86d610\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24994)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=24992)\u001b[0m Tanh()\n",
      "Result for train_boston_a0c6bc50:\n",
      "  date: 2020-11-26_12-51-02\n",
      "  done: false\n",
      "  experiment_id: 204f8548fd534413aebc05f01751e15d\n",
      "  experiment_tag: 54_adam=0.46059,droupout_prob=0.30847,hidden_dim=219.44,lr=0.033678,model=0.57949,n_layer=1.6572,sigmoid_func=0.4198,steps=20,weight_decay=0.04248\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 10.275782534950658\n",
      "  mean_accuracy: 10.905845240542764\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24992\n",
      "  time_since_restore: 4.295040130615234\n",
      "  time_this_iter_s: 4.295040130615234\n",
      "  time_total_s: 4.295040130615234\n",
      "  timestamp: 1606391462\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a0c6bc50\n",
      "  \n",
      "Result for train_boston_9dd334a6:\n",
      "  date: 2020-11-26_12-51-02\n",
      "  done: false\n",
      "  experiment_id: 0c968b91cc8b4457bedaf2fa2f030a87\n",
      "  experiment_tag: 52_adam=0.17139,droupout_prob=0.14411,hidden_dim=190.22,lr=0.024465,model=0.63409,n_layer=1.8427,sigmoid_func=0.15457,steps=20,weight_decay=0.05768\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 15.960296630859375\n",
      "  mean_accuracy: 17.680410284745065\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24994\n",
      "  time_since_restore: 4.248256683349609\n",
      "  time_this_iter_s: 4.248256683349609\n",
      "  time_total_s: 4.248256683349609\n",
      "  timestamp: 1606391462\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9dd334a6\n",
      "  \n",
      "Result for train_boston_9dd334a6:\n",
      "  date: 2020-11-26_12-51-02\n",
      "  done: true\n",
      "  experiment_id: 0c968b91cc8b4457bedaf2fa2f030a87\n",
      "  experiment_tag: 52_adam=0.17139,droupout_prob=0.14411,hidden_dim=190.22,lr=0.024465,model=0.63409,n_layer=1.8427,sigmoid_func=0.15457,steps=20,weight_decay=0.05768\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 7.632464760228207\n",
      "  mean_accuracy: 7.604978059467516\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24994\n",
      "  time_since_restore: 4.65186619758606\n",
      "  time_this_iter_s: 0.24259495735168457\n",
      "  time_total_s: 4.65186619758606\n",
      "  timestamp: 1606391462\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 9dd334a6\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24993)\u001b[0m Tanh()\n",
      "Result for train_boston_a0c6bc50:\n",
      "  date: 2020-11-26_12-51-02\n",
      "  done: true\n",
      "  experiment_id: 204f8548fd534413aebc05f01751e15d\n",
      "  experiment_tag: 54_adam=0.46059,droupout_prob=0.30847,hidden_dim=219.44,lr=0.033678,model=0.57949,n_layer=1.6572,sigmoid_func=0.4198,steps=20,weight_decay=0.04248\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.780426025390625\n",
      "  mean_accuracy: 2.5497181541041325\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24992\n",
      "  time_since_restore: 4.740725755691528\n",
      "  time_this_iter_s: 0.3000938892364502\n",
      "  time_total_s: 4.740725755691528\n",
      "  timestamp: 1606391462\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: a0c6bc50\n",
      "  \n",
      "Result for train_boston_9dec32d0:\n",
      "  date: 2020-11-26_12-51-02\n",
      "  done: false\n",
      "  experiment_id: d26d72accbb64c19a06e9ff41d2a889b\n",
      "  experiment_tag: 53_adam=0.2476,droupout_prob=0.18974,hidden_dim=168.9,lr=0.020865,model=0.70073,n_layer=1.8734,sigmoid_func=0.48349,steps=20,weight_decay=0.066496\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 15.227839419716283\n",
      "  mean_accuracy: 17.34666041324013\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24993\n",
      "  time_since_restore: 4.341650485992432\n",
      "  time_this_iter_s: 4.341650485992432\n",
      "  time_total_s: 4.341650485992432\n",
      "  timestamp: 1606391462\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 9dec32d0\n",
      "  \n",
      "Result for train_boston_9dec32d0:\n",
      "  date: 2020-11-26_12-51-03\n",
      "  done: true\n",
      "  experiment_id: d26d72accbb64c19a06e9ff41d2a889b\n",
      "  experiment_tag: 53_adam=0.2476,droupout_prob=0.18974,hidden_dim=168.9,lr=0.020865,model=0.70073,n_layer=1.8734,sigmoid_func=0.48349,steps=20,weight_decay=0.066496\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 9.272591038754111\n",
      "  mean_accuracy: 9.770462839226974\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24993\n",
      "  time_since_restore: 4.786314487457275\n",
      "  time_this_iter_s: 0.2614567279815674\n",
      "  time_total_s: 4.786314487457275\n",
      "  timestamp: 1606391463\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 9dec32d0\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=49\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03188372756305494 | Iter 4.000: -1.0949524829262183 | Iter 1.000: -17.659633034153988<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 61 (8 RUNNING, 53 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_9d736dd2</td><td>RUNNING   </td><td>192.168.1.34:24973</td><td style=\"text-align: right;\">0.600616</td><td style=\"text-align: right;\">       0.125398</td><td style=\"text-align: right;\">    150.717 </td><td style=\"text-align: right;\">0.0481093</td><td style=\"text-align: right;\">0.723921</td><td style=\"text-align: right;\">  2.84673</td><td style=\"text-align: right;\">      0.686256</td><td style=\"text-align: right;\">     0.0452479</td><td style=\"text-align: right;\">     0.0154841</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         6.11209</td><td style=\"text-align: right;\">     0.012482 </td></tr>\n",
       "<tr><td>train_boston_a1479262</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.686723</td><td style=\"text-align: right;\">       0.233829</td><td style=\"text-align: right;\">    134.376 </td><td style=\"text-align: right;\">0.0325007</td><td style=\"text-align: right;\">0.614192</td><td style=\"text-align: right;\">  1.91777</td><td style=\"text-align: right;\">      0.5039  </td><td style=\"text-align: right;\">     0.0547438</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a619df52</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.31741 </td><td style=\"text-align: right;\">       0.177132</td><td style=\"text-align: right;\">    127.643 </td><td style=\"text-align: right;\">0.0317783</td><td style=\"text-align: right;\">0.415688</td><td style=\"text-align: right;\">  1.69883</td><td style=\"text-align: right;\">      0.322531</td><td style=\"text-align: right;\">     0.0617326</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a655edf8</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.143659</td><td style=\"text-align: right;\">       0.242156</td><td style=\"text-align: right;\">    125.77  </td><td style=\"text-align: right;\">0.0272236</td><td style=\"text-align: right;\">0.488714</td><td style=\"text-align: right;\">  1.70491</td><td style=\"text-align: right;\">      0.379345</td><td style=\"text-align: right;\">     0.043532 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a8169502</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.620664</td><td style=\"text-align: right;\">       0.239108</td><td style=\"text-align: right;\">    140.904 </td><td style=\"text-align: right;\">0.0430372</td><td style=\"text-align: right;\">0.667116</td><td style=\"text-align: right;\">  1.54519</td><td style=\"text-align: right;\">      0.146794</td><td style=\"text-align: right;\">     0.0383176</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a87e46de</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.297687</td><td style=\"text-align: right;\">       0.177826</td><td style=\"text-align: right;\">    118.41  </td><td style=\"text-align: right;\">0.0477556</td><td style=\"text-align: right;\">0.800832</td><td style=\"text-align: right;\">  1.70228</td><td style=\"text-align: right;\">      0.262115</td><td style=\"text-align: right;\">     0.0505323</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a894b91e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.520757</td><td style=\"text-align: right;\">       0.193417</td><td style=\"text-align: right;\">    155.741 </td><td style=\"text-align: right;\">0.0471642</td><td style=\"text-align: right;\">0.699423</td><td style=\"text-align: right;\">  1.75887</td><td style=\"text-align: right;\">      0.387444</td><td style=\"text-align: right;\">     0.0210444</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a8dcb872</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.354969</td><td style=\"text-align: right;\">       0.261372</td><td style=\"text-align: right;\">    189.504 </td><td style=\"text-align: right;\">0.0374622</td><td style=\"text-align: right;\">0.714425</td><td style=\"text-align: right;\">  2.45151</td><td style=\"text-align: right;\">      0.267378</td><td style=\"text-align: right;\">     0.0350583</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 41 more trials not shown (41 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25003)\u001b[0m Tanh()\n",
      "Result for train_boston_a1479262:\n",
      "  date: 2020-11-26_12-51-03\n",
      "  done: false\n",
      "  experiment_id: 5a9f9152f64a44faa62bcab386fa479b\n",
      "  experiment_tag: 55_adam=0.68672,droupout_prob=0.23383,hidden_dim=134.38,lr=0.032501,model=0.61419,n_layer=1.9178,sigmoid_func=0.5039,steps=20,weight_decay=0.054744\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 11.89481634842722\n",
      "  mean_accuracy: 12.33488544664885\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25003\n",
      "  time_since_restore: 2.1421313285827637\n",
      "  time_this_iter_s: 2.1421313285827637\n",
      "  time_total_s: 2.1421313285827637\n",
      "  timestamp: 1606391463\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a1479262\n",
      "  \n",
      "Result for train_boston_a1479262:\n",
      "  date: 2020-11-26_12-51-04\n",
      "  done: true\n",
      "  experiment_id: 5a9f9152f64a44faa62bcab386fa479b\n",
      "  experiment_tag: 55_adam=0.68672,droupout_prob=0.23383,hidden_dim=134.38,lr=0.032501,model=0.61419,n_layer=1.9178,sigmoid_func=0.5039,steps=20,weight_decay=0.054744\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.2100759807385897\n",
      "  mean_accuracy: 2.7965240478515625\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25003\n",
      "  time_since_restore: 2.6491641998291016\n",
      "  time_this_iter_s: 0.24553799629211426\n",
      "  time_total_s: 2.6491641998291016\n",
      "  timestamp: 1606391464\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: a1479262\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25071)\u001b[0m Tanh()\n",
      "Result for train_boston_a655edf8:\n",
      "  date: 2020-11-26_12-51-08\n",
      "  done: true\n",
      "  experiment_id: 9628ab564d8a48c682eeece6cba4afd7\n",
      "  experiment_tag: 57_adam=0.14366,droupout_prob=0.24216,hidden_dim=125.77,lr=0.027224,model=0.48871,n_layer=1.7049,sigmoid_func=0.37934,steps=20,weight_decay=0.043532\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 305.9726048519737\n",
      "  mean_accuracy: 276.36502878289474\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25071\n",
      "  time_since_restore: 4.1620752811431885\n",
      "  time_this_iter_s: 4.1620752811431885\n",
      "  time_total_s: 4.1620752811431885\n",
      "  timestamp: 1606391468\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a655edf8\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25113)\u001b[0m ReLU()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:10,327\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 1.4167759418487549 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using AsyncHyperBand: num_stopped=51\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03188372756305494 | Iter 4.000: -1.0958239655745656 | Iter 1.000: -16.96504090961657<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 63 (7 RUNNING, 56 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">    model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_a619df52</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.31741 </td><td style=\"text-align: right;\">       0.177132</td><td style=\"text-align: right;\">    127.643 </td><td style=\"text-align: right;\">0.0317783</td><td style=\"text-align: right;\">0.415688 </td><td style=\"text-align: right;\">  1.69883</td><td style=\"text-align: right;\">     0.322531 </td><td style=\"text-align: right;\">     0.0617326</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a8169502</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.620664</td><td style=\"text-align: right;\">       0.239108</td><td style=\"text-align: right;\">    140.904 </td><td style=\"text-align: right;\">0.0430372</td><td style=\"text-align: right;\">0.667116 </td><td style=\"text-align: right;\">  1.54519</td><td style=\"text-align: right;\">     0.146794 </td><td style=\"text-align: right;\">     0.0383176</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a87e46de</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.297687</td><td style=\"text-align: right;\">       0.177826</td><td style=\"text-align: right;\">    118.41  </td><td style=\"text-align: right;\">0.0477556</td><td style=\"text-align: right;\">0.800832 </td><td style=\"text-align: right;\">  1.70228</td><td style=\"text-align: right;\">     0.262115 </td><td style=\"text-align: right;\">     0.0505323</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a894b91e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.520757</td><td style=\"text-align: right;\">       0.193417</td><td style=\"text-align: right;\">    155.741 </td><td style=\"text-align: right;\">0.0471642</td><td style=\"text-align: right;\">0.699423 </td><td style=\"text-align: right;\">  1.75887</td><td style=\"text-align: right;\">     0.387444 </td><td style=\"text-align: right;\">     0.0210444</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a8dcb872</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.354969</td><td style=\"text-align: right;\">       0.261372</td><td style=\"text-align: right;\">    189.504 </td><td style=\"text-align: right;\">0.0374622</td><td style=\"text-align: right;\">0.714425 </td><td style=\"text-align: right;\">  2.45151</td><td style=\"text-align: right;\">     0.267378 </td><td style=\"text-align: right;\">     0.0350583</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a927a940</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.381451</td><td style=\"text-align: right;\">       0.456116</td><td style=\"text-align: right;\">     75.9657</td><td style=\"text-align: right;\">0.0339213</td><td style=\"text-align: right;\">0.0262608</td><td style=\"text-align: right;\">  2.19612</td><td style=\"text-align: right;\">     0.0681097</td><td style=\"text-align: right;\">     0.0494103</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_a98b1520</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.184063</td><td style=\"text-align: right;\">       0.27652 </td><td style=\"text-align: right;\">    204.805 </td><td style=\"text-align: right;\">0.0412174</td><td style=\"text-align: right;\">0.50092  </td><td style=\"text-align: right;\">  1.9418 </td><td style=\"text-align: right;\">     0.686055 </td><td style=\"text-align: right;\">     0.0279622</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167   </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">     0.724931 </td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511 </td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">     0.605291 </td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439 </td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">     0.213596 </td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437  </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">     0.456714 </td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208 </td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">     0.598601 </td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297 </td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">     0.388659 </td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587 </td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">     0.480465 </td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198 </td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">     0.551065 </td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214 </td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">     0.45047  </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292 </td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">     0.706545 </td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898 </td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">     0.845624 </td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705 </td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">     0.265565 </td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189 </td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">     0.496419 </td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 43 more trials not shown (43 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_a619df52:\n",
      "  date: 2020-11-26_12-51-09\n",
      "  done: true\n",
      "  experiment_id: 9b3a19313c1a4c7eb1887c8e00f09803\n",
      "  experiment_tag: 56_adam=0.31741,droupout_prob=0.17713,hidden_dim=127.64,lr=0.031778,model=0.41569,n_layer=1.6988,sigmoid_func=0.32253,steps=20,weight_decay=0.061733\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 451.6800986842105\n",
      "  mean_accuracy: 395.50025699013156\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25113\n",
      "  time_since_restore: 2.4180796146392822\n",
      "  time_this_iter_s: 2.4180796146392822\n",
      "  time_total_s: 2.4180796146392822\n",
      "  timestamp: 1606391469\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a619df52\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:12,273\tWARNING util.py:139 -- The `process_trial` operation took 1.3090901374816895 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25045)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=25101)\u001b[0m ReLU()\n",
      "Result for train_boston_a8169502:\n",
      "  date: 2020-11-26_12-51-13\n",
      "  done: true\n",
      "  experiment_id: cf8cb441885e43e38deeef0cabc4111d\n",
      "  experiment_tag: 58_adam=0.62066,droupout_prob=0.23911,hidden_dim=140.9,lr=0.043037,model=0.66712,n_layer=1.5452,sigmoid_func=0.14679,steps=20,weight_decay=0.038318\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 29.76563142475329\n",
      "  mean_accuracy: 30.02169639185855\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25045\n",
      "  time_since_restore: 4.5088560581207275\n",
      "  time_this_iter_s: 4.5088560581207275\n",
      "  time_total_s: 4.5088560581207275\n",
      "  timestamp: 1606391473\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a8169502\n",
      "  \n",
      "Result for train_boston_a87e46de:\n",
      "  date: 2020-11-26_12-51-13\n",
      "  done: true\n",
      "  experiment_id: 5cb57587ceae4437996e94ee4b504c86\n",
      "  experiment_tag: 59_adam=0.29769,droupout_prob=0.17783,hidden_dim=118.41,lr=0.047756,model=0.80083,n_layer=1.7023,sigmoid_func=0.26212,steps=20,weight_decay=0.050532\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 36.22895893297697\n",
      "  mean_accuracy: 35.736173930921055\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25101\n",
      "  time_since_restore: 4.366756439208984\n",
      "  time_this_iter_s: 4.366756439208984\n",
      "  time_total_s: 4.366756439208984\n",
      "  timestamp: 1606391473\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a87e46de\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=24996)\u001b[0m Tanh()\n",
      "Result for train_boston_a894b91e:\n",
      "  date: 2020-11-26_12-51-14\n",
      "  done: false\n",
      "  experiment_id: b8419c81008c490993808f84b9b5b0d9\n",
      "  experiment_tag: 60_adam=0.52076,droupout_prob=0.19342,hidden_dim=155.74,lr=0.047164,model=0.69942,n_layer=1.7589,sigmoid_func=0.38744,steps=20,weight_decay=0.021044\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.408928017867239\n",
      "  mean_accuracy: 1.7222635369551809\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24996\n",
      "  time_since_restore: 4.043892860412598\n",
      "  time_this_iter_s: 4.043892860412598\n",
      "  time_total_s: 4.043892860412598\n",
      "  timestamp: 1606391474\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a894b91e\n",
      "  \n",
      "Result for train_boston_a894b91e:\n",
      "  date: 2020-11-26_12-51-14\n",
      "  done: true\n",
      "  experiment_id: b8419c81008c490993808f84b9b5b0d9\n",
      "  experiment_tag: 60_adam=0.52076,droupout_prob=0.19342,hidden_dim=155.74,lr=0.047164,model=0.69942,n_layer=1.7589,sigmoid_func=0.38744,steps=20,weight_decay=0.021044\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.838252017372533\n",
      "  mean_accuracy: 3.763071562114515\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 24996\n",
      "  time_since_restore: 4.356199026107788\n",
      "  time_this_iter_s: 0.20222949981689453\n",
      "  time_total_s: 4.356199026107788\n",
      "  timestamp: 1606391474\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: a894b91e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25348)\u001b[0m ReLU()\n",
      "Result for train_boston_a8dcb872:\n",
      "  date: 2020-11-26_12-51-17\n",
      "  done: false\n",
      "  experiment_id: 0211b3e4641e42a1acf95688a8ddcd94\n",
      "  experiment_tag: 61_adam=0.35497,droupout_prob=0.26137,hidden_dim=189.5,lr=0.037462,model=0.71442,n_layer=2.4515,sigmoid_func=0.26738,steps=20,weight_decay=0.035058\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3.969637820595189\n",
      "  mean_accuracy: 4.526506524336965\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25348\n",
      "  time_since_restore: 1.5835003852844238\n",
      "  time_this_iter_s: 1.5835003852844238\n",
      "  time_total_s: 1.5835003852844238\n",
      "  timestamp: 1606391477\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a8dcb872\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=55\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03188372756305494 | Iter 4.000: -1.0966954482229132 | Iter 1.000: -16.379222669099505<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 68 (8 RUNNING, 60 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">    model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_a8dcb872</td><td>RUNNING   </td><td>192.168.1.34:25348</td><td style=\"text-align: right;\">0.354969</td><td style=\"text-align: right;\">       0.261372</td><td style=\"text-align: right;\">    189.504 </td><td style=\"text-align: right;\">0.0374622</td><td style=\"text-align: right;\">0.714425 </td><td style=\"text-align: right;\">  2.45151</td><td style=\"text-align: right;\">     0.267378 </td><td style=\"text-align: right;\">     0.0350583</td><td style=\"text-align: right;\">     4.52651  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.5835 </td><td style=\"text-align: right;\">     3.96964  </td></tr>\n",
       "<tr><td>train_boston_a927a940</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.381451</td><td style=\"text-align: right;\">       0.456116</td><td style=\"text-align: right;\">     75.9657</td><td style=\"text-align: right;\">0.0339213</td><td style=\"text-align: right;\">0.0262608</td><td style=\"text-align: right;\">  2.19612</td><td style=\"text-align: right;\">     0.0681097</td><td style=\"text-align: right;\">     0.0494103</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_a98b1520</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.184063</td><td style=\"text-align: right;\">       0.27652 </td><td style=\"text-align: right;\">    204.805 </td><td style=\"text-align: right;\">0.0412174</td><td style=\"text-align: right;\">0.50092  </td><td style=\"text-align: right;\">  1.9418 </td><td style=\"text-align: right;\">     0.686055 </td><td style=\"text-align: right;\">     0.0279622</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_ad291b28</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.839875</td><td style=\"text-align: right;\">       0.254554</td><td style=\"text-align: right;\">    214.978 </td><td style=\"text-align: right;\">0.0429358</td><td style=\"text-align: right;\">0.485615 </td><td style=\"text-align: right;\">  1.56329</td><td style=\"text-align: right;\">     0.722377 </td><td style=\"text-align: right;\">     0.036167 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_ae7cce02</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.566836</td><td style=\"text-align: right;\">       0.345332</td><td style=\"text-align: right;\">    120.636 </td><td style=\"text-align: right;\">0.0319898</td><td style=\"text-align: right;\">0.661581 </td><td style=\"text-align: right;\">  1.87137</td><td style=\"text-align: right;\">     0.524062 </td><td style=\"text-align: right;\">     0.0339464</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_aefce2c2</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.499413</td><td style=\"text-align: right;\">       0.269809</td><td style=\"text-align: right;\">    139.778 </td><td style=\"text-align: right;\">0.0336974</td><td style=\"text-align: right;\">0.635814 </td><td style=\"text-align: right;\">  2.05088</td><td style=\"text-align: right;\">     0.322634 </td><td style=\"text-align: right;\">     0.0339496</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_af1781c2</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.331966</td><td style=\"text-align: right;\">       0.312748</td><td style=\"text-align: right;\">    152.254 </td><td style=\"text-align: right;\">0.0502742</td><td style=\"text-align: right;\">0.70239  </td><td style=\"text-align: right;\">  1.48588</td><td style=\"text-align: right;\">     0.293042 </td><td style=\"text-align: right;\">     0.0527697</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_af91bb40</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.62351 </td><td style=\"text-align: right;\">       0.380807</td><td style=\"text-align: right;\">    152.723 </td><td style=\"text-align: right;\">0.0391377</td><td style=\"text-align: right;\">0.651054 </td><td style=\"text-align: right;\">  2.29525</td><td style=\"text-align: right;\">     0.462491 </td><td style=\"text-align: right;\">     0.0828395</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167   </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">     0.724931 </td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511 </td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">     0.605291 </td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439 </td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">     0.213596 </td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437  </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">     0.456714 </td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208 </td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">     0.598601 </td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297 </td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">     0.388659 </td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587 </td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">     0.480465 </td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198 </td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">     0.551065 </td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214 </td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">     0.45047  </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292 </td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">     0.706545 </td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898 </td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">     0.845624 </td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705 </td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">     0.265565 </td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 48 more trials not shown (48 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25364)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=25356)\u001b[0m Sigmoid()\n",
      "Result for train_boston_a98b1520:\n",
      "  date: 2020-11-26_12-51-17\n",
      "  done: false\n",
      "  experiment_id: 8749a2f806184d1e8267402093207ff2\n",
      "  experiment_tag: 63_adam=0.18406,droupout_prob=0.27652,hidden_dim=204.81,lr=0.041217,model=0.50092,n_layer=1.9418,sigmoid_func=0.68605,steps=20,weight_decay=0.027962\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 10.115929051449424\n",
      "  mean_accuracy: 13.705036364103618\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25356\n",
      "  time_since_restore: 2.050520658493042\n",
      "  time_this_iter_s: 2.050520658493042\n",
      "  time_total_s: 2.050520658493042\n",
      "  timestamp: 1606391477\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a98b1520\n",
      "  \n",
      "Result for train_boston_a927a940:\n",
      "  date: 2020-11-26_12-51-17\n",
      "  done: true\n",
      "  experiment_id: 3ea637765d5d4d6782d74c59dca1d063\n",
      "  experiment_tag: 62_adam=0.38145,droupout_prob=0.45612,hidden_dim=75.966,lr=0.033921,model=0.026261,n_layer=2.1961,sigmoid_func=0.06811,steps=20,weight_decay=0.04941\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 37.539306640625\n",
      "  mean_accuracy: 32.802194695723685\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25364\n",
      "  time_since_restore: 2.2526497840881348\n",
      "  time_this_iter_s: 2.2526497840881348\n",
      "  time_total_s: 2.2526497840881348\n",
      "  timestamp: 1606391477\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: a927a940\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25372)\u001b[0m Sigmoid()\n",
      "Result for train_boston_a8dcb872:\n",
      "  date: 2020-11-26_12-51-17\n",
      "  done: true\n",
      "  experiment_id: 0211b3e4641e42a1acf95688a8ddcd94\n",
      "  experiment_tag: 61_adam=0.35497,droupout_prob=0.26137,hidden_dim=189.5,lr=0.037462,model=0.71442,n_layer=2.4515,sigmoid_func=0.26738,steps=20,weight_decay=0.035058\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.042503178119659424\n",
      "  mean_accuracy: 0.049856872935044136\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25348\n",
      "  time_since_restore: 2.3976004123687744\n",
      "  time_this_iter_s: 0.050147294998168945\n",
      "  time_total_s: 2.3976004123687744\n",
      "  timestamp: 1606391477\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: a8dcb872\n",
      "  \n",
      "Result for train_boston_a98b1520:\n",
      "  date: 2020-11-26_12-51-18\n",
      "  done: true\n",
      "  experiment_id: 8749a2f806184d1e8267402093207ff2\n",
      "  experiment_tag: 63_adam=0.18406,droupout_prob=0.27652,hidden_dim=204.81,lr=0.041217,model=0.50092,n_layer=1.9418,sigmoid_func=0.68605,steps=20,weight_decay=0.027962\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.7048159147563733\n",
      "  mean_accuracy: 2.0307675411826684\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25356\n",
      "  time_since_restore: 2.4476780891418457\n",
      "  time_this_iter_s: 0.28775835037231445\n",
      "  time_total_s: 2.4476780891418457\n",
      "  timestamp: 1606391478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: a98b1520\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25375)\u001b[0m Tanh()\n",
      "Result for train_boston_ad291b28:\n",
      "  date: 2020-11-26_12-51-18\n",
      "  done: true\n",
      "  experiment_id: b125b2dbad824126be6d123345f93e54\n",
      "  experiment_tag: 64_adam=0.83987,droupout_prob=0.25455,hidden_dim=214.98,lr=0.042936,model=0.48561,n_layer=1.5633,sigmoid_func=0.72238,steps=20,weight_decay=0.036167\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 297693.9736842105\n",
      "  mean_accuracy: 271941.2368421053\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25372\n",
      "  time_since_restore: 1.6698877811431885\n",
      "  time_this_iter_s: 1.6698877811431885\n",
      "  time_total_s: 1.6698877811431885\n",
      "  timestamp: 1606391478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ad291b28\n",
      "  \n",
      "Result for train_boston_ae7cce02:\n",
      "  date: 2020-11-26_12-51-18\n",
      "  done: false\n",
      "  experiment_id: b62933cd06a24f8dbd6808220d35d8d7\n",
      "  experiment_tag: 65_adam=0.56684,droupout_prob=0.34533,hidden_dim=120.64,lr=0.03199,model=0.66158,n_layer=1.8714,sigmoid_func=0.52406,steps=20,weight_decay=0.033946\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7901629899677477\n",
      "  mean_accuracy: 0.8539634503816304\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25375\n",
      "  time_since_restore: 2.30521297454834\n",
      "  time_this_iter_s: 2.30521297454834\n",
      "  time_total_s: 2.30521297454834\n",
      "  timestamp: 1606391478\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ae7cce02\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25373)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=25374)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=25524)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:20,615\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 1.6759483814239502 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_af1781c2:\n",
      "  date: 2020-11-26_12-51-19\n",
      "  done: false\n",
      "  experiment_id: 57f4f91554eb45f88f1eeb521505223a\n",
      "  experiment_tag: 67_adam=0.33197,droupout_prob=0.31275,hidden_dim=152.25,lr=0.050274,model=0.70239,n_layer=1.4859,sigmoid_func=0.29304,steps=20,weight_decay=0.05277\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 14.676666259765625\n",
      "  mean_accuracy: 12.935966090152139\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25373\n",
      "  time_since_restore: 2.124856472015381\n",
      "  time_this_iter_s: 2.124856472015381\n",
      "  time_total_s: 2.124856472015381\n",
      "  timestamp: 1606391479\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: af1781c2\n",
      "  \n",
      "Result for train_boston_aefce2c2:\n",
      "  date: 2020-11-26_12-51-19\n",
      "  done: true\n",
      "  experiment_id: 54685e0bbed9440f83e14f7a5649afeb\n",
      "  experiment_tag: 66_adam=0.49941,droupout_prob=0.26981,hidden_dim=139.78,lr=0.033697,model=0.63581,n_layer=2.0509,sigmoid_func=0.32263,steps=20,weight_decay=0.03395\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 26.05997025339227\n",
      "  mean_accuracy: 26.93665353875411\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25374\n",
      "  time_since_restore: 2.090385913848877\n",
      "  time_this_iter_s: 2.090385913848877\n",
      "  time_total_s: 2.090385913848877\n",
      "  timestamp: 1606391479\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: aefce2c2\n",
      "  \n",
      "Result for train_boston_af91bb40:\n",
      "  date: 2020-11-26_12-51-20\n",
      "  done: true\n",
      "  experiment_id: f5c20ec755bf44068273d6fd8820e066\n",
      "  experiment_tag: 68_adam=0.62351,droupout_prob=0.38081,hidden_dim=152.72,lr=0.039138,model=0.65105,n_layer=2.2952,sigmoid_func=0.46249,steps=20,weight_decay=0.082839\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 28.39177181846217\n",
      "  mean_accuracy: 30.68415591591283\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25524\n",
      "  time_since_restore: 1.9945480823516846\n",
      "  time_this_iter_s: 1.9945480823516846\n",
      "  time_total_s: 1.9945480823516846\n",
      "  timestamp: 1606391480\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: af91bb40\n",
      "  \n",
      "Result for train_boston_af1781c2:\n",
      "  date: 2020-11-26_12-51-21\n",
      "  done: true\n",
      "  experiment_id: 57f4f91554eb45f88f1eeb521505223a\n",
      "  experiment_tag: 67_adam=0.33197,droupout_prob=0.31275,hidden_dim=152.25,lr=0.050274,model=0.70239,n_layer=1.4859,sigmoid_func=0.29304,steps=20,weight_decay=0.05277\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.3890409971538342\n",
      "  mean_accuracy: 0.9390026895623458\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25373\n",
      "  time_since_restore: 4.057278156280518\n",
      "  time_this_iter_s: 0.357180118560791\n",
      "  time_total_s: 4.057278156280518\n",
      "  timestamp: 1606391481\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: af1781c2\n",
      "  \n",
      "Result for train_boston_ae7cce02:\n",
      "  date: 2020-11-26_12-51-21\n",
      "  done: true\n",
      "  experiment_id: b62933cd06a24f8dbd6808220d35d8d7\n",
      "  experiment_tag: 65_adam=0.56684,droupout_prob=0.34533,hidden_dim=120.64,lr=0.03199,model=0.66158,n_layer=1.8714,sigmoid_func=0.52406,steps=20,weight_decay=0.033946\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.03299206181576377\n",
      "  mean_accuracy: 0.024555888615156476\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25375\n",
      "  time_since_restore: 5.326099634170532\n",
      "  time_this_iter_s: 0.06346273422241211\n",
      "  time_total_s: 5.326099634170532\n",
      "  timestamp: 1606391481\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: ae7cce02\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25355)\u001b[0m Tanh()\n",
      "Result for train_boston_b1c6d116:\n",
      "  date: 2020-11-26_12-51-27\n",
      "  done: true\n",
      "  experiment_id: 5b693e95b36c4ec8a3535e892219f990\n",
      "  experiment_tag: 70_adam=0.45493,droupout_prob=0.32399,hidden_dim=150.76,lr=0.036787,model=0.66585,n_layer=1.7946,sigmoid_func=0.37966,steps=20,weight_decay=0.043687\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 25.35245714689556\n",
      "  mean_accuracy: 28.57973118832237\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25355\n",
      "  time_since_restore: 5.681159734725952\n",
      "  time_this_iter_s: 5.681159734725952\n",
      "  time_total_s: 5.681159734725952\n",
      "  timestamp: 1606391487\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b1c6d116\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using AsyncHyperBand: num_stopped=64\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03299206181576377 | Iter 4.000: -1.0932095176295231 | Iter 1.000: -15.960296630859375<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 76 (7 RUNNING, 69 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_b1aaa7a2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.432021</td><td style=\"text-align: right;\">       0.286984</td><td style=\"text-align: right;\">    107.074 </td><td style=\"text-align: right;\">0.0539774</td><td style=\"text-align: right;\">0.645092</td><td style=\"text-align: right;\">  1.82461</td><td style=\"text-align: right;\">      0.621021</td><td style=\"text-align: right;\">   0.0370416  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b1dad436</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.683908</td><td style=\"text-align: right;\">       0.274684</td><td style=\"text-align: right;\">    183.091 </td><td style=\"text-align: right;\">0.0617144</td><td style=\"text-align: right;\">0.954308</td><td style=\"text-align: right;\">  1.96979</td><td style=\"text-align: right;\">      0.351341</td><td style=\"text-align: right;\">   0.0495231  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b1f53d6c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.542912</td><td style=\"text-align: right;\">       0.349619</td><td style=\"text-align: right;\">    152.015 </td><td style=\"text-align: right;\">0.0704049</td><td style=\"text-align: right;\">0.803264</td><td style=\"text-align: right;\">  2.59959</td><td style=\"text-align: right;\">      0.703883</td><td style=\"text-align: right;\">   0.0394677  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b3512c0c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.444367</td><td style=\"text-align: right;\">       0.255866</td><td style=\"text-align: right;\">    191.087 </td><td style=\"text-align: right;\">0.0488875</td><td style=\"text-align: right;\">0.721741</td><td style=\"text-align: right;\">  1.52851</td><td style=\"text-align: right;\">      0.702947</td><td style=\"text-align: right;\">   0.0656437  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b3680602</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.474213</td><td style=\"text-align: right;\">       0.409848</td><td style=\"text-align: right;\">    142.929 </td><td style=\"text-align: right;\">0.0395739</td><td style=\"text-align: right;\">0.261121</td><td style=\"text-align: right;\">  2.44284</td><td style=\"text-align: right;\">      0.851161</td><td style=\"text-align: right;\">   0.000838513</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b39d9312</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.547233</td><td style=\"text-align: right;\">       0.153082</td><td style=\"text-align: right;\">    144.683 </td><td style=\"text-align: right;\">0.0715696</td><td style=\"text-align: right;\">0.994815</td><td style=\"text-align: right;\">  1.94286</td><td style=\"text-align: right;\">      0.32218 </td><td style=\"text-align: right;\">   0.0365085  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b3d5b6a2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.362208</td><td style=\"text-align: right;\">       0.452299</td><td style=\"text-align: right;\">     98.3715</td><td style=\"text-align: right;\">0.079424 </td><td style=\"text-align: right;\">0.502342</td><td style=\"text-align: right;\">  2.22116</td><td style=\"text-align: right;\">      0.598859</td><td style=\"text-align: right;\">   0.0184037  </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">   0.0546216  </td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">   0.0513878  </td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">   0.0349197  </td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">   0.0449583  </td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">   0.0497963  </td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">   0.0758267  </td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">   0.0476026  </td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">   0.0943057  </td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">   0.0571592  </td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">   0.0520324  </td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">   0.0270363  </td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">   0.0229244  </td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">   0.0462234  </td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 56 more trials not shown (56 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25475)\u001b[0m Tanh()\n",
      "Result for train_boston_b1aaa7a2:\n",
      "  date: 2020-11-26_12-51-28\n",
      "  done: false\n",
      "  experiment_id: 3fb440a4e679423bbcf98f51760d1088\n",
      "  experiment_tag: 69_adam=0.43202,droupout_prob=0.28698,hidden_dim=107.07,lr=0.053977,model=0.64509,n_layer=1.8246,sigmoid_func=0.62102,steps=20,weight_decay=0.037042\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3.630319695723684\n",
      "  mean_accuracy: 4.0366881521124585\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25475\n",
      "  time_since_restore: 2.5824086666107178\n",
      "  time_this_iter_s: 2.5824086666107178\n",
      "  time_total_s: 2.5824086666107178\n",
      "  timestamp: 1606391488\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b1aaa7a2\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:31,025\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 2.028385639190674 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_b1aaa7a2:\n",
      "  date: 2020-11-26_12-51-31\n",
      "  done: true\n",
      "  experiment_id: 3fb440a4e679423bbcf98f51760d1088\n",
      "  experiment_tag: 69_adam=0.43202,droupout_prob=0.28698,hidden_dim=107.07,lr=0.053977,model=0.64509,n_layer=1.8246,sigmoid_func=0.62102,steps=20,weight_decay=0.037042\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.168115314684416\n",
      "  mean_accuracy: 2.3820216530247738\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25475\n",
      "  time_since_restore: 4.866530418395996\n",
      "  time_this_iter_s: 2.0773258209228516\n",
      "  time_total_s: 4.866530418395996\n",
      "  timestamp: 1606391491\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: b1aaa7a2\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25774)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25778)\u001b[0m Sigmoid()\n",
      "Result for train_boston_b1dad436:\n",
      "  date: 2020-11-26_12-51-31\n",
      "  done: true\n",
      "  experiment_id: a82f51d2eecf48938e78cf31565a4b0c\n",
      "  experiment_tag: 71_adam=0.68391,droupout_prob=0.27468,hidden_dim=183.09,lr=0.061714,model=0.95431,n_layer=1.9698,sigmoid_func=0.35134,steps=20,weight_decay=0.049523\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 44.40352911698191\n",
      "  mean_accuracy: 44.093505859375\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25774\n",
      "  time_since_restore: 1.997809886932373\n",
      "  time_this_iter_s: 1.997809886932373\n",
      "  time_total_s: 1.997809886932373\n",
      "  timestamp: 1606391491\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b1dad436\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25768)\u001b[0m Sigmoid()\n",
      "Result for train_boston_b3512c0c:\n",
      "  date: 2020-11-26_12-51-31\n",
      "  done: false\n",
      "  experiment_id: 65df0fe852e84ea4a24964bfbd992618\n",
      "  experiment_tag: 73_adam=0.44437,droupout_prob=0.25587,hidden_dim=191.09,lr=0.048887,model=0.72174,n_layer=1.5285,sigmoid_func=0.70295,steps=20,weight_decay=0.065644\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4.174152575041118\n",
      "  mean_accuracy: 4.776528609426398\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25778\n",
      "  time_since_restore: 1.7265033721923828\n",
      "  time_this_iter_s: 1.7265033721923828\n",
      "  time_total_s: 1.7265033721923828\n",
      "  timestamp: 1606391491\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b3512c0c\n",
      "  \n",
      "Result for train_boston_b1f53d6c:\n",
      "\u001b[2m\u001b[36m(pid=25773)\u001b[0m Sigmoid()\n",
      "  date: 2020-11-26_12-51-31\n",
      "  done: true\n",
      "  experiment_id: 07f7b0af262a4442a6bd6f03ee3a13e6\n",
      "  experiment_tag: 72_adam=0.54291,droupout_prob=0.34962,hidden_dim=152.02,lr=0.070405,model=0.80326,n_layer=2.5996,sigmoid_func=0.70388,steps=20,weight_decay=0.039468\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 35.80607524671053\n",
      "  mean_accuracy: 37.242768940172695\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25768\n",
      "  time_since_restore: 2.0593769550323486\n",
      "  time_this_iter_s: 2.0593769550323486\n",
      "  time_total_s: 2.0593769550323486\n",
      "  timestamp: 1606391491\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b1f53d6c\n",
      "  \n",
      "Result for train_boston_b3680602:\n",
      "  date: 2020-11-26_12-51-31\n",
      "  done: true\n",
      "  experiment_id: da2057bbae654b639fe925efa58e8fc0\n",
      "  experiment_tag: 74_adam=0.47421,droupout_prob=0.40985,hidden_dim=142.93,lr=0.039574,model=0.26112,n_layer=2.4428,sigmoid_func=0.85116,steps=20,weight_decay=0.00083851\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 91.72627981085526\n",
      "  mean_accuracy: 99.52630293996711\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25773\n",
      "  time_since_restore: 2.0200870037078857\n",
      "  time_this_iter_s: 2.0200870037078857\n",
      "  time_total_s: 2.0200870037078857\n",
      "  timestamp: 1606391491\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b3680602\n",
      "  \n",
      "Result for train_boston_b3512c0c:\n",
      "  date: 2020-11-26_12-51-32\n",
      "  done: true\n",
      "  experiment_id: 65df0fe852e84ea4a24964bfbd992618\n",
      "  experiment_tag: 73_adam=0.44437,droupout_prob=0.25587,hidden_dim=191.09,lr=0.048887,model=0.72174,n_layer=1.5285,sigmoid_func=0.70295,steps=20,weight_decay=0.065644\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.223505321301912\n",
      "  mean_accuracy: 2.2514172604209497\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25778\n",
      "  time_since_restore: 2.929929256439209\n",
      "  time_this_iter_s: 0.4779226779937744\n",
      "  time_total_s: 2.929929256439209\n",
      "  timestamp: 1606391492\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: b3512c0c\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=69\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03299206181576377 | Iter 4.000: -1.0949524829262183 | Iter 1.000: -15.410953722502056<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 81 (7 RUNNING, 74 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_b39d9312</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.547233</td><td style=\"text-align: right;\">      0.153082 </td><td style=\"text-align: right;\">    144.683 </td><td style=\"text-align: right;\">0.0715696</td><td style=\"text-align: right;\">0.994815</td><td style=\"text-align: right;\">  1.94286</td><td style=\"text-align: right;\">      0.32218 </td><td style=\"text-align: right;\">    0.0365085 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b3d5b6a2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.362208</td><td style=\"text-align: right;\">      0.452299 </td><td style=\"text-align: right;\">     98.3715</td><td style=\"text-align: right;\">0.079424 </td><td style=\"text-align: right;\">0.502342</td><td style=\"text-align: right;\">  2.22116</td><td style=\"text-align: right;\">      0.598859</td><td style=\"text-align: right;\">    0.0184037 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b756bec0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.522346</td><td style=\"text-align: right;\">      0.205115 </td><td style=\"text-align: right;\">    136.295 </td><td style=\"text-align: right;\">0.0350616</td><td style=\"text-align: right;\">0.62359 </td><td style=\"text-align: right;\">  2.14651</td><td style=\"text-align: right;\">      0.454182</td><td style=\"text-align: right;\">    0.0417697 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b98e56ee</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.264312</td><td style=\"text-align: right;\">      0.0927229</td><td style=\"text-align: right;\">    116.877 </td><td style=\"text-align: right;\">0.0521002</td><td style=\"text-align: right;\">0.38644 </td><td style=\"text-align: right;\">  1.96333</td><td style=\"text-align: right;\">      0.627763</td><td style=\"text-align: right;\">    0.00101418</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b9bad552</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.621262</td><td style=\"text-align: right;\">      0.185881 </td><td style=\"text-align: right;\">    205.993 </td><td style=\"text-align: right;\">0.0426874</td><td style=\"text-align: right;\">0.732505</td><td style=\"text-align: right;\">  2.53372</td><td style=\"text-align: right;\">      0.408185</td><td style=\"text-align: right;\">    0.0732777 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b9ec290e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.400141</td><td style=\"text-align: right;\">      0.284487 </td><td style=\"text-align: right;\">    178.964 </td><td style=\"text-align: right;\">0.0126345</td><td style=\"text-align: right;\">0.443879</td><td style=\"text-align: right;\">  2.09371</td><td style=\"text-align: right;\">      0.498973</td><td style=\"text-align: right;\">    0.0497483 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_ba0d39d2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.459795</td><td style=\"text-align: right;\">      0.31571  </td><td style=\"text-align: right;\">    200.429 </td><td style=\"text-align: right;\">0.0674684</td><td style=\"text-align: right;\">0.561107</td><td style=\"text-align: right;\">  1.72424</td><td style=\"text-align: right;\">      0.579005</td><td style=\"text-align: right;\">    0.0503761 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">      0.260551 </td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">    0.0546216 </td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">      0.20246  </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">    0.0513878 </td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">      0.261284 </td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">    0.0349197 </td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">      0.220367 </td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">    0.0449583 </td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">      0.262921 </td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">    0.0497963 </td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">      0.119361 </td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">    0.0758267 </td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">      0.223395 </td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">    0.0476026 </td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">      0.427066 </td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">    0.0943057 </td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">      0.260289 </td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">    0.0571592 </td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">      0.181991 </td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">    0.0520324 </td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">      0.287867 </td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">    0.0270363 </td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">      0.306476 </td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">    0.0229244 </td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">      0.304245 </td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">    0.0462234 </td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 61 more trials not shown (61 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25819)\u001b[0m ReLU()\n",
      "Result for train_boston_b39d9312:\n",
      "  date: 2020-11-26_12-51-36\n",
      "  done: false\n",
      "  experiment_id: bb5c74f254bd49da8ea7928ff6ccc2ea\n",
      "  experiment_tag: 75_adam=0.54723,droupout_prob=0.15308,hidden_dim=144.68,lr=0.07157,model=0.99481,n_layer=1.9429,sigmoid_func=0.32218,steps=20,weight_decay=0.036509\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4.370360525030839\n",
      "  mean_accuracy: 3.8378468563682153\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25819\n",
      "  time_since_restore: 4.903024196624756\n",
      "  time_this_iter_s: 4.903024196624756\n",
      "  time_total_s: 4.903024196624756\n",
      "  timestamp: 1606391496\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b39d9312\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25822)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25820)\u001b[0m Tanh()\n",
      "Result for train_boston_b756bec0:\n",
      "  date: 2020-11-26_12-51-36\n",
      "  done: false\n",
      "  experiment_id: f413d49e7da046d2bcce00b0b5cb0969\n",
      "  experiment_tag: 77_adam=0.52235,droupout_prob=0.20511,hidden_dim=136.29,lr=0.035062,model=0.62359,n_layer=2.1465,sigmoid_func=0.45418,steps=20,weight_decay=0.04177\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.1658377396432975\n",
      "  mean_accuracy: 2.6538401151958264\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25822\n",
      "  time_since_restore: 5.162459373474121\n",
      "  time_this_iter_s: 5.162459373474121\n",
      "  time_total_s: 5.162459373474121\n",
      "  timestamp: 1606391496\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b756bec0\n",
      "  \n",
      "Result for train_boston_b3d5b6a2:\n",
      "  date: 2020-11-26_12-51-37\n",
      "  done: true\n",
      "  experiment_id: 0ebd40731954421b8960054f036e9ab5\n",
      "  experiment_tag: 76_adam=0.36221,droupout_prob=0.4523,hidden_dim=98.372,lr=0.079424,model=0.50234,n_layer=2.2212,sigmoid_func=0.59886,steps=20,weight_decay=0.018404\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 64.7022705078125\n",
      "  mean_accuracy: 63.69432951274671\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25820\n",
      "  time_since_restore: 5.147036790847778\n",
      "  time_this_iter_s: 5.147036790847778\n",
      "  time_total_s: 5.147036790847778\n",
      "  timestamp: 1606391497\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b3d5b6a2\n",
      "  \n",
      "Result for train_boston_b756bec0:\n",
      "  date: 2020-11-26_12-51-37\n",
      "  done: true\n",
      "  experiment_id: f413d49e7da046d2bcce00b0b5cb0969\n",
      "  experiment_tag: 77_adam=0.52235,droupout_prob=0.20511,hidden_dim=136.29,lr=0.035062,model=0.62359,n_layer=2.1465,sigmoid_func=0.45418,steps=20,weight_decay=0.04177\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.0880769428453947\n",
      "  mean_accuracy: 2.742569371273643\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25822\n",
      "  time_since_restore: 5.427899122238159\n",
      "  time_this_iter_s: 0.17535018920898438\n",
      "  time_total_s: 5.427899122238159\n",
      "  timestamp: 1606391497\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: b756bec0\n",
      "  \n",
      "Result for train_boston_b39d9312:\n",
      "  date: 2020-11-26_12-51-37\n",
      "  done: true\n",
      "  experiment_id: bb5c74f254bd49da8ea7928ff6ccc2ea\n",
      "  experiment_tag: 75_adam=0.54723,droupout_prob=0.15308,hidden_dim=144.68,lr=0.07157,model=0.99481,n_layer=1.9429,sigmoid_func=0.32218,steps=20,weight_decay=0.036509\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.03621426381562885\n",
      "  mean_accuracy: 0.03286898763556229\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25819\n",
      "  time_since_restore: 5.959066152572632\n",
      "  time_this_iter_s: 0.03751659393310547\n",
      "  time_total_s: 5.959066152572632\n",
      "  timestamp: 1606391497\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: b39d9312\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25936)\u001b[0m Tanh()\n",
      "Result for train_boston_b98e56ee:\n",
      "  date: 2020-11-26_12-51-40\n",
      "  done: true\n",
      "  experiment_id: f5e3795df0bc47e79c6a19a638a34091\n",
      "  experiment_tag: 78_adam=0.26431,droupout_prob=0.092723,hidden_dim=116.88,lr=0.0521,model=0.38644,n_layer=1.9633,sigmoid_func=0.62776,steps=20,weight_decay=0.0010142\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2000.9239309210527\n",
      "  mean_accuracy: 2123.6474095394738\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25936\n",
      "  time_since_restore: 2.212214469909668\n",
      "  time_this_iter_s: 2.212214469909668\n",
      "  time_total_s: 2.212214469909668\n",
      "  timestamp: 1606391500\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b98e56ee\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25955)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25888)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25870)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25904)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:42,169\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 2.117011070251465 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=73\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03307016037012401 | Iter 4.000: -1.0932095176295231 | Iter 1.000: -14.81445954975329<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 85 (7 RUNNING, 78 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_b9bad552</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.621262</td><td style=\"text-align: right;\">      0.185881 </td><td style=\"text-align: right;\">    205.993 </td><td style=\"text-align: right;\">0.0426874</td><td style=\"text-align: right;\">0.732505</td><td style=\"text-align: right;\">  2.53372</td><td style=\"text-align: right;\">      0.408185</td><td style=\"text-align: right;\">     0.0732777</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_b9ec290e</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.400141</td><td style=\"text-align: right;\">      0.284487 </td><td style=\"text-align: right;\">    178.964 </td><td style=\"text-align: right;\">0.0126345</td><td style=\"text-align: right;\">0.443879</td><td style=\"text-align: right;\">  2.09371</td><td style=\"text-align: right;\">      0.498973</td><td style=\"text-align: right;\">     0.0497483</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_ba0d39d2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.459795</td><td style=\"text-align: right;\">      0.31571  </td><td style=\"text-align: right;\">    200.429 </td><td style=\"text-align: right;\">0.0674684</td><td style=\"text-align: right;\">0.561107</td><td style=\"text-align: right;\">  1.72424</td><td style=\"text-align: right;\">      0.579005</td><td style=\"text-align: right;\">     0.0503761</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_ba7b7a8c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.534113</td><td style=\"text-align: right;\">      0.176892 </td><td style=\"text-align: right;\">    197.909 </td><td style=\"text-align: right;\">0.0389022</td><td style=\"text-align: right;\">0.61196 </td><td style=\"text-align: right;\">  2.14611</td><td style=\"text-align: right;\">      0.498739</td><td style=\"text-align: right;\">     0.0342538</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_bd111810</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.422742</td><td style=\"text-align: right;\">      0.234044 </td><td style=\"text-align: right;\">    171.781 </td><td style=\"text-align: right;\">0.0501579</td><td style=\"text-align: right;\">0.550436</td><td style=\"text-align: right;\">  2.01101</td><td style=\"text-align: right;\">      0.621373</td><td style=\"text-align: right;\">     0.0430053</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_bd264776</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.521283</td><td style=\"text-align: right;\">      0.0791755</td><td style=\"text-align: right;\">    157.624 </td><td style=\"text-align: right;\">0.0549776</td><td style=\"text-align: right;\">0.709891</td><td style=\"text-align: right;\">  2.0782 </td><td style=\"text-align: right;\">      0.626014</td><td style=\"text-align: right;\">     0.0525295</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_bd56fbdc</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.246312</td><td style=\"text-align: right;\">      0.205365 </td><td style=\"text-align: right;\">    146.841 </td><td style=\"text-align: right;\">0.0258175</td><td style=\"text-align: right;\">0.675798</td><td style=\"text-align: right;\">  2.00062</td><td style=\"text-align: right;\">      0.906868</td><td style=\"text-align: right;\">     0.0542552</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">      0.260551 </td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">      0.20246  </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">      0.261284 </td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">      0.220367 </td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">      0.262921 </td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">      0.119361 </td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">      0.223395 </td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">      0.427066 </td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">      0.260289 </td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">      0.181991 </td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">      0.287867 </td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">      0.306476 </td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">      0.304245 </td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 65 more trials not shown (65 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_ba7b7a8c:\n",
      "  date: 2020-11-26_12-51-41\n",
      "  done: false\n",
      "  experiment_id: 471e4396f2984201924f657bb9d191f6\n",
      "  experiment_tag: 82_adam=0.53411,droupout_prob=0.17689,hidden_dim=197.91,lr=0.038902,model=0.61196,n_layer=2.1461,sigmoid_func=0.49874,steps=20,weight_decay=0.034254\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3.021067368356805\n",
      "  mean_accuracy: 3.7872479087428044\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25870\n",
      "  time_since_restore: 2.4812865257263184\n",
      "  time_this_iter_s: 2.4812865257263184\n",
      "  time_total_s: 2.4812865257263184\n",
      "  timestamp: 1606391501\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ba7b7a8c\n",
      "  \n",
      "Result for train_boston_b9ec290e:\n",
      "  date: 2020-11-26_12-51-41\n",
      "  done: true\n",
      "  experiment_id: 8e89fe0b7ee148259885a951ee319c39\n",
      "  experiment_tag: 80_adam=0.40014,droupout_prob=0.28449,hidden_dim=178.96,lr=0.012635,model=0.44388,n_layer=2.0937,sigmoid_func=0.49897,steps=20,weight_decay=0.049748\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 121.20570775082237\n",
      "  mean_accuracy: 111.94535104851974\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25888\n",
      "  time_since_restore: 2.5232179164886475\n",
      "  time_this_iter_s: 2.5232179164886475\n",
      "  time_total_s: 2.5232179164886475\n",
      "  timestamp: 1606391501\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b9ec290e\n",
      "  \n",
      "Result for train_boston_b9bad552:\n",
      "  date: 2020-11-26_12-51-41\n",
      "  done: true\n",
      "  experiment_id: 1e06adf879c042b3a7e1a2fb29743b5b\n",
      "  experiment_tag: 79_adam=0.62126,droupout_prob=0.18588,hidden_dim=205.99,lr=0.042687,model=0.7325,n_layer=2.5337,sigmoid_func=0.40819,steps=20,weight_decay=0.073278\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 35.43006656044408\n",
      "  mean_accuracy: 32.969961065995065\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25955\n",
      "  time_since_restore: 2.317493438720703\n",
      "  time_this_iter_s: 2.317493438720703\n",
      "  time_total_s: 2.317493438720703\n",
      "  timestamp: 1606391501\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: b9bad552\n",
      "  \n",
      "Result for train_boston_ba0d39d2:\n",
      "  date: 2020-11-26_12-51-41\n",
      "  done: true\n",
      "  experiment_id: ad6f9657cad74aebb04653236d6eea43\n",
      "  experiment_tag: 81_adam=0.45979,droupout_prob=0.31571,hidden_dim=200.43,lr=0.067468,model=0.56111,n_layer=1.7242,sigmoid_func=0.579,steps=20,weight_decay=0.050376\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 66.49065198396382\n",
      "  mean_accuracy: 61.89906712582237\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25904\n",
      "  time_since_restore: 2.9885613918304443\n",
      "  time_this_iter_s: 2.9885613918304443\n",
      "  time_total_s: 2.9885613918304443\n",
      "  timestamp: 1606391501\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ba0d39d2\n",
      "  \n",
      "Result for train_boston_ba7b7a8c:\n",
      "  date: 2020-11-26_12-51-42\n",
      "  done: true\n",
      "  experiment_id: 471e4396f2984201924f657bb9d191f6\n",
      "  experiment_tag: 82_adam=0.53411,droupout_prob=0.17689,hidden_dim=197.91,lr=0.038902,model=0.61196,n_layer=2.1461,sigmoid_func=0.49874,steps=20,weight_decay=0.034254\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.6382596869217723\n",
      "  mean_accuracy: 1.6146155909488076\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25870\n",
      "  time_since_restore: 3.7555456161499023\n",
      "  time_this_iter_s: 0.38172125816345215\n",
      "  time_total_s: 3.7555456161499023\n",
      "  timestamp: 1606391502\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: ba7b7a8c\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25850)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=25818)\u001b[0m Tanh()\n",
      "Result for train_boston_bd264776:\n",
      "  date: 2020-11-26_12-51-43\n",
      "  done: false\n",
      "  experiment_id: cc497c4bafe94e699fbff2b359caed35\n",
      "  experiment_tag: 84_adam=0.52128,droupout_prob=0.079175,hidden_dim=157.62,lr=0.054978,model=0.70989,n_layer=2.0782,sigmoid_func=0.62601,steps=20,weight_decay=0.052529\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 13.383185938784951\n",
      "  mean_accuracy: 13.41952835886102\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25818\n",
      "  time_since_restore: 1.6791870594024658\n",
      "  time_this_iter_s: 1.6791870594024658\n",
      "  time_total_s: 1.6791870594024658\n",
      "  timestamp: 1606391503\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: bd264776\n",
      "  \n",
      "Result for train_boston_bd111810:\n",
      "  date: 2020-11-26_12-51-43\n",
      "  done: true\n",
      "  experiment_id: 2caa386e7de44278bdc0bb11f47f1d9f\n",
      "  experiment_tag: 83_adam=0.42274,droupout_prob=0.23404,hidden_dim=171.78,lr=0.050158,model=0.55044,n_layer=2.011,sigmoid_func=0.62137,steps=20,weight_decay=0.043005\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 35.14484927528783\n",
      "  mean_accuracy: 38.817678351151315\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25850\n",
      "  time_since_restore: 2.5987420082092285\n",
      "  time_this_iter_s: 2.5987420082092285\n",
      "  time_total_s: 2.5987420082092285\n",
      "  timestamp: 1606391503\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: bd111810\n",
      "  \n",
      "Result for train_boston_bd264776:\n",
      "  date: 2020-11-26_12-51-46\n",
      "  done: true\n",
      "  experiment_id: cc497c4bafe94e699fbff2b359caed35\n",
      "  experiment_tag: 84_adam=0.52128,droupout_prob=0.079175,hidden_dim=157.62,lr=0.054978,model=0.70989,n_layer=2.0782,sigmoid_func=0.62601,steps=20,weight_decay=0.052529\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 29.78394839638158\n",
      "  mean_accuracy: 25.582423159950658\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 25818\n",
      "  time_since_restore: 5.1827685832977295\n",
      "  time_this_iter_s: 3.3739266395568848\n",
      "  time_total_s: 5.1827685832977295\n",
      "  timestamp: 1606391506\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: bd264776\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26182)\u001b[0m Sigmoid()\n",
      "Result for train_boston_bd56fbdc:\n",
      "  date: 2020-11-26_12-51-52\n",
      "  done: true\n",
      "  experiment_id: fa8ada7d78f340a6ac59675e4296621d\n",
      "  experiment_tag: 85_adam=0.24631,droupout_prob=0.20536,hidden_dim=146.84,lr=0.025817,model=0.6758,n_layer=2.0006,sigmoid_func=0.90687,steps=20,weight_decay=0.054255\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 79.37034205386513\n",
      "  mean_accuracy: 72.11619808799342\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26182\n",
      "  time_since_restore: 4.099052667617798\n",
      "  time_this_iter_s: 4.099052667617798\n",
      "  time_total_s: 4.099052667617798\n",
      "  timestamp: 1606391512\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: bd56fbdc\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26204)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:51:54,339\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 1.6060078144073486 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=26208)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26214)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=26187)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26223)\u001b[0m Tanh()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using AsyncHyperBand: num_stopped=80\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03307016037012401 | Iter 4.000: -1.0949524829262183 | Iter 1.000: -14.676666259765625<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 92 (7 RUNNING, 85 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_c02201c2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.585283</td><td style=\"text-align: right;\">       0.168031</td><td style=\"text-align: right;\">    192.706 </td><td style=\"text-align: right;\">0.0580726 </td><td style=\"text-align: right;\">0.761556</td><td style=\"text-align: right;\">  2.77035</td><td style=\"text-align: right;\">      0.622071</td><td style=\"text-align: right;\">     0.0555236</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c03a7626</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.55804 </td><td style=\"text-align: right;\">       0.420679</td><td style=\"text-align: right;\">    133.881 </td><td style=\"text-align: right;\">0.00662518</td><td style=\"text-align: right;\">0.839039</td><td style=\"text-align: right;\">  2.07005</td><td style=\"text-align: right;\">      0.669954</td><td style=\"text-align: right;\">     0.012033 </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c0511ac0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.165168</td><td style=\"text-align: right;\">       0.252135</td><td style=\"text-align: right;\">    151.586 </td><td style=\"text-align: right;\">0.0169563 </td><td style=\"text-align: right;\">0.348973</td><td style=\"text-align: right;\">  1.68868</td><td style=\"text-align: right;\">      0.13619 </td><td style=\"text-align: right;\">     0.0333214</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c06a5f6c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.521259</td><td style=\"text-align: right;\">       0.26567 </td><td style=\"text-align: right;\">    178.269 </td><td style=\"text-align: right;\">0.0110667 </td><td style=\"text-align: right;\">0.604324</td><td style=\"text-align: right;\">  2.67193</td><td style=\"text-align: right;\">      0.116338</td><td style=\"text-align: right;\">     0.0254044</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c0882556</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.39573 </td><td style=\"text-align: right;\">       0.229489</td><td style=\"text-align: right;\">    171.26  </td><td style=\"text-align: right;\">0.0407829 </td><td style=\"text-align: right;\">0.727558</td><td style=\"text-align: right;\">  2.20729</td><td style=\"text-align: right;\">      0.373129</td><td style=\"text-align: right;\">     0.0325459</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c0ea4632</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.452183</td><td style=\"text-align: right;\">       0.200138</td><td style=\"text-align: right;\">    112.481 </td><td style=\"text-align: right;\">0.0605911 </td><td style=\"text-align: right;\">0.370068</td><td style=\"text-align: right;\">  1.87888</td><td style=\"text-align: right;\">      0.69175 </td><td style=\"text-align: right;\">     0.0139369</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_c2f9df5a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.429438</td><td style=\"text-align: right;\">       0.240486</td><td style=\"text-align: right;\">    167.28  </td><td style=\"text-align: right;\">0.0203529 </td><td style=\"text-align: right;\">0.547552</td><td style=\"text-align: right;\">  2.79207</td><td style=\"text-align: right;\">      0.492813</td><td style=\"text-align: right;\">     0.0387693</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386  </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 72 more trials not shown (72 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_c2f9df5a:\n",
      "  date: 2020-11-26_12-51-54\n",
      "  done: false\n",
      "  experiment_id: 24b1a53d233d4f78936b8792a35c2d1c\n",
      "  experiment_tag: 92_adam=0.42944,droupout_prob=0.24049,hidden_dim=167.28,lr=0.020353,model=0.54755,n_layer=2.7921,sigmoid_func=0.49281,steps=20,weight_decay=0.038769\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 5.378432022897821\n",
      "  mean_accuracy: 5.598119635331003\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26223\n",
      "  time_since_restore: 1.744170904159546\n",
      "  time_this_iter_s: 1.744170904159546\n",
      "  time_total_s: 1.744170904159546\n",
      "  timestamp: 1606391514\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c2f9df5a\n",
      "  \n",
      "Result for train_boston_c02201c2:\n",
      "  date: 2020-11-26_12-51-54\n",
      "  done: true\n",
      "  experiment_id: b18cc34ebdc84097ba1b1d52b0063c59\n",
      "  experiment_tag: 86_adam=0.58528,droupout_prob=0.16803,hidden_dim=192.71,lr=0.058073,model=0.76156,n_layer=2.7703,sigmoid_func=0.62207,steps=20,weight_decay=0.055524\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 17.12848542865954\n",
      "  mean_accuracy: 18.98404091282895\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26208\n",
      "  time_since_restore: 1.8586146831512451\n",
      "  time_this_iter_s: 1.8586146831512451\n",
      "  time_total_s: 1.8586146831512451\n",
      "  timestamp: 1606391514\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c02201c2\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26206)\u001b[0m ReLU()\n",
      "Result for train_boston_c06a5f6c:\n",
      "  date: 2020-11-26_12-51-54\n",
      "  done: true\n",
      "  experiment_id: f92b429751e94cf387ea7710423f6035\n",
      "  experiment_tag: 89_adam=0.52126,droupout_prob=0.26567,hidden_dim=178.27,lr=0.011067,model=0.60432,n_layer=2.6719,sigmoid_func=0.11634,steps=20,weight_decay=0.025404\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 49.085054096422695\n",
      "  mean_accuracy: 46.177988152754935\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26206\n",
      "  time_since_restore: 1.7034528255462646\n",
      "  time_this_iter_s: 1.7034528255462646\n",
      "  time_total_s: 1.7034528255462646\n",
      "  timestamp: 1606391514\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c06a5f6c\n",
      "  \n",
      "Result for train_boston_c03a7626:\n",
      "  date: 2020-11-26_12-51-53\n",
      "  done: true\n",
      "  experiment_id: b6003f663c0043189592251bc8855a7d\n",
      "  experiment_tag: 87_adam=0.55804,droupout_prob=0.42068,hidden_dim=133.88,lr=0.0066252,model=0.83904,n_layer=2.07,sigmoid_func=0.66995,steps=20,weight_decay=0.012033\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 58.0368074115954\n",
      "  mean_accuracy: 51.88599275287829\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26204\n",
      "  time_since_restore: 1.6398422718048096\n",
      "  time_this_iter_s: 1.6398422718048096\n",
      "  time_total_s: 1.6398422718048096\n",
      "  timestamp: 1606391513\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c03a7626\n",
      "  \n",
      "Result for train_boston_c0511ac0:\n",
      "  date: 2020-11-26_12-51-54\n",
      "  done: false\n",
      "  experiment_id: b56cb7136eac4493bd2720dd81e13c13\n",
      "  experiment_tag: 88_adam=0.16517,droupout_prob=0.25213,hidden_dim=151.59,lr=0.016956,model=0.34897,n_layer=1.6887,sigmoid_func=0.13619,steps=20,weight_decay=0.033321\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 9.897818314401727\n",
      "  mean_accuracy: 8.824425145199424\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26214\n",
      "  time_since_restore: 1.8850247859954834\n",
      "  time_this_iter_s: 1.8850247859954834\n",
      "  time_total_s: 1.8850247859954834\n",
      "  timestamp: 1606391514\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c0511ac0\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26222)\u001b[0m Sigmoid()\n",
      "Result for train_boston_c0882556:\n",
      "  date: 2020-11-26_12-51-54\n",
      "  done: true\n",
      "  experiment_id: f9194028216742e0b66941038edc04d9\n",
      "  experiment_tag: 90_adam=0.39573,droupout_prob=0.22949,hidden_dim=171.26,lr=0.040783,model=0.72756,n_layer=2.2073,sigmoid_func=0.37313,steps=20,weight_decay=0.032546\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 26.98904579564145\n",
      "  mean_accuracy: 23.72449533562911\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26187\n",
      "  time_since_restore: 2.200023651123047\n",
      "  time_this_iter_s: 2.200023651123047\n",
      "  time_total_s: 2.200023651123047\n",
      "  timestamp: 1606391514\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c0882556\n",
      "  \n",
      "Result for train_boston_c0ea4632:\n",
      "  date: 2020-11-26_12-51-55\n",
      "  done: true\n",
      "  experiment_id: 03fd2690ef7b413a93d7e8943b64f6b2\n",
      "  experiment_tag: 91_adam=0.45218,droupout_prob=0.20014,hidden_dim=112.48,lr=0.060591,model=0.37007,n_layer=1.8789,sigmoid_func=0.69175,steps=20,weight_decay=0.013937\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 79633.08552631579\n",
      "  mean_accuracy: 78875.82236842105\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26222\n",
      "  time_since_restore: 2.0990049839019775\n",
      "  time_this_iter_s: 2.0990049839019775\n",
      "  time_total_s: 2.0990049839019775\n",
      "  timestamp: 1606391515\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c0ea4632\n",
      "  \n",
      "Result for train_boston_c2f9df5a:\n",
      "  date: 2020-11-26_12-51-55\n",
      "  done: true\n",
      "  experiment_id: 24b1a53d233d4f78936b8792a35c2d1c\n",
      "  experiment_tag: 92_adam=0.42944,droupout_prob=0.24049,hidden_dim=167.28,lr=0.020353,model=0.54755,n_layer=2.7921,sigmoid_func=0.49281,steps=20,weight_decay=0.038769\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.5579010812859786\n",
      "  mean_accuracy: 2.656464827688117\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26223\n",
      "  time_since_restore: 2.7165122032165527\n",
      "  time_this_iter_s: 0.36925220489501953\n",
      "  time_total_s: 2.7165122032165527\n",
      "  timestamp: 1606391515\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: c2f9df5a\n",
      "  \n",
      "Result for train_boston_c0511ac0:\n",
      "  date: 2020-11-26_12-51-55\n",
      "  done: true\n",
      "  experiment_id: b56cb7136eac4493bd2720dd81e13c13\n",
      "  experiment_tag: 88_adam=0.16517,droupout_prob=0.25213,hidden_dim=151.59,lr=0.016956,model=0.34897,n_layer=1.6887,sigmoid_func=0.13619,steps=20,weight_decay=0.033321\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 156.41288034539474\n",
      "  mean_accuracy: 140.98195929276315\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26214\n",
      "  time_since_restore: 3.3008480072021484\n",
      "  time_this_iter_s: 0.4729580879211426\n",
      "  time_total_s: 3.3008480072021484\n",
      "  timestamp: 1606391515\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: c0511ac0\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26366)\u001b[0m ReLU()\n",
      "Result for train_boston_c75c4c86:\n",
      "  date: 2020-11-26_12-52-05\n",
      "  done: false\n",
      "  experiment_id: 0f11cc6e7a0e480bac293d2083debc06\n",
      "  experiment_tag: 93_adam=0.3043,droupout_prob=0.24676,hidden_dim=132.33,lr=0.035588,model=0.5631,n_layer=2.2264,sigmoid_func=0.31293,steps=20,weight_decay=0.047864\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 5.342864588687294\n",
      "  mean_accuracy: 5.56876614219264\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26366\n",
      "  time_since_restore: 4.776719331741333\n",
      "  time_this_iter_s: 4.776719331741333\n",
      "  time_total_s: 4.776719331741333\n",
      "  timestamp: 1606391525\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c75c4c86\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:52:09,130\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 3.6941659450531006 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using AsyncHyperBand: num_stopped=87\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03307016037012401 | Iter 4.000: -1.0966954482229132 | Iter 1.000: -13.383185938784951<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 100 (8 RUNNING, 92 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_c75c4c86</td><td>RUNNING   </td><td>192.168.1.34:26366</td><td style=\"text-align: right;\">0.304298</td><td style=\"text-align: right;\">      0.246762 </td><td style=\"text-align: right;\">    132.332 </td><td style=\"text-align: right;\">0.0355878 </td><td style=\"text-align: right;\">0.563097</td><td style=\"text-align: right;\">  2.22637</td><td style=\"text-align: right;\">      0.312926</td><td style=\"text-align: right;\">    0.0478637 </td><td style=\"text-align: right;\">     5.56877  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.77672</td><td style=\"text-align: right;\">     5.34286  </td></tr>\n",
       "<tr><td>train_boston_c77aef7e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.314747</td><td style=\"text-align: right;\">      0.108231 </td><td style=\"text-align: right;\">    133.761 </td><td style=\"text-align: right;\">0.00115683</td><td style=\"text-align: right;\">0.55424 </td><td style=\"text-align: right;\">  2.66099</td><td style=\"text-align: right;\">      0.841634</td><td style=\"text-align: right;\">    0.0792514 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c79444c4</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.465198</td><td style=\"text-align: right;\">      0.201419 </td><td style=\"text-align: right;\">    165.285 </td><td style=\"text-align: right;\">0.0453268 </td><td style=\"text-align: right;\">0.501317</td><td style=\"text-align: right;\">  1.95597</td><td style=\"text-align: right;\">      0.833234</td><td style=\"text-align: right;\">    0.0255069 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c7a4fe72</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.348503</td><td style=\"text-align: right;\">      0.223727 </td><td style=\"text-align: right;\">    166.284 </td><td style=\"text-align: right;\">0.0400411 </td><td style=\"text-align: right;\">0.490048</td><td style=\"text-align: right;\">  1.77199</td><td style=\"text-align: right;\">      0.287644</td><td style=\"text-align: right;\">    0.0289654 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c7c7a760</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.916014</td><td style=\"text-align: right;\">      0.0366582</td><td style=\"text-align: right;\">    160.643 </td><td style=\"text-align: right;\">0.00548437</td><td style=\"text-align: right;\">0.861054</td><td style=\"text-align: right;\">  2.84773</td><td style=\"text-align: right;\">      0.414485</td><td style=\"text-align: right;\">    0.0562033 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c7e683ec</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.633669</td><td style=\"text-align: right;\">      0.145897 </td><td style=\"text-align: right;\">    141.157 </td><td style=\"text-align: right;\">0.0662082 </td><td style=\"text-align: right;\">0.610333</td><td style=\"text-align: right;\">  2.08791</td><td style=\"text-align: right;\">      0.606378</td><td style=\"text-align: right;\">    0.00271616</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c803a08a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.318625</td><td style=\"text-align: right;\">      0.241181 </td><td style=\"text-align: right;\">    177.937 </td><td style=\"text-align: right;\">0.0354724 </td><td style=\"text-align: right;\">0.751875</td><td style=\"text-align: right;\">  2.25168</td><td style=\"text-align: right;\">      0.399819</td><td style=\"text-align: right;\">    0.015136  </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_c82faafe</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.574652</td><td style=\"text-align: right;\">      0.173798 </td><td style=\"text-align: right;\">    151.4   </td><td style=\"text-align: right;\">0.0488563 </td><td style=\"text-align: right;\">0.663589</td><td style=\"text-align: right;\">  2.10754</td><td style=\"text-align: right;\">      0.524317</td><td style=\"text-align: right;\">    0.0474275 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">      0.260551 </td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">    0.0546216 </td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">      0.20246  </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">    0.0513878 </td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">      0.261284 </td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">    0.0349197 </td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">      0.220367 </td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">    0.0449583 </td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">      0.262921 </td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">    0.0497963 </td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">      0.119361 </td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">    0.0758267 </td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">      0.223395 </td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">    0.0476026 </td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">      0.427066 </td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">    0.0943057 </td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">      0.260289 </td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">    0.0571592 </td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">      0.181991 </td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">    0.0520324 </td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">      0.287867 </td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">    0.0270363 </td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">      0.306476 </td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">    0.0229244 </td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 80 more trials not shown (80 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_c75c4c86:\n",
      "  date: 2020-11-26_12-52-10\n",
      "  done: false\n",
      "  experiment_id: 0f11cc6e7a0e480bac293d2083debc06\n",
      "  experiment_tag: 93_adam=0.3043,droupout_prob=0.24676,hidden_dim=132.33,lr=0.035588,model=0.5631,n_layer=2.2264,sigmoid_func=0.31293,steps=20,weight_decay=0.047864\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 13\n",
      "  loss: 0.024229586124420166\n",
      "  mean_accuracy: 0.024445811384602598\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26366\n",
      "  time_since_restore: 9.867937088012695\n",
      "  time_this_iter_s: 0.5685234069824219\n",
      "  time_total_s: 9.867937088012695\n",
      "  timestamp: 1606391530\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 13\n",
      "  trial_id: c75c4c86\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:52:11,501\tWARNING util.py:139 -- The `process_trial` operation took 1.0001411437988281 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=26492)\u001b[0m Sigmoid()\n",
      "Result for train_boston_c79444c4:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: false\n",
      "  experiment_id: 50d0ec6137d544cca540cd03129e4b3d\n",
      "  experiment_tag: 95_adam=0.4652,droupout_prob=0.20142,hidden_dim=165.28,lr=0.045327,model=0.50132,n_layer=1.956,sigmoid_func=0.83323,steps=20,weight_decay=0.025507\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 12.855429398386102\n",
      "  mean_accuracy: 16.05647036903783\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26492\n",
      "  time_since_restore: 3.883866310119629\n",
      "  time_this_iter_s: 3.883866310119629\n",
      "  time_total_s: 3.883866310119629\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c79444c4\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26495)\u001b[0m Sigmoid()\n",
      "Result for train_boston_c77aef7e:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: false\n",
      "  experiment_id: cc2d33cebbf04efb983baf3d96a46fc4\n",
      "  experiment_tag: 94_adam=0.31475,droupout_prob=0.10823,hidden_dim=133.76,lr=0.0011568,model=0.55424,n_layer=2.661,sigmoid_func=0.84163,steps=20,weight_decay=0.079251\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 7.251292981599507\n",
      "  mean_accuracy: 6.765624196905839\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26495\n",
      "  time_since_restore: 3.7302002906799316\n",
      "  time_this_iter_s: 3.7302002906799316\n",
      "  time_total_s: 3.7302002906799316\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c77aef7e\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26501)\u001b[0m Tanh()\n",
      "Result for train_boston_c79444c4:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: 50d0ec6137d544cca540cd03129e4b3d\n",
      "  experiment_tag: 95_adam=0.4652,droupout_prob=0.20142,hidden_dim=165.28,lr=0.045327,model=0.50132,n_layer=1.956,sigmoid_func=0.83323,steps=20,weight_decay=0.025507\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.11827930651213\n",
      "  mean_accuracy: 4.31005859375\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26492\n",
      "  time_since_restore: 4.339155673980713\n",
      "  time_this_iter_s: 0.2779226303100586\n",
      "  time_total_s: 4.339155673980713\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: c79444c4\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26498)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=26504)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26496)\u001b[0m Tanh()\n",
      "Result for train_boston_c7e683ec:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: 17259764f50b4286ae9a9e6cc148f48c\n",
      "  experiment_tag: 98_adam=0.63367,droupout_prob=0.1459,hidden_dim=141.16,lr=0.066208,model=0.61033,n_layer=2.0879,sigmoid_func=0.60638,steps=20,weight_decay=0.0027162\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 35.9216951069079\n",
      "  mean_accuracy: 32.956205669202305\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26504\n",
      "  time_since_restore: 3.744457483291626\n",
      "  time_this_iter_s: 3.744457483291626\n",
      "  time_total_s: 3.744457483291626\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c7e683ec\n",
      "  \n",
      "Result for train_boston_c7a4fe72:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: db5a1241912241bfb5f73a04a6773f3c\n",
      "  experiment_tag: 96_adam=0.3485,droupout_prob=0.22373,hidden_dim=166.28,lr=0.040041,model=0.49005,n_layer=1.772,sigmoid_func=0.28764,steps=20,weight_decay=0.028965\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1554.08984375\n",
      "  mean_accuracy: 1597.5386513157894\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26498\n",
      "  time_since_restore: 4.018697023391724\n",
      "  time_this_iter_s: 4.018697023391724\n",
      "  time_total_s: 4.018697023391724\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c7a4fe72\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26500)\u001b[0m Tanh()\n",
      "Result for train_boston_c7c7a760:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: e5b6c34fab6e404b808bb91c09d05689\n",
      "  experiment_tag: 97_adam=0.91601,droupout_prob=0.036658,hidden_dim=160.64,lr=0.0054844,model=0.86105,n_layer=2.8477,sigmoid_func=0.41448,steps=20,weight_decay=0.056203\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 96.01190506784539\n",
      "  mean_accuracy: 90.02304559004934\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26496\n",
      "  time_since_restore: 4.024054527282715\n",
      "  time_this_iter_s: 4.024054527282715\n",
      "  time_total_s: 4.024054527282715\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c7c7a760\n",
      "  \n",
      "Result for train_boston_c77aef7e:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: cc2d33cebbf04efb983baf3d96a46fc4\n",
      "  experiment_tag: 94_adam=0.31475,droupout_prob=0.10823,hidden_dim=133.76,lr=0.0011568,model=0.55424,n_layer=2.661,sigmoid_func=0.84163,steps=20,weight_decay=0.079251\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.8929973401521383\n",
      "  mean_accuracy: 3.171024523283306\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26495\n",
      "  time_since_restore: 4.20959734916687\n",
      "  time_this_iter_s: 0.28809523582458496\n",
      "  time_total_s: 4.20959734916687\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: c77aef7e\n",
      "  \n",
      "Result for train_boston_c803a08a:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: 34dc09e91dc244ac9846b50671a74a09\n",
      "  experiment_tag: 99_adam=0.31863,droupout_prob=0.24118,hidden_dim=177.94,lr=0.035472,model=0.75188,n_layer=2.2517,sigmoid_func=0.39982,steps=20,weight_decay=0.015136\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 30.667856316817435\n",
      "  mean_accuracy: 30.02217503597862\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26501\n",
      "  time_since_restore: 3.8440983295440674\n",
      "  time_this_iter_s: 3.8440983295440674\n",
      "  time_total_s: 3.8440983295440674\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c803a08a\n",
      "  \n",
      "Result for train_boston_c82faafe:\n",
      "  date: 2020-11-26_12-52-12\n",
      "  done: true\n",
      "  experiment_id: dbb2871eb8744f288b282ae77fc9b86a\n",
      "  experiment_tag: 100_adam=0.57465,droupout_prob=0.1738,hidden_dim=151.4,lr=0.048856,model=0.66359,n_layer=2.1075,sigmoid_func=0.52432,steps=20,weight_decay=0.047428\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 19.224228708367598\n",
      "  mean_accuracy: 18.11334870990954\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26500\n",
      "  time_since_restore: 3.7831718921661377\n",
      "  time_this_iter_s: 3.7831718921661377\n",
      "  time_total_s: 3.7831718921661377\n",
      "  timestamp: 1606391532\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c82faafe\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26650)\u001b[0m ReLU()\n",
      "Result for train_boston_d2043d74:\n",
      "  date: 2020-11-26_12-52-19\n",
      "  done: false\n",
      "  experiment_id: 4643fb95ae294263b8145d91db9091a3\n",
      "  experiment_tag: 101_adam=0.24963,droupout_prob=0.44192,hidden_dim=138.74,lr=0.038852,model=0.50271,n_layer=1.997,sigmoid_func=0.25666,steps=20,weight_decay=0.014023\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 8.775319952713815\n",
      "  mean_accuracy: 8.852045962685033\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26650\n",
      "  time_since_restore: 1.6537199020385742\n",
      "  time_this_iter_s: 1.6537199020385742\n",
      "  time_total_s: 1.6537199020385742\n",
      "  timestamp: 1606391539\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d2043d74\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26612)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26634)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=26575)\u001b[0m Sigmoid()\n",
      "\u001b[2m\u001b[36m(pid=26598)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26502)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26571)\u001b[0m ReLU()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:52:23,689\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 4.006011486053467 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using AsyncHyperBand: num_stopped=94\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.03172753045433446 | Iter 4.000: -1.0958239655745656 | Iter 1.000: -12.855429398386102<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 108 (8 RUNNING, 100 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_d2043d74</td><td>RUNNING   </td><td>192.168.1.34:26650</td><td style=\"text-align: right;\">0.249628</td><td style=\"text-align: right;\">       0.441918</td><td style=\"text-align: right;\">    138.743 </td><td style=\"text-align: right;\">0.0388521</td><td style=\"text-align: right;\">0.502709</td><td style=\"text-align: right;\">  1.99702</td><td style=\"text-align: right;\">      0.256661</td><td style=\"text-align: right;\">     0.0140234</td><td style=\"text-align: right;\">     8.85205  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.65372</td><td style=\"text-align: right;\">     8.77532  </td></tr>\n",
       "<tr><td>train_boston_d22b0e54</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.328407</td><td style=\"text-align: right;\">       0.240359</td><td style=\"text-align: right;\">    248.086 </td><td style=\"text-align: right;\">0.0386214</td><td style=\"text-align: right;\">0.268264</td><td style=\"text-align: right;\">  2.42006</td><td style=\"text-align: right;\">      0.887825</td><td style=\"text-align: right;\">     0.0482474</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d23b1bfa</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.632657</td><td style=\"text-align: right;\">       0.30583 </td><td style=\"text-align: right;\">    199.382 </td><td style=\"text-align: right;\">0.0283715</td><td style=\"text-align: right;\">0.645763</td><td style=\"text-align: right;\">  1.64681</td><td style=\"text-align: right;\">      0.313485</td><td style=\"text-align: right;\">     0.042413 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d24c24a4</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.305587</td><td style=\"text-align: right;\">       0.229061</td><td style=\"text-align: right;\">    126.225 </td><td style=\"text-align: right;\">0.0309762</td><td style=\"text-align: right;\">0.806676</td><td style=\"text-align: right;\">  2.06026</td><td style=\"text-align: right;\">      0.40201 </td><td style=\"text-align: right;\">     0.0441451</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d25ddeba</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.459255</td><td style=\"text-align: right;\">       0.286607</td><td style=\"text-align: right;\">    154.31  </td><td style=\"text-align: right;\">0.0472496</td><td style=\"text-align: right;\">0.588179</td><td style=\"text-align: right;\">  1.77063</td><td style=\"text-align: right;\">      0.363265</td><td style=\"text-align: right;\">     0.0527194</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d26a772e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.219714</td><td style=\"text-align: right;\">       0.242229</td><td style=\"text-align: right;\">    191.99  </td><td style=\"text-align: right;\">0.0361168</td><td style=\"text-align: right;\">0.791222</td><td style=\"text-align: right;\">  2.09743</td><td style=\"text-align: right;\">      0.320401</td><td style=\"text-align: right;\">     0.0485871</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d27bec2a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.424446</td><td style=\"text-align: right;\">       0.229613</td><td style=\"text-align: right;\">    181.607 </td><td style=\"text-align: right;\">0.0289984</td><td style=\"text-align: right;\">0.763126</td><td style=\"text-align: right;\">  1.73589</td><td style=\"text-align: right;\">      0.367082</td><td style=\"text-align: right;\">     0.0374044</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d28eb92c</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.512514</td><td style=\"text-align: right;\">       0.122741</td><td style=\"text-align: right;\">    163.015 </td><td style=\"text-align: right;\">0.0245732</td><td style=\"text-align: right;\">0.538402</td><td style=\"text-align: right;\">  1.71663</td><td style=\"text-align: right;\">      0.337302</td><td style=\"text-align: right;\">     0.0520644</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 88 more trials not shown (88 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_d26a772e:\n",
      "  date: 2020-11-26_12-52-22\n",
      "  done: false\n",
      "  experiment_id: 30bc648bbf0a44638ba3ca25650aedd2\n",
      "  experiment_tag: 106_adam=0.21971,droupout_prob=0.24223,hidden_dim=191.99,lr=0.036117,model=0.79122,n_layer=2.0974,sigmoid_func=0.3204,steps=20,weight_decay=0.048587\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8004114251387747\n",
      "  mean_accuracy: 0.8387542523835835\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26571\n",
      "  time_since_restore: 3.0389840602874756\n",
      "  time_this_iter_s: 3.0389840602874756\n",
      "  time_total_s: 3.0389840602874756\n",
      "  timestamp: 1606391542\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d26a772e\n",
      "  \n",
      "Result for train_boston_d25ddeba:\n",
      "  date: 2020-11-26_12-52-20\n",
      "  done: true\n",
      "  experiment_id: a2e435dcccd84c68a691250dc8042a72\n",
      "  experiment_tag: 105_adam=0.45926,droupout_prob=0.28661,hidden_dim=154.31,lr=0.04725,model=0.58818,n_layer=1.7706,sigmoid_func=0.36327,steps=20,weight_decay=0.052719\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 14.983279579564146\n",
      "  mean_accuracy: 14.240735505756579\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26598\n",
      "  time_since_restore: 1.8749189376831055\n",
      "  time_this_iter_s: 1.8749189376831055\n",
      "  time_total_s: 1.8749189376831055\n",
      "  timestamp: 1606391540\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d25ddeba\n",
      "  \n",
      "Result for train_boston_d27bec2a:\n",
      "  date: 2020-11-26_12-52-21\n",
      "  done: false\n",
      "  experiment_id: 1f072448e6d24d9b83328d591c0abebc\n",
      "  experiment_tag: 107_adam=0.42445,droupout_prob=0.22961,hidden_dim=181.61,lr=0.028998,model=0.76313,n_layer=1.7359,sigmoid_func=0.36708,steps=20,weight_decay=0.037404\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.4310764513517681\n",
      "  mean_accuracy: 0.40172413775795385\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26502\n",
      "  time_since_restore: 2.125633478164673\n",
      "  time_this_iter_s: 2.125633478164673\n",
      "  time_total_s: 2.125633478164673\n",
      "  timestamp: 1606391541\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d27bec2a\n",
      "  \n",
      "Result for train_boston_d24c24a4:\n",
      "  date: 2020-11-26_12-52-20\n",
      "  done: false\n",
      "  experiment_id: da38aec29add456f83b16f250db8c863\n",
      "  experiment_tag: 104_adam=0.30559,droupout_prob=0.22906,hidden_dim=126.23,lr=0.030976,model=0.80668,n_layer=2.0603,sigmoid_func=0.40201,steps=20,weight_decay=0.044145\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8110456466674805\n",
      "  mean_accuracy: 0.7281036878886976\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26612\n",
      "  time_since_restore: 1.9235949516296387\n",
      "  time_this_iter_s: 1.9235949516296387\n",
      "  time_total_s: 1.9235949516296387\n",
      "  timestamp: 1606391540\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d24c24a4\n",
      "  \n",
      "Result for train_boston_d22b0e54:\n",
      "  date: 2020-11-26_12-52-20\n",
      "  done: true\n",
      "  experiment_id: 1c2c2fb09a8f42c5b6163d68f1d56557\n",
      "  experiment_tag: 102_adam=0.32841,droupout_prob=0.24036,hidden_dim=248.09,lr=0.038621,model=0.26826,n_layer=2.4201,sigmoid_func=0.88782,steps=20,weight_decay=0.048247\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 14927.065789473685\n",
      "  mean_accuracy: 13102.949835526315\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26575\n",
      "  time_since_restore: 1.9015352725982666\n",
      "  time_this_iter_s: 1.9015352725982666\n",
      "  time_total_s: 1.9015352725982666\n",
      "  timestamp: 1606391540\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d22b0e54\n",
      "  \n",
      "Result for train_boston_d2043d74:\n",
      "  date: 2020-11-26_12-52-24\n",
      "  done: true\n",
      "  experiment_id: 4643fb95ae294263b8145d91db9091a3\n",
      "  experiment_tag: 101_adam=0.24963,droupout_prob=0.44192,hidden_dim=138.74,lr=0.038852,model=0.50271,n_layer=1.997,sigmoid_func=0.25666,steps=20,weight_decay=0.014023\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.3348201952482526\n",
      "  mean_accuracy: 1.5718551434968646\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26650\n",
      "  time_since_restore: 6.074611663818359\n",
      "  time_this_iter_s: 0.21364927291870117\n",
      "  time_total_s: 6.074611663818359\n",
      "  timestamp: 1606391544\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d2043d74\n",
      "  \n",
      "Result for train_boston_d23b1bfa:\n",
      "  date: 2020-11-26_12-52-20\n",
      "  done: false\n",
      "  experiment_id: 81e22c0523284f7d9b4e6bb33d8999cb\n",
      "  experiment_tag: 103_adam=0.63266,droupout_prob=0.30583,hidden_dim=199.38,lr=0.028371,model=0.64576,n_layer=1.6468,sigmoid_func=0.31348,steps=20,weight_decay=0.042413\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 3.557462591873972\n",
      "  mean_accuracy: 4.01645218698602\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26634\n",
      "  time_since_restore: 1.9859602451324463\n",
      "  time_this_iter_s: 1.9859602451324463\n",
      "  time_total_s: 1.9859602451324463\n",
      "  timestamp: 1606391540\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d23b1bfa\n",
      "  \n",
      "Result for train_boston_d26a772e:\n",
      "  date: 2020-11-26_12-52-24\n",
      "  done: true\n",
      "  experiment_id: 30bc648bbf0a44638ba3ca25650aedd2\n",
      "  experiment_tag: 106_adam=0.21971,droupout_prob=0.24223,hidden_dim=191.99,lr=0.036117,model=0.79122,n_layer=2.0974,sigmoid_func=0.3204,steps=20,weight_decay=0.048587\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.6422304856149774\n",
      "  mean_accuracy: 1.1066091437088816\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26571\n",
      "  time_since_restore: 4.777222394943237\n",
      "  time_this_iter_s: 0.38892507553100586\n",
      "  time_total_s: 4.777222394943237\n",
      "  timestamp: 1606391544\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d26a772e\n",
      "  \n",
      "Result for train_boston_d24c24a4:\n",
      "  date: 2020-11-26_12-52-25\n",
      "  done: true\n",
      "  experiment_id: da38aec29add456f83b16f250db8c863\n",
      "  experiment_tag: 104_adam=0.30559,droupout_prob=0.22906,hidden_dim=126.23,lr=0.030976,model=0.80668,n_layer=2.0603,sigmoid_func=0.40201,steps=20,weight_decay=0.044145\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.24946782463475278\n",
      "  mean_accuracy: 0.2665586471557617\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26612\n",
      "  time_since_restore: 7.041380882263184\n",
      "  time_this_iter_s: 0.0557248592376709\n",
      "  time_total_s: 7.041380882263184\n",
      "  timestamp: 1606391545\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: d24c24a4\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26891)\u001b[0m Tanh()\n",
      "Result for train_boston_d28eb92c:\n",
      "  date: 2020-11-26_12-52-25\n",
      "  done: false\n",
      "  experiment_id: 6d7b6e12d26e4552910c66e62d3fa664\n",
      "  experiment_tag: 108_adam=0.51251,droupout_prob=0.12274,hidden_dim=163.02,lr=0.024573,model=0.5384,n_layer=1.7166,sigmoid_func=0.3373,steps=20,weight_decay=0.052064\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4.105044314735814\n",
      "  mean_accuracy: 5.907701994243421\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26891\n",
      "  time_since_restore: 2.0922367572784424\n",
      "  time_this_iter_s: 2.0922367572784424\n",
      "  time_total_s: 2.0922367572784424\n",
      "  timestamp: 1606391545\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d28eb92c\n",
      "  \n",
      "Result for train_boston_d28eb92c:\n",
      "  date: 2020-11-26_12-52-26\n",
      "  done: true\n",
      "  experiment_id: 6d7b6e12d26e4552910c66e62d3fa664\n",
      "  experiment_tag: 108_adam=0.51251,droupout_prob=0.12274,hidden_dim=163.02,lr=0.024573,model=0.5384,n_layer=1.7166,sigmoid_func=0.3373,steps=20,weight_decay=0.052064\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.2262957723517167\n",
      "  mean_accuracy: 1.8424594276829769\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26891\n",
      "  time_since_restore: 2.6267101764678955\n",
      "  time_this_iter_s: 0.35293006896972656\n",
      "  time_total_s: 2.6267101764678955\n",
      "  timestamp: 1606391546\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d28eb92c\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26926)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=26931)\u001b[0m Tanh()\n",
      "Result for train_boston_d8dde582:\n",
      "  date: 2020-11-26_12-52-32\n",
      "  done: false\n",
      "  experiment_id: eaf776ba8b97429bb19e9affa4df17e4\n",
      "  experiment_tag: 109_adam=0.089556,droupout_prob=0.083594,hidden_dim=166.92,lr=0.0061609,model=0.36667,n_layer=2.3954,sigmoid_func=0.39699,steps=20,weight_decay=0.037879\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 9.493642706620065\n",
      "  mean_accuracy: 8.500893843801398\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26926\n",
      "  time_since_restore: 2.3552446365356445\n",
      "  time_this_iter_s: 2.3552446365356445\n",
      "  time_total_s: 2.3552446365356445\n",
      "  timestamp: 1606391552\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d8dde582\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=26929)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:52:35,140\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 2.433626890182495 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using AsyncHyperBand: num_stopped=100\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.0296166939170737 | Iter 4.000: -1.058819795909681 | Iter 1.000: -10.115929051449424<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 116 (8 RUNNING, 108 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">     adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_d8dde582</td><td>RUNNING   </td><td>192.168.1.34:26926</td><td style=\"text-align: right;\">0.0895564</td><td style=\"text-align: right;\">       0.083594</td><td style=\"text-align: right;\">    166.922 </td><td style=\"text-align: right;\">0.00616087</td><td style=\"text-align: right;\">0.36667 </td><td style=\"text-align: right;\">  2.39537</td><td style=\"text-align: right;\">      0.396986</td><td style=\"text-align: right;\">     0.0378791</td><td style=\"text-align: right;\">     8.50089  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.35524</td><td style=\"text-align: right;\">     9.49364  </td></tr>\n",
       "<tr><td>train_boston_d90204da</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.317111 </td><td style=\"text-align: right;\">       0.286415</td><td style=\"text-align: right;\">    154.64  </td><td style=\"text-align: right;\">0.0463643 </td><td style=\"text-align: right;\">0.668853</td><td style=\"text-align: right;\">  2.00882</td><td style=\"text-align: right;\">      0.410662</td><td style=\"text-align: right;\">     0.0331191</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d9133264</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.575527 </td><td style=\"text-align: right;\">       0.157671</td><td style=\"text-align: right;\">    163.32  </td><td style=\"text-align: right;\">0.0575224 </td><td style=\"text-align: right;\">0.703595</td><td style=\"text-align: right;\">  1.90434</td><td style=\"text-align: right;\">      0.60033 </td><td style=\"text-align: right;\">     0.0494504</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d92939c4</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.194536 </td><td style=\"text-align: right;\">       0.13331 </td><td style=\"text-align: right;\">    155.238 </td><td style=\"text-align: right;\">0.0280468 </td><td style=\"text-align: right;\">0.750081</td><td style=\"text-align: right;\">  1.46877</td><td style=\"text-align: right;\">      0.322766</td><td style=\"text-align: right;\">     0.041729 </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_d9c93cda</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.422027 </td><td style=\"text-align: right;\">       0.233265</td><td style=\"text-align: right;\">    156.662 </td><td style=\"text-align: right;\">0.0524353 </td><td style=\"text-align: right;\">0.577937</td><td style=\"text-align: right;\">  2.00133</td><td style=\"text-align: right;\">      0.525987</td><td style=\"text-align: right;\">     0.0361351</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_da14be9e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.555371 </td><td style=\"text-align: right;\">       0.265831</td><td style=\"text-align: right;\">    202.264 </td><td style=\"text-align: right;\">0.0415319 </td><td style=\"text-align: right;\">0.737418</td><td style=\"text-align: right;\">  1.94238</td><td style=\"text-align: right;\">      0.558193</td><td style=\"text-align: right;\">     0.0577774</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_da1f242e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.368043 </td><td style=\"text-align: right;\">       0.345821</td><td style=\"text-align: right;\">    164.035 </td><td style=\"text-align: right;\">0.0633466 </td><td style=\"text-align: right;\">0.650749</td><td style=\"text-align: right;\">  1.82483</td><td style=\"text-align: right;\">      0.360626</td><td style=\"text-align: right;\">     0.0491105</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_da85479a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.645727 </td><td style=\"text-align: right;\">       0.287729</td><td style=\"text-align: right;\">    191.326 </td><td style=\"text-align: right;\">0.0444176 </td><td style=\"text-align: right;\">0.861246</td><td style=\"text-align: right;\">  2.09999</td><td style=\"text-align: right;\">      0.810171</td><td style=\"text-align: right;\">     0.0236152</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419 </td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123 </td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193 </td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114 </td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835 </td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813 </td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524 </td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177 </td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349 </td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636 </td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124 </td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248 </td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 96 more trials not shown (96 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_d90204da:\n",
      "  date: 2020-11-26_12-52-32\n",
      "  done: false\n",
      "  experiment_id: 96d818501db340d9907d901c13a38a94\n",
      "  experiment_tag: 110_adam=0.31711,droupout_prob=0.28641,hidden_dim=154.64,lr=0.046364,model=0.66885,n_layer=2.0088,sigmoid_func=0.41066,steps=20,weight_decay=0.033119\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 8.494441785310444\n",
      "  mean_accuracy: 8.844547472502056\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26931\n",
      "  time_since_restore: 2.1795644760131836\n",
      "  time_this_iter_s: 2.1795644760131836\n",
      "  time_total_s: 2.1795644760131836\n",
      "  timestamp: 1606391552\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d90204da\n",
      "  \n",
      "Result for train_boston_d9133264:\n",
      "  date: 2020-11-26_12-52-32\n",
      "  done: false\n",
      "  experiment_id: 83ff5b2fc30a404c8a7681944758782e\n",
      "  experiment_tag: 111_adam=0.57553,droupout_prob=0.15767,hidden_dim=163.32,lr=0.057522,model=0.70359,n_layer=1.9043,sigmoid_func=0.60033,steps=20,weight_decay=0.04945\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 4.068482248406661\n",
      "  mean_accuracy: 4.224908126027961\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26929\n",
      "  time_since_restore: 2.2693588733673096\n",
      "  time_this_iter_s: 2.2693588733673096\n",
      "  time_total_s: 2.2693588733673096\n",
      "  timestamp: 1606391552\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d9133264\n",
      "  \n",
      "Result for train_boston_d9133264:\n",
      "  date: 2020-11-26_12-52-35\n",
      "  done: true\n",
      "  experiment_id: 83ff5b2fc30a404c8a7681944758782e\n",
      "  experiment_tag: 111_adam=0.57553,droupout_prob=0.15767,hidden_dim=163.32,lr=0.057522,model=0.70359,n_layer=1.9043,sigmoid_func=0.60033,steps=20,weight_decay=0.04945\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.572244644165039\n",
      "  mean_accuracy: 1.6524727469996403\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26929\n",
      "  time_since_restore: 5.0990400314331055\n",
      "  time_this_iter_s: 0.36339831352233887\n",
      "  time_total_s: 5.0990400314331055\n",
      "  timestamp: 1606391555\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d9133264\n",
      "  \n",
      "Result for train_boston_d8dde582:\n",
      "  date: 2020-11-26_12-52-35\n",
      "  done: true\n",
      "  experiment_id: eaf776ba8b97429bb19e9affa4df17e4\n",
      "  experiment_tag: 109_adam=0.089556,droupout_prob=0.083594,hidden_dim=166.92,lr=0.0061609,model=0.36667,n_layer=2.3954,sigmoid_func=0.39699,steps=20,weight_decay=0.037879\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 42.81068179481908\n",
      "  mean_accuracy: 38.973562140213815\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 26926\n",
      "  time_since_restore: 5.516313076019287\n",
      "  time_this_iter_s: 0.3294343948364258\n",
      "  time_total_s: 5.516313076019287\n",
      "  timestamp: 1606391555\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d8dde582\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27049)\u001b[0m ReLU()\n",
      "Result for train_boston_d92939c4:\n",
      "  date: 2020-11-26_12-52-39\n",
      "  done: true\n",
      "  experiment_id: c9c23fa61ea84d0dada4734fad2165ef\n",
      "  experiment_tag: 112_adam=0.19454,droupout_prob=0.13331,hidden_dim=155.24,lr=0.028047,model=0.75008,n_layer=1.4688,sigmoid_func=0.32277,steps=20,weight_decay=0.041729\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 19.207472951788652\n",
      "  mean_accuracy: 26.20701036955181\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27049\n",
      "  time_since_restore: 5.354326009750366\n",
      "  time_this_iter_s: 5.354326009750366\n",
      "  time_total_s: 5.354326009750366\n",
      "  timestamp: 1606391559\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d92939c4\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27070)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27080)\u001b[0m Sigmoid()\n",
      "\u001b[2m\u001b[36m(pid=27069)\u001b[0m Tanh()\n",
      "Result for train_boston_d9c93cda:\n",
      "  date: 2020-11-26_12-52-40\n",
      "  done: false\n",
      "  experiment_id: 7f2123d76c3b41fabc97c59590ee07e2\n",
      "  experiment_tag: 113_adam=0.42203,droupout_prob=0.23327,hidden_dim=156.66,lr=0.052435,model=0.57794,n_layer=2.0013,sigmoid_func=0.52599,steps=20,weight_decay=0.036135\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.9597447043971011\n",
      "  mean_accuracy: 1.3497402793482731\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27070\n",
      "  time_since_restore: 5.732755184173584\n",
      "  time_this_iter_s: 5.732755184173584\n",
      "  time_total_s: 5.732755184173584\n",
      "  timestamp: 1606391560\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: d9c93cda\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27072)\u001b[0m Tanh()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using AsyncHyperBand: num_stopped=103\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.028294892687546577 | Iter 4.000: -1.0473565553364002 | Iter 1.000: -9.493642706620065<br>Resources requested: 8/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 120 (8 RUNNING, 112 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">           acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_d9c93cda</td><td>RUNNING   </td><td>192.168.1.34:27070</td><td style=\"text-align: right;\">0.422027</td><td style=\"text-align: right;\">       0.233265</td><td style=\"text-align: right;\">    156.662 </td><td style=\"text-align: right;\">0.0524353</td><td style=\"text-align: right;\">0.577937</td><td style=\"text-align: right;\">  2.00133</td><td style=\"text-align: right;\">      0.525987</td><td style=\"text-align: right;\">     0.0361351</td><td style=\"text-align: right;\">     1.34974  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.73276</td><td style=\"text-align: right;\">     0.959745 </td></tr>\n",
       "<tr><td>train_boston_da14be9e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.555371</td><td style=\"text-align: right;\">       0.265831</td><td style=\"text-align: right;\">    202.264 </td><td style=\"text-align: right;\">0.0415319</td><td style=\"text-align: right;\">0.737418</td><td style=\"text-align: right;\">  1.94238</td><td style=\"text-align: right;\">      0.558193</td><td style=\"text-align: right;\">     0.0577774</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_da1f242e</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.368043</td><td style=\"text-align: right;\">       0.345821</td><td style=\"text-align: right;\">    164.035 </td><td style=\"text-align: right;\">0.0633466</td><td style=\"text-align: right;\">0.650749</td><td style=\"text-align: right;\">  1.82483</td><td style=\"text-align: right;\">      0.360626</td><td style=\"text-align: right;\">     0.0491105</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_da85479a</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.645727</td><td style=\"text-align: right;\">       0.287729</td><td style=\"text-align: right;\">    191.326 </td><td style=\"text-align: right;\">0.0444176</td><td style=\"text-align: right;\">0.861246</td><td style=\"text-align: right;\">  2.09999</td><td style=\"text-align: right;\">      0.810171</td><td style=\"text-align: right;\">     0.0236152</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_e00b6bfe</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.379436</td><td style=\"text-align: right;\">       0.263814</td><td style=\"text-align: right;\">    186.224 </td><td style=\"text-align: right;\">0.0413872</td><td style=\"text-align: right;\">0.530491</td><td style=\"text-align: right;\">  1.75214</td><td style=\"text-align: right;\">      0.636183</td><td style=\"text-align: right;\">     0.0499047</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_e0273c76</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.65063 </td><td style=\"text-align: right;\">       0.291014</td><td style=\"text-align: right;\">    144.552 </td><td style=\"text-align: right;\">0.0427627</td><td style=\"text-align: right;\">0.646817</td><td style=\"text-align: right;\">  1.94688</td><td style=\"text-align: right;\">      0.534796</td><td style=\"text-align: right;\">     0.0542326</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_e28845be</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.406783</td><td style=\"text-align: right;\">       0.274347</td><td style=\"text-align: right;\">    196.436 </td><td style=\"text-align: right;\">0.0473229</td><td style=\"text-align: right;\">0.629718</td><td style=\"text-align: right;\">  1.71301</td><td style=\"text-align: right;\">      0.504793</td><td style=\"text-align: right;\">     0.0482668</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_e295abe6</td><td>RUNNING   </td><td>                  </td><td style=\"text-align: right;\">0.365921</td><td style=\"text-align: right;\">       0.38996 </td><td style=\"text-align: right;\">    120.259 </td><td style=\"text-align: right;\">0.0834971</td><td style=\"text-align: right;\">0.982896</td><td style=\"text-align: right;\">  2.03265</td><td style=\"text-align: right;\">      0.288967</td><td style=\"text-align: right;\">     0.0632007</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615 </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044    </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302</td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281</td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622        </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877</td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7      </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6      </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189   </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52     </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906   </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 100 more trials not shown (100 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_da14be9e:\n",
      "  date: 2020-11-26_12-52-40\n",
      "  done: true\n",
      "  experiment_id: 49414b5272474661afe66f92096f2333\n",
      "  experiment_tag: 114_adam=0.55537,droupout_prob=0.26583,hidden_dim=202.26,lr=0.041532,model=0.73742,n_layer=1.9424,sigmoid_func=0.55819,steps=20,weight_decay=0.057777\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 18.223758095189144\n",
      "  mean_accuracy: 18.27916837993421\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27069\n",
      "  time_since_restore: 5.054292440414429\n",
      "  time_this_iter_s: 5.054292440414429\n",
      "  time_total_s: 5.054292440414429\n",
      "  timestamp: 1606391560\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: da14be9e\n",
      "  \n",
      "Result for train_boston_da85479a:\n",
      "  date: 2020-11-26_12-52-40\n",
      "  done: true\n",
      "  experiment_id: 286827a04b9c411b8e9bebad7687d10d\n",
      "  experiment_tag: 116_adam=0.64573,droupout_prob=0.28773,hidden_dim=191.33,lr=0.044418,model=0.86125,n_layer=2.1,sigmoid_func=0.81017,steps=20,weight_decay=0.023615\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 33.61246851870888\n",
      "  mean_accuracy: 30.82081363075658\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27080\n",
      "  time_since_restore: 5.521353244781494\n",
      "  time_this_iter_s: 5.521353244781494\n",
      "  time_total_s: 5.521353244781494\n",
      "  timestamp: 1606391560\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: da85479a\n",
      "  \n",
      "Result for train_boston_da1f242e:\n",
      "  date: 2020-11-26_12-52-40\n",
      "  done: true\n",
      "  experiment_id: 265fc32459994cd4b9c4a451aa969dfa\n",
      "  experiment_tag: 115_adam=0.36804,droupout_prob=0.34582,hidden_dim=164.04,lr=0.063347,model=0.65075,n_layer=1.8248,sigmoid_func=0.36063,steps=20,weight_decay=0.04911\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 57.43628572162829\n",
      "  mean_accuracy: 52.96382542660362\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27072\n",
      "  time_since_restore: 5.148049354553223\n",
      "  time_this_iter_s: 5.148049354553223\n",
      "  time_total_s: 5.148049354553223\n",
      "  timestamp: 1606391560\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: da1f242e\n",
      "  \n",
      "Result for train_boston_d9c93cda:\n",
      "  date: 2020-11-26_12-52-41\n",
      "  done: true\n",
      "  experiment_id: 7f2123d76c3b41fabc97c59590ee07e2\n",
      "  experiment_tag: 113_adam=0.42203,droupout_prob=0.23327,hidden_dim=156.66,lr=0.052435,model=0.57794,n_layer=2.0013,sigmoid_func=0.52599,steps=20,weight_decay=0.036135\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.9448497169896175\n",
      "  mean_accuracy: 1.8643100136204769\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27070\n",
      "  time_since_restore: 6.150266885757446\n",
      "  time_this_iter_s: 0.3040494918823242\n",
      "  time_total_s: 6.150266885757446\n",
      "  timestamp: 1606391561\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: d9c93cda\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27160)\u001b[0m Tanh()\n",
      "Result for train_boston_e00b6bfe:\n",
      "  date: 2020-11-26_12-52-49\n",
      "  done: true\n",
      "  experiment_id: 330b2092bc594d3ab8aaf1c1fe49e419\n",
      "  experiment_tag: 117_adam=0.37944,droupout_prob=0.26381,hidden_dim=186.22,lr=0.041387,model=0.53049,n_layer=1.7521,sigmoid_func=0.63618,steps=20,weight_decay=0.049905\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 20.22903121145148\n",
      "  mean_accuracy: 21.523304186369245\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27160\n",
      "  time_since_restore: 4.743705987930298\n",
      "  time_this_iter_s: 4.743705987930298\n",
      "  time_total_s: 4.743705987930298\n",
      "  timestamp: 1606391569\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e00b6bfe\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27192)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27127)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27173)\u001b[0m ReLU()\n",
      "\u001b[2m\u001b[36m(pid=27083)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27081)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27084)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:52:54,773\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 4.98158597946167 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using AsyncHyperBand: num_stopped=108\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.028294892687546577 | Iter 4.000: -1.058819795909681 | Iter 1.000: -9.897818314401727<br>Resources requested: 7/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 124 (7 RUNNING, 117 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_e0273c76</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.65063 </td><td style=\"text-align: right;\">       0.291014</td><td style=\"text-align: right;\">    144.552 </td><td style=\"text-align: right;\">0.0427627</td><td style=\"text-align: right;\">0.646817</td><td style=\"text-align: right;\">  1.94688</td><td style=\"text-align: right;\">      0.534796</td><td style=\"text-align: right;\">     0.0542326</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e28845be</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.406783</td><td style=\"text-align: right;\">       0.274347</td><td style=\"text-align: right;\">    196.436 </td><td style=\"text-align: right;\">0.0473229</td><td style=\"text-align: right;\">0.629718</td><td style=\"text-align: right;\">  1.71301</td><td style=\"text-align: right;\">      0.504793</td><td style=\"text-align: right;\">     0.0482668</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e295abe6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.365921</td><td style=\"text-align: right;\">       0.38996 </td><td style=\"text-align: right;\">    120.259 </td><td style=\"text-align: right;\">0.0834971</td><td style=\"text-align: right;\">0.982896</td><td style=\"text-align: right;\">  2.03265</td><td style=\"text-align: right;\">      0.288967</td><td style=\"text-align: right;\">     0.0632007</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e31aa71a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.43888 </td><td style=\"text-align: right;\">       0.169569</td><td style=\"text-align: right;\">    189.343 </td><td style=\"text-align: right;\">0.0156347</td><td style=\"text-align: right;\">0.864789</td><td style=\"text-align: right;\">  1.92329</td><td style=\"text-align: right;\">      0.344962</td><td style=\"text-align: right;\">     0.0442503</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e333981a</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.520433</td><td style=\"text-align: right;\">       0.269347</td><td style=\"text-align: right;\">    122.589 </td><td style=\"text-align: right;\">0.0107742</td><td style=\"text-align: right;\">0.680391</td><td style=\"text-align: right;\">  2.15362</td><td style=\"text-align: right;\">      0.381279</td><td style=\"text-align: right;\">     0.0607642</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e344bd20</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.565389</td><td style=\"text-align: right;\">       0.266531</td><td style=\"text-align: right;\">    114.282 </td><td style=\"text-align: right;\">0.0463944</td><td style=\"text-align: right;\">0.6675  </td><td style=\"text-align: right;\">  1.69079</td><td style=\"text-align: right;\">      0.452873</td><td style=\"text-align: right;\">     0.0380224</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_e357fbba</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.512893</td><td style=\"text-align: right;\">       0.29894 </td><td style=\"text-align: right;\">    134.835 </td><td style=\"text-align: right;\">0.0561337</td><td style=\"text-align: right;\">0.44575 </td><td style=\"text-align: right;\">  2.01904</td><td style=\"text-align: right;\">      0.700637</td><td style=\"text-align: right;\">     0.0419177</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">       0.260551</td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">       0.20246 </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">       0.261284</td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">       0.220367</td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">       0.262921</td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">       0.119361</td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">       0.223395</td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">       0.427066</td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">       0.260289</td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">       0.181991</td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">       0.287867</td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">       0.306476</td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">       0.304245</td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 104 more trials not shown (104 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_e344bd20:\n",
      "  date: 2020-11-26_12-52-54\n",
      "  done: false\n",
      "  experiment_id: 538fd5ae9fae4ff49dfffd18d51a2524\n",
      "  experiment_tag: 123_adam=0.56539,droupout_prob=0.26653,hidden_dim=114.28,lr=0.046394,model=0.6675,n_layer=1.6908,sigmoid_func=0.45287,steps=20,weight_decay=0.038022\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.8020890888414884\n",
      "  mean_accuracy: 1.7838783264160156\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27081\n",
      "  time_since_restore: 4.332514047622681\n",
      "  time_this_iter_s: 4.332514047622681\n",
      "  time_total_s: 4.332514047622681\n",
      "  timestamp: 1606391574\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e344bd20\n",
      "  \n",
      "Result for train_boston_e0273c76:\n",
      "  date: 2020-11-26_12-52-50\n",
      "  done: true\n",
      "  experiment_id: abe42e2d9e3e45b990b11d2ef1a4f5af\n",
      "  experiment_tag: 118_adam=0.65063,droupout_prob=0.29101,hidden_dim=144.55,lr=0.042763,model=0.64682,n_layer=1.9469,sigmoid_func=0.5348,steps=20,weight_decay=0.054233\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 21.12813206722862\n",
      "  mean_accuracy: 18.128130461040296\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27192\n",
      "  time_since_restore: 4.589594125747681\n",
      "  time_this_iter_s: 4.589594125747681\n",
      "  time_total_s: 4.589594125747681\n",
      "  timestamp: 1606391570\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e0273c76\n",
      "  \n",
      "Result for train_boston_e295abe6:\n",
      "  date: 2020-11-26_12-52-51\n",
      "  done: true\n",
      "  experiment_id: 06915879754445c28ea63596c3fdb48f\n",
      "  experiment_tag: 120_adam=0.36592,droupout_prob=0.38996,hidden_dim=120.26,lr=0.083497,model=0.9829,n_layer=2.0327,sigmoid_func=0.28897,steps=20,weight_decay=0.063201\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 90.02066200657895\n",
      "  mean_accuracy: 87.57207930715461\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27173\n",
      "  time_since_restore: 4.7024853229522705\n",
      "  time_this_iter_s: 4.7024853229522705\n",
      "  time_total_s: 4.7024853229522705\n",
      "  timestamp: 1606391571\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e295abe6\n",
      "  \n",
      "Result for train_boston_e333981a:\n",
      "  date: 2020-11-26_12-52-52\n",
      "  done: true\n",
      "  experiment_id: 3a561e94aee04554a99f4acdeb59d1d0\n",
      "  experiment_tag: 122_adam=0.52043,droupout_prob=0.26935,hidden_dim=122.59,lr=0.010774,model=0.68039,n_layer=2.1536,sigmoid_func=0.38128,steps=20,weight_decay=0.060764\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 17.66994114925987\n",
      "  mean_accuracy: 17.99599095394737\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27083\n",
      "  time_since_restore: 2.4584665298461914\n",
      "  time_this_iter_s: 2.4584665298461914\n",
      "  time_total_s: 2.4584665298461914\n",
      "  timestamp: 1606391572\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e333981a\n",
      "  \n",
      "Result for train_boston_e28845be:\n",
      "  date: 2020-11-26_12-52-50\n",
      "  done: false\n",
      "  experiment_id: 1e4050b890394e7e8162d32825345037\n",
      "  experiment_tag: 119_adam=0.40678,droupout_prob=0.27435,hidden_dim=196.44,lr=0.047323,model=0.62972,n_layer=1.713,sigmoid_func=0.50479,steps=20,weight_decay=0.048267\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.791012211849815\n",
      "  mean_accuracy: 1.986370889764083\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27127\n",
      "  time_since_restore: 4.321712970733643\n",
      "  time_this_iter_s: 4.321712970733643\n",
      "  time_total_s: 4.321712970733643\n",
      "  timestamp: 1606391570\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e28845be\n",
      "  \n",
      "Result for train_boston_e31aa71a:\n",
      "  date: 2020-11-26_12-52-54\n",
      "  done: true\n",
      "  experiment_id: ef9ba33e5cf04e6f9873daa8eefbe9d3\n",
      "  experiment_tag: 121_adam=0.43888,droupout_prob=0.16957,hidden_dim=189.34,lr=0.015635,model=0.86479,n_layer=1.9233,sigmoid_func=0.34496,steps=20,weight_decay=0.04425\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 121.60066303453948\n",
      "  mean_accuracy: 110.82939710115132\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27084\n",
      "  time_since_restore: 4.749218463897705\n",
      "  time_this_iter_s: 4.749218463897705\n",
      "  time_total_s: 4.749218463897705\n",
      "  timestamp: 1606391574\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e31aa71a\n",
      "  \n",
      "Result for train_boston_e344bd20:\n",
      "  date: 2020-11-26_12-52-55\n",
      "  done: true\n",
      "  experiment_id: 538fd5ae9fae4ff49dfffd18d51a2524\n",
      "  experiment_tag: 123_adam=0.56539,droupout_prob=0.26653,hidden_dim=114.28,lr=0.046394,model=0.6675,n_layer=1.6908,sigmoid_func=0.45287,steps=20,weight_decay=0.038022\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 4\n",
      "  loss: 3.6325177644428455\n",
      "  mean_accuracy: 2.9914906150416325\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27081\n",
      "  time_since_restore: 5.419278144836426\n",
      "  time_this_iter_s: 0.545555591583252\n",
      "  time_total_s: 5.419278144836426\n",
      "  timestamp: 1606391575\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: e344bd20\n",
      "  \n",
      "Result for train_boston_e28845be:\n",
      "  date: 2020-11-26_12-52-56\n",
      "  done: true\n",
      "  experiment_id: 1e4050b890394e7e8162d32825345037\n",
      "  experiment_tag: 119_adam=0.40678,droupout_prob=0.27435,hidden_dim=196.44,lr=0.047323,model=0.62972,n_layer=1.713,sigmoid_func=0.50479,steps=20,weight_decay=0.048267\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.02945349090977719\n",
      "  mean_accuracy: 0.03568319270485326\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27127\n",
      "  time_since_restore: 10.236830949783325\n",
      "  time_this_iter_s: 0.07413411140441895\n",
      "  time_total_s: 10.236830949783325\n",
      "  timestamp: 1606391576\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: e28845be\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27425)\u001b[0m Sigmoid()\n",
      "Result for train_boston_e357fbba:\n",
      "  date: 2020-11-26_12-52-57\n",
      "  done: true\n",
      "  experiment_id: c2260e76f5a9420f815abdadbd510995\n",
      "  experiment_tag: 124_adam=0.51289,droupout_prob=0.29894,hidden_dim=134.83,lr=0.056134,model=0.44575,n_layer=2.019,sigmoid_func=0.70064,steps=20,weight_decay=0.041918\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 664845.3684210526\n",
      "  mean_accuracy: 612281.4736842106\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27425\n",
      "  time_since_restore: 2.1305367946624756\n",
      "  time_this_iter_s: 2.1305367946624756\n",
      "  time_total_s: 2.1305367946624756\n",
      "  timestamp: 1606391577\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e357fbba\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27451)\u001b[0m Tanh()\n",
      "Result for train_boston_ebab14aa:\n",
      "  date: 2020-11-26_12-53-02\n",
      "  done: true\n",
      "  experiment_id: 79654a29758c488f97a7b16f4abdc968\n",
      "  experiment_tag: 128_adam=0.17971,droupout_prob=0.19591,hidden_dim=157.81,lr=0.040317,model=0.6871,n_layer=1.8155,sigmoid_func=0.65771,steps=20,weight_decay=0.042308\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 19.009812204461348\n",
      "  mean_accuracy: 16.447267231188324\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27451\n",
      "  time_since_restore: 3.47928786277771\n",
      "  time_this_iter_s: 3.47928786277771\n",
      "  time_total_s: 3.47928786277771\n",
      "  timestamp: 1606391582\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ebab14aa\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=27469)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27465)\u001b[0m Tanh()\n",
      "\u001b[2m\u001b[36m(pid=27457)\u001b[0m Tanh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-26 12:53:06,672\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 4.250474452972412 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using AsyncHyperBand: num_stopped=116\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.028814742439671567 | Iter 4.000: -1.025953167363217 | Iter 1.000: -9.897818314401727<br>Resources requested: 3/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 128 (3 RUNNING, 125 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_eb5fd9c2</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.219405</td><td style=\"text-align: right;\">      0.0754853</td><td style=\"text-align: right;\">    248.861 </td><td style=\"text-align: right;\">0.0718356</td><td style=\"text-align: right;\">0.832539</td><td style=\"text-align: right;\">  2.18882</td><td style=\"text-align: right;\">      0.631572</td><td style=\"text-align: right;\">     0.0420215</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_eb75d81c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.490105</td><td style=\"text-align: right;\">      0.29266  </td><td style=\"text-align: right;\">    162.987 </td><td style=\"text-align: right;\">0.0567315</td><td style=\"text-align: right;\">0.770855</td><td style=\"text-align: right;\">  1.94192</td><td style=\"text-align: right;\">      0.584038</td><td style=\"text-align: right;\">     0.0552273</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_eb8a5760</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">0.345771</td><td style=\"text-align: right;\">      0.207913 </td><td style=\"text-align: right;\">    170.16  </td><td style=\"text-align: right;\">0.0592772</td><td style=\"text-align: right;\">0.742629</td><td style=\"text-align: right;\">  1.63732</td><td style=\"text-align: right;\">      0.526789</td><td style=\"text-align: right;\">     0.0451881</td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419</td><td style=\"text-align: right;\">      0.260551 </td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926</td><td style=\"text-align: right;\">0.6167  </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">      0.724931</td><td style=\"text-align: right;\">     0.0546216</td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123</td><td style=\"text-align: right;\">      0.20246  </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208</td><td style=\"text-align: right;\">0.486511</td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">      0.605291</td><td style=\"text-align: right;\">     0.0513878</td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193</td><td style=\"text-align: right;\">      0.261284 </td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436</td><td style=\"text-align: right;\">0.865439</td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">      0.213596</td><td style=\"text-align: right;\">     0.0349197</td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114</td><td style=\"text-align: right;\">      0.220367 </td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805</td><td style=\"text-align: right;\">0.53437 </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">      0.456714</td><td style=\"text-align: right;\">     0.0449583</td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835</td><td style=\"text-align: right;\">      0.262921 </td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565</td><td style=\"text-align: right;\">0.386208</td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">      0.598601</td><td style=\"text-align: right;\">     0.0497963</td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813</td><td style=\"text-align: right;\">      0.119361 </td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233</td><td style=\"text-align: right;\">0.518297</td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">      0.388659</td><td style=\"text-align: right;\">     0.0758267</td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524</td><td style=\"text-align: right;\">      0.223395 </td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387</td><td style=\"text-align: right;\">0.401587</td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">      0.480465</td><td style=\"text-align: right;\">     0.0476026</td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177</td><td style=\"text-align: right;\">      0.427066 </td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821</td><td style=\"text-align: right;\">0.474198</td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">      0.551065</td><td style=\"text-align: right;\">     0.0943057</td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349</td><td style=\"text-align: right;\">      0.260289 </td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424</td><td style=\"text-align: right;\">0.591214</td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">      0.45047 </td><td style=\"text-align: right;\">     0.0571592</td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636</td><td style=\"text-align: right;\">      0.181991 </td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225</td><td style=\"text-align: right;\">0.295292</td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">      0.706545</td><td style=\"text-align: right;\">     0.0520324</td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124</td><td style=\"text-align: right;\">      0.287867 </td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847</td><td style=\"text-align: right;\">0.662898</td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">      0.845624</td><td style=\"text-align: right;\">     0.0270363</td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248</td><td style=\"text-align: right;\">      0.306476 </td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575</td><td style=\"text-align: right;\">0.597705</td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">      0.265565</td><td style=\"text-align: right;\">     0.0229244</td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224</td><td style=\"text-align: right;\">      0.304245 </td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386 </td><td style=\"text-align: right;\">0.440189</td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">      0.496419</td><td style=\"text-align: right;\">     0.0462234</td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "<tr><td>train_boston_7d80f670</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.475309</td><td style=\"text-align: right;\">      0.306478 </td><td style=\"text-align: right;\">    152.23  </td><td style=\"text-align: right;\">0.0375412</td><td style=\"text-align: right;\">0.73664 </td><td style=\"text-align: right;\">  1.73359</td><td style=\"text-align: right;\">      0.77795 </td><td style=\"text-align: right;\">     0.0482804</td><td style=\"text-align: right;\">     0.766571   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.339  </td><td style=\"text-align: right;\">     1.09321    </td></tr>\n",
       "<tr><td>train_boston_7da39392</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.369321</td><td style=\"text-align: right;\">      0.316607 </td><td style=\"text-align: right;\">    247.265 </td><td style=\"text-align: right;\">0.0839222</td><td style=\"text-align: right;\">0.224336</td><td style=\"text-align: right;\">  1.52021</td><td style=\"text-align: right;\">      0.929015</td><td style=\"text-align: right;\">     0.0712747</td><td style=\"text-align: right;\">     1.27116e+08</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.6518 </td><td style=\"text-align: right;\">     1.30926e+08</td></tr>\n",
       "<tr><td>train_boston_7eceaf9a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.575254</td><td style=\"text-align: right;\">      0.286455 </td><td style=\"text-align: right;\">    136.97  </td><td style=\"text-align: right;\">0.0563368</td><td style=\"text-align: right;\">0.542539</td><td style=\"text-align: right;\">  2.26409</td><td style=\"text-align: right;\">      0.511323</td><td style=\"text-align: right;\">     0.0503148</td><td style=\"text-align: right;\">    24.4111     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.8423 </td><td style=\"text-align: right;\">    27.726      </td></tr>\n",
       "<tr><td>train_boston_8025da8a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.305203</td><td style=\"text-align: right;\">      0.343845 </td><td style=\"text-align: right;\">    179.556 </td><td style=\"text-align: right;\">0.0699987</td><td style=\"text-align: right;\">0.30811 </td><td style=\"text-align: right;\">  1.91827</td><td style=\"text-align: right;\">      0.228793</td><td style=\"text-align: right;\">     0.0543419</td><td style=\"text-align: right;\">     3.57271e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.44681</td><td style=\"text-align: right;\">     3.75465e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 108 more trials not shown (108 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_boston_eb75d81c:\n",
      "  date: 2020-11-26_12-53-02\n",
      "  done: true\n",
      "  experiment_id: d3cde5d4c45e4982bc59dca48d6607fc\n",
      "  experiment_tag: 126_adam=0.4901,droupout_prob=0.29266,hidden_dim=162.99,lr=0.056732,model=0.77085,n_layer=1.9419,sigmoid_func=0.58404,steps=20,weight_decay=0.055227\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 45.53129818564967\n",
      "  mean_accuracy: 44.14233077199835\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27469\n",
      "  time_since_restore: 2.8357114791870117\n",
      "  time_this_iter_s: 2.8357114791870117\n",
      "  time_total_s: 2.8357114791870117\n",
      "  timestamp: 1606391582\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eb75d81c\n",
      "  \n",
      "Result for train_boston_eb8a5760:\n",
      "  date: 2020-11-26_12-53-03\n",
      "  done: true\n",
      "  experiment_id: c58d1e0dc81545bb86c058d118ff0307\n",
      "  experiment_tag: 127_adam=0.34577,droupout_prob=0.20791,hidden_dim=170.16,lr=0.059277,model=0.74263,n_layer=1.6373,sigmoid_func=0.52679,steps=20,weight_decay=0.045188\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 51.22626374897204\n",
      "  mean_accuracy: 46.524324115953945\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27457\n",
      "  time_since_restore: 1.9101998805999756\n",
      "  time_this_iter_s: 1.9101998805999756\n",
      "  time_total_s: 1.9101998805999756\n",
      "  timestamp: 1606391583\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eb8a5760\n",
      "  \n",
      "Result for train_boston_eb5fd9c2:\n",
      "  date: 2020-11-26_12-53-03\n",
      "  done: true\n",
      "  experiment_id: 8dcd4c638f734a16b7b1a76fc8795cae\n",
      "  experiment_tag: 125_adam=0.2194,droupout_prob=0.075485,hidden_dim=248.86,lr=0.071836,model=0.83254,n_layer=2.1888,sigmoid_func=0.63157,steps=20,weight_decay=0.042021\n",
      "  hostname: antoine-HP-Spectre-Laptop-13-af0xx\n",
      "  iterations_since_restore: 1\n",
      "  loss: 54.1754792865954\n",
      "  mean_accuracy: 51.78539396587171\n",
      "  node_ip: 192.168.1.34\n",
      "  pid: 27465\n",
      "  time_since_restore: 1.7267343997955322\n",
      "  time_this_iter_s: 1.7267343997955322\n",
      "  time_total_s: 1.7267343997955322\n",
      "  timestamp: 1606391583\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eb5fd9c2\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using AsyncHyperBand: num_stopped=119\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.028814742439671567 | Iter 4.000: -1.025953167363217 | Iter 1.000: -10.0614013671875<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/antoine/ray_results/PSO<br>Number of trials: 128 (128 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">     adam</th><th style=\"text-align: right;\">  droupout_prob</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">    model</th><th style=\"text-align: right;\">  n_layer</th><th style=\"text-align: right;\">  sigmoid_func</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">             acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">            loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_boston_7540e272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.188419 </td><td style=\"text-align: right;\">      0.260551 </td><td style=\"text-align: right;\">    166.073 </td><td style=\"text-align: right;\">0.0547926 </td><td style=\"text-align: right;\">0.6167   </td><td style=\"text-align: right;\">  2.16253</td><td style=\"text-align: right;\">     0.724931 </td><td style=\"text-align: right;\">   0.0546216  </td><td style=\"text-align: right;\">     0.469486   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.31415</td><td style=\"text-align: right;\">     0.562615   </td></tr>\n",
       "<tr><td>train_boston_7548ef44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454123 </td><td style=\"text-align: right;\">      0.20246  </td><td style=\"text-align: right;\">    128.497 </td><td style=\"text-align: right;\">0.0381208 </td><td style=\"text-align: right;\">0.486511 </td><td style=\"text-align: right;\">  2.37246</td><td style=\"text-align: right;\">     0.605291 </td><td style=\"text-align: right;\">   0.0513878  </td><td style=\"text-align: right;\">   254.449      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.19202</td><td style=\"text-align: right;\">   272.044      </td></tr>\n",
       "<tr><td>train_boston_754e6294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.256193 </td><td style=\"text-align: right;\">      0.261284 </td><td style=\"text-align: right;\">    147.647 </td><td style=\"text-align: right;\">0.0296436 </td><td style=\"text-align: right;\">0.865439 </td><td style=\"text-align: right;\">  2.01516</td><td style=\"text-align: right;\">     0.213596 </td><td style=\"text-align: right;\">   0.0349197  </td><td style=\"text-align: right;\">     0.0637586  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.23226</td><td style=\"text-align: right;\">     0.0815302  </td></tr>\n",
       "<tr><td>train_boston_75511af2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.343114 </td><td style=\"text-align: right;\">      0.220367 </td><td style=\"text-align: right;\">     75.4364</td><td style=\"text-align: right;\">0.0332805 </td><td style=\"text-align: right;\">0.53437  </td><td style=\"text-align: right;\">  2.33137</td><td style=\"text-align: right;\">     0.456714 </td><td style=\"text-align: right;\">   0.0449583  </td><td style=\"text-align: right;\">     0.0342529  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.33447</td><td style=\"text-align: right;\">     0.0403281  </td></tr>\n",
       "<tr><td>train_boston_75549844</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565835 </td><td style=\"text-align: right;\">      0.262921 </td><td style=\"text-align: right;\">    158.792 </td><td style=\"text-align: right;\">0.0539565 </td><td style=\"text-align: right;\">0.386208 </td><td style=\"text-align: right;\">  2.01022</td><td style=\"text-align: right;\">     0.598601 </td><td style=\"text-align: right;\">   0.0497963  </td><td style=\"text-align: right;\">284563          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.14587</td><td style=\"text-align: right;\">304622          </td></tr>\n",
       "<tr><td>train_boston_7558c3a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.420813 </td><td style=\"text-align: right;\">      0.119361 </td><td style=\"text-align: right;\">    134.877 </td><td style=\"text-align: right;\">0.0261233 </td><td style=\"text-align: right;\">0.518297 </td><td style=\"text-align: right;\">  1.98774</td><td style=\"text-align: right;\">     0.388659 </td><td style=\"text-align: right;\">   0.0758267  </td><td style=\"text-align: right;\">     0.0160625  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.54167</td><td style=\"text-align: right;\">     0.0164877  </td></tr>\n",
       "<tr><td>train_boston_755c2078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.551524 </td><td style=\"text-align: right;\">      0.223395 </td><td style=\"text-align: right;\">    111.733 </td><td style=\"text-align: right;\">0.0701387 </td><td style=\"text-align: right;\">0.401587 </td><td style=\"text-align: right;\">  1.7094 </td><td style=\"text-align: right;\">     0.480465 </td><td style=\"text-align: right;\">   0.0476026  </td><td style=\"text-align: right;\"> 83715          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63938</td><td style=\"text-align: right;\"> 89648.7        </td></tr>\n",
       "<tr><td>train_boston_755e3c1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.518177 </td><td style=\"text-align: right;\">      0.427066 </td><td style=\"text-align: right;\">    115.087 </td><td style=\"text-align: right;\">0.0526821 </td><td style=\"text-align: right;\">0.474198 </td><td style=\"text-align: right;\">  1.74848</td><td style=\"text-align: right;\">     0.551065 </td><td style=\"text-align: right;\">   0.0943057  </td><td style=\"text-align: right;\"> 20169.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06989</td><td style=\"text-align: right;\"> 20156.6        </td></tr>\n",
       "<tr><td>train_boston_7ac548a0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.556349 </td><td style=\"text-align: right;\">      0.260289 </td><td style=\"text-align: right;\">     98.7471</td><td style=\"text-align: right;\">0.0584424 </td><td style=\"text-align: right;\">0.591214 </td><td style=\"text-align: right;\">  2.00223</td><td style=\"text-align: right;\">     0.45047  </td><td style=\"text-align: right;\">   0.0571592  </td><td style=\"text-align: right;\">    28.0844     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.37029</td><td style=\"text-align: right;\">    32.1189     </td></tr>\n",
       "<tr><td>train_boston_7bbee518</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585636 </td><td style=\"text-align: right;\">      0.181991 </td><td style=\"text-align: right;\">    165.975 </td><td style=\"text-align: right;\">0.0352225 </td><td style=\"text-align: right;\">0.295292 </td><td style=\"text-align: right;\">  1.9169 </td><td style=\"text-align: right;\">     0.706545 </td><td style=\"text-align: right;\">   0.0520324  </td><td style=\"text-align: right;\">  1232.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36512</td><td style=\"text-align: right;\">  1411.52       </td></tr>\n",
       "<tr><td>train_boston_7c03b06c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.470124 </td><td style=\"text-align: right;\">      0.287867 </td><td style=\"text-align: right;\">    159.496 </td><td style=\"text-align: right;\">0.0948847 </td><td style=\"text-align: right;\">0.662898 </td><td style=\"text-align: right;\">  1.6981 </td><td style=\"text-align: right;\">     0.845624 </td><td style=\"text-align: right;\">   0.0270363  </td><td style=\"text-align: right;\">    27.9596     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.13957</td><td style=\"text-align: right;\">    29.4906     </td></tr>\n",
       "<tr><td>train_boston_7c69811c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.769248 </td><td style=\"text-align: right;\">      0.306476 </td><td style=\"text-align: right;\">    145.372 </td><td style=\"text-align: right;\">0.0531575 </td><td style=\"text-align: right;\">0.597705 </td><td style=\"text-align: right;\">  1.66393</td><td style=\"text-align: right;\">     0.265565 </td><td style=\"text-align: right;\">   0.0229244  </td><td style=\"text-align: right;\">     0.0237987  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.11145</td><td style=\"text-align: right;\">     0.0177892  </td></tr>\n",
       "<tr><td>train_boston_7d55830a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.535224 </td><td style=\"text-align: right;\">      0.304245 </td><td style=\"text-align: right;\">    204.113 </td><td style=\"text-align: right;\">0.059386  </td><td style=\"text-align: right;\">0.440189 </td><td style=\"text-align: right;\">  2.14833</td><td style=\"text-align: right;\">     0.496419 </td><td style=\"text-align: right;\">   0.0462234  </td><td style=\"text-align: right;\">     3.81909e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32851</td><td style=\"text-align: right;\">     4.13092e+06</td></tr>\n",
       "<tr><td>train_boston_7d80f670</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.475309 </td><td style=\"text-align: right;\">      0.306478 </td><td style=\"text-align: right;\">    152.23  </td><td style=\"text-align: right;\">0.0375412 </td><td style=\"text-align: right;\">0.73664  </td><td style=\"text-align: right;\">  1.73359</td><td style=\"text-align: right;\">     0.77795  </td><td style=\"text-align: right;\">   0.0482804  </td><td style=\"text-align: right;\">     0.766571   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.339  </td><td style=\"text-align: right;\">     1.09321    </td></tr>\n",
       "<tr><td>train_boston_7da39392</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.369321 </td><td style=\"text-align: right;\">      0.316607 </td><td style=\"text-align: right;\">    247.265 </td><td style=\"text-align: right;\">0.0839222 </td><td style=\"text-align: right;\">0.224336 </td><td style=\"text-align: right;\">  1.52021</td><td style=\"text-align: right;\">     0.929015 </td><td style=\"text-align: right;\">   0.0712747  </td><td style=\"text-align: right;\">     1.27116e+08</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.6518 </td><td style=\"text-align: right;\">     1.30926e+08</td></tr>\n",
       "<tr><td>train_boston_7eceaf9a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.575254 </td><td style=\"text-align: right;\">      0.286455 </td><td style=\"text-align: right;\">    136.97  </td><td style=\"text-align: right;\">0.0563368 </td><td style=\"text-align: right;\">0.542539 </td><td style=\"text-align: right;\">  2.26409</td><td style=\"text-align: right;\">     0.511323 </td><td style=\"text-align: right;\">   0.0503148  </td><td style=\"text-align: right;\">    24.4111     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.8423 </td><td style=\"text-align: right;\">    27.726      </td></tr>\n",
       "<tr><td>train_boston_8025da8a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.305203 </td><td style=\"text-align: right;\">      0.343845 </td><td style=\"text-align: right;\">    179.556 </td><td style=\"text-align: right;\">0.0699987 </td><td style=\"text-align: right;\">0.30811  </td><td style=\"text-align: right;\">  1.91827</td><td style=\"text-align: right;\">     0.228793 </td><td style=\"text-align: right;\">   0.0543419  </td><td style=\"text-align: right;\">     3.57271e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.44681</td><td style=\"text-align: right;\">     3.75465e+06</td></tr>\n",
       "<tr><td>train_boston_81ed3c32</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.772075 </td><td style=\"text-align: right;\">      0.380641 </td><td style=\"text-align: right;\">    124.213 </td><td style=\"text-align: right;\">0.0506693 </td><td style=\"text-align: right;\">0.186222 </td><td style=\"text-align: right;\">  1.53622</td><td style=\"text-align: right;\">     0.973479 </td><td style=\"text-align: right;\">   0.0520169  </td><td style=\"text-align: right;\"> 73822.6        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.53983</td><td style=\"text-align: right;\"> 81902.1        </td></tr>\n",
       "<tr><td>train_boston_82083082</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0505669</td><td style=\"text-align: right;\">      0.453085 </td><td style=\"text-align: right;\">    166.969 </td><td style=\"text-align: right;\">0.0517356 </td><td style=\"text-align: right;\">0.477148 </td><td style=\"text-align: right;\">  2.27609</td><td style=\"text-align: right;\">     0.158823 </td><td style=\"text-align: right;\">   0.0566153  </td><td style=\"text-align: right;\">118713          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.30361</td><td style=\"text-align: right;\">121181          </td></tr>\n",
       "<tr><td>train_boston_837b5dc2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.390983 </td><td style=\"text-align: right;\">      0.354028 </td><td style=\"text-align: right;\">    159.117 </td><td style=\"text-align: right;\">0.0208905 </td><td style=\"text-align: right;\">0.645194 </td><td style=\"text-align: right;\">  2.05507</td><td style=\"text-align: right;\">     0.111876 </td><td style=\"text-align: right;\">   0.0704925  </td><td style=\"text-align: right;\">    40.4591     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.30809</td><td style=\"text-align: right;\">    41.0704     </td></tr>\n",
       "<tr><td>train_boston_83a3fb2e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.543966 </td><td style=\"text-align: right;\">      0.24523  </td><td style=\"text-align: right;\">    196.352 </td><td style=\"text-align: right;\">0.0581661 </td><td style=\"text-align: right;\">0.661737 </td><td style=\"text-align: right;\">  1.97642</td><td style=\"text-align: right;\">     0.34117  </td><td style=\"text-align: right;\">   0.0426898  </td><td style=\"text-align: right;\">     4.13491    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.59674</td><td style=\"text-align: right;\">     3.99615    </td></tr>\n",
       "<tr><td>train_boston_840ac264</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.514273 </td><td style=\"text-align: right;\">      0.256305 </td><td style=\"text-align: right;\">    170.317 </td><td style=\"text-align: right;\">0.081395  </td><td style=\"text-align: right;\">0.783381 </td><td style=\"text-align: right;\">  1.94618</td><td style=\"text-align: right;\">     0.272791 </td><td style=\"text-align: right;\">   0.0440964  </td><td style=\"text-align: right;\">    42.1487     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.01601</td><td style=\"text-align: right;\">    47.3369     </td></tr>\n",
       "<tr><td>train_boston_85a94a1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.580567 </td><td style=\"text-align: right;\">      0.173613 </td><td style=\"text-align: right;\">     72.5299</td><td style=\"text-align: right;\">0.0461383 </td><td style=\"text-align: right;\">0.358077 </td><td style=\"text-align: right;\">  1.91745</td><td style=\"text-align: right;\">     0.569655 </td><td style=\"text-align: right;\">   0.0698312  </td><td style=\"text-align: right;\">    33.6999     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.20513</td><td style=\"text-align: right;\">    40.4179     </td></tr>\n",
       "<tr><td>train_boston_8666208a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.450489 </td><td style=\"text-align: right;\">      0.309289 </td><td style=\"text-align: right;\">    146.657 </td><td style=\"text-align: right;\">0.043224  </td><td style=\"text-align: right;\">0.37235  </td><td style=\"text-align: right;\">  2.33081</td><td style=\"text-align: right;\">     0.658507 </td><td style=\"text-align: right;\">   0.0494516  </td><td style=\"text-align: right;\">  9157.47       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.39169</td><td style=\"text-align: right;\"> 10173.4        </td></tr>\n",
       "<tr><td>train_boston_8680a8f6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.588127 </td><td style=\"text-align: right;\">      0.287602 </td><td style=\"text-align: right;\">    158.416 </td><td style=\"text-align: right;\">0.0803231 </td><td style=\"text-align: right;\">0.468602 </td><td style=\"text-align: right;\">  1.86437</td><td style=\"text-align: right;\">     0.447076 </td><td style=\"text-align: right;\">   0.0201235  </td><td style=\"text-align: right;\">     1.50045e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.30669</td><td style=\"text-align: right;\">     1.575e+06  </td></tr>\n",
       "<tr><td>train_boston_86a573c0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.560147 </td><td style=\"text-align: right;\">      0.202155 </td><td style=\"text-align: right;\">    196.665 </td><td style=\"text-align: right;\">0.0387627 </td><td style=\"text-align: right;\">0.389404 </td><td style=\"text-align: right;\">  1.81587</td><td style=\"text-align: right;\">     0.454186 </td><td style=\"text-align: right;\">   0.039257   </td><td style=\"text-align: right;\">  8677.7        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.14835</td><td style=\"text-align: right;\">  8281.86       </td></tr>\n",
       "<tr><td>train_boston_86b55cfe</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.232166 </td><td style=\"text-align: right;\">      0.183912 </td><td style=\"text-align: right;\">    178.755 </td><td style=\"text-align: right;\">0.0347913 </td><td style=\"text-align: right;\">0.448565 </td><td style=\"text-align: right;\">  2.08806</td><td style=\"text-align: right;\">     0.636718 </td><td style=\"text-align: right;\">   0.0440773  </td><td style=\"text-align: right;\">  3856.34       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.56535</td><td style=\"text-align: right;\">  4420.64       </td></tr>\n",
       "<tr><td>train_boston_8819ce54</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.4756   </td><td style=\"text-align: right;\">      0.191849 </td><td style=\"text-align: right;\">    161.988 </td><td style=\"text-align: right;\">0.0416444 </td><td style=\"text-align: right;\">0.439134 </td><td style=\"text-align: right;\">  2.06967</td><td style=\"text-align: right;\">     0.760968 </td><td style=\"text-align: right;\">   0.0327703  </td><td style=\"text-align: right;\">  1707.69       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.48401</td><td style=\"text-align: right;\">  1967.04       </td></tr>\n",
       "<tr><td>train_boston_8878c8c8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.196616 </td><td style=\"text-align: right;\">      0.488324 </td><td style=\"text-align: right;\">    137.797 </td><td style=\"text-align: right;\">0.0381632 </td><td style=\"text-align: right;\">0.638953 </td><td style=\"text-align: right;\">  1.85837</td><td style=\"text-align: right;\">     0.420841 </td><td style=\"text-align: right;\">   0.0276748  </td><td style=\"text-align: right;\">    24.7012     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.36004</td><td style=\"text-align: right;\">    25.817      </td></tr>\n",
       "<tr><td>train_boston_88ba232c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.531391 </td><td style=\"text-align: right;\">      0.324633 </td><td style=\"text-align: right;\">    149.53  </td><td style=\"text-align: right;\">0.0458652 </td><td style=\"text-align: right;\">0.610559 </td><td style=\"text-align: right;\">  1.59093</td><td style=\"text-align: right;\">     0.369628 </td><td style=\"text-align: right;\">   0.00882965 </td><td style=\"text-align: right;\">     1.25632    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.03253</td><td style=\"text-align: right;\">     1.0967     </td></tr>\n",
       "<tr><td>train_boston_8ace59b2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.496993 </td><td style=\"text-align: right;\">      0.283734 </td><td style=\"text-align: right;\">    167.81  </td><td style=\"text-align: right;\">0.0453557 </td><td style=\"text-align: right;\">0.480904 </td><td style=\"text-align: right;\">  2.19746</td><td style=\"text-align: right;\">     0.508595 </td><td style=\"text-align: right;\">   0.048109   </td><td style=\"text-align: right;\">243534          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.2545 </td><td style=\"text-align: right;\">247563          </td></tr>\n",
       "<tr><td>train_boston_8b264fa0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.273215 </td><td style=\"text-align: right;\">      0.32134  </td><td style=\"text-align: right;\">    185.265 </td><td style=\"text-align: right;\">0.07584   </td><td style=\"text-align: right;\">0.315134 </td><td style=\"text-align: right;\">  2.17474</td><td style=\"text-align: right;\">     0.599772 </td><td style=\"text-align: right;\">   0.0569899  </td><td style=\"text-align: right;\">     3.24649e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.79707</td><td style=\"text-align: right;\">     3.5349e+06 </td></tr>\n",
       "<tr><td>train_boston_8e1d0a50</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.626487 </td><td style=\"text-align: right;\">      0.22813  </td><td style=\"text-align: right;\">    142.876 </td><td style=\"text-align: right;\">0.0526973 </td><td style=\"text-align: right;\">0.655796 </td><td style=\"text-align: right;\">  1.57547</td><td style=\"text-align: right;\">     0.589421 </td><td style=\"text-align: right;\">   0.0459849  </td><td style=\"text-align: right;\">    35.3605     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.71148</td><td style=\"text-align: right;\">    38.6361     </td></tr>\n",
       "<tr><td>train_boston_8e34675e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.508515 </td><td style=\"text-align: right;\">      0.161228 </td><td style=\"text-align: right;\">    149.042 </td><td style=\"text-align: right;\">0.0607732 </td><td style=\"text-align: right;\">0.445264 </td><td style=\"text-align: right;\">  1.95511</td><td style=\"text-align: right;\">     0.591311 </td><td style=\"text-align: right;\">   0.064355   </td><td style=\"text-align: right;\">306834          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.63648</td><td style=\"text-align: right;\">335771          </td></tr>\n",
       "<tr><td>train_boston_8e4d8248</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.553402 </td><td style=\"text-align: right;\">      0.272397 </td><td style=\"text-align: right;\">    113.694 </td><td style=\"text-align: right;\">0.0590917 </td><td style=\"text-align: right;\">0.549114 </td><td style=\"text-align: right;\">  2.68141</td><td style=\"text-align: right;\">     0.590711 </td><td style=\"text-align: right;\">   0.0340919  </td><td style=\"text-align: right;\">     3.71557    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.47844</td><td style=\"text-align: right;\">     2.31166    </td></tr>\n",
       "<tr><td>train_boston_8e6474d0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.186636 </td><td style=\"text-align: right;\">      0.264184 </td><td style=\"text-align: right;\">    170.633 </td><td style=\"text-align: right;\">0.0382452 </td><td style=\"text-align: right;\">0.56633  </td><td style=\"text-align: right;\">  1.61423</td><td style=\"text-align: right;\">     0.626378 </td><td style=\"text-align: right;\">   0.0564889  </td><td style=\"text-align: right;\">    48.5583     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.75353</td><td style=\"text-align: right;\">    55.6977     </td></tr>\n",
       "<tr><td>train_boston_8ef451ea</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.129275 </td><td style=\"text-align: right;\">      0.468378 </td><td style=\"text-align: right;\">     66.0149</td><td style=\"text-align: right;\">0.0548892 </td><td style=\"text-align: right;\">0.022351 </td><td style=\"text-align: right;\">  2.42869</td><td style=\"text-align: right;\">     0.490687 </td><td style=\"text-align: right;\">   0.0216447  </td><td style=\"text-align: right;\">  8254.07       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.3882 </td><td style=\"text-align: right;\">  8560.57       </td></tr>\n",
       "<tr><td>train_boston_8f4cff3e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.368331 </td><td style=\"text-align: right;\">      0.259028 </td><td style=\"text-align: right;\">     76.1335</td><td style=\"text-align: right;\">0.0318749 </td><td style=\"text-align: right;\">0.66818  </td><td style=\"text-align: right;\">  1.93408</td><td style=\"text-align: right;\">     0.326297 </td><td style=\"text-align: right;\">   0.052371   </td><td style=\"text-align: right;\">     0.185438   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         6.80843</td><td style=\"text-align: right;\">     0.222921   </td></tr>\n",
       "<tr><td>train_boston_8f640ba2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.570075 </td><td style=\"text-align: right;\">      0.262998 </td><td style=\"text-align: right;\">    137.453 </td><td style=\"text-align: right;\">0.0426636 </td><td style=\"text-align: right;\">0.639005 </td><td style=\"text-align: right;\">  2.21432</td><td style=\"text-align: right;\">     0.445552 </td><td style=\"text-align: right;\">   0.0633783  </td><td style=\"text-align: right;\">    21.8664     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.36657</td><td style=\"text-align: right;\">    24.016      </td></tr>\n",
       "<tr><td>train_boston_8ff22c2a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.536743 </td><td style=\"text-align: right;\">      0.183535 </td><td style=\"text-align: right;\">    160.026 </td><td style=\"text-align: right;\">0.0294304 </td><td style=\"text-align: right;\">0.572604 </td><td style=\"text-align: right;\">  1.69583</td><td style=\"text-align: right;\">     0.625712 </td><td style=\"text-align: right;\">   0.0319367  </td><td style=\"text-align: right;\">     2.83216    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.60642</td><td style=\"text-align: right;\">     2.57555    </td></tr>\n",
       "<tr><td>train_boston_95e1dacc</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.3026   </td><td style=\"text-align: right;\">      0.347987 </td><td style=\"text-align: right;\">    174.771 </td><td style=\"text-align: right;\">0.0179181 </td><td style=\"text-align: right;\">0.929853 </td><td style=\"text-align: right;\">  1.37883</td><td style=\"text-align: right;\">     0.519301 </td><td style=\"text-align: right;\">   0.0206717  </td><td style=\"text-align: right;\">     8.14262    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.59933</td><td style=\"text-align: right;\">     6.47387    </td></tr>\n",
       "<tr><td>train_boston_95f8ad6a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.416446 </td><td style=\"text-align: right;\">      0.20317  </td><td style=\"text-align: right;\">    192.763 </td><td style=\"text-align: right;\">0.00893143</td><td style=\"text-align: right;\">0.565872 </td><td style=\"text-align: right;\">  1.32265</td><td style=\"text-align: right;\">     0.448567 </td><td style=\"text-align: right;\">   0.056085   </td><td style=\"text-align: right;\">     6.07718    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.4979 </td><td style=\"text-align: right;\">     5.021      </td></tr>\n",
       "<tr><td>train_boston_960b6478</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.531108 </td><td style=\"text-align: right;\">      0.226554 </td><td style=\"text-align: right;\">    118.425 </td><td style=\"text-align: right;\">0.0508432 </td><td style=\"text-align: right;\">0.472605 </td><td style=\"text-align: right;\">  1.91847</td><td style=\"text-align: right;\">     0.490214 </td><td style=\"text-align: right;\">   0.0398267  </td><td style=\"text-align: right;\">  2325.55       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.22014</td><td style=\"text-align: right;\">  2422.1        </td></tr>\n",
       "<tr><td>train_boston_9634e06e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.56664  </td><td style=\"text-align: right;\">      0.374291 </td><td style=\"text-align: right;\">    113.872 </td><td style=\"text-align: right;\">0.0434054 </td><td style=\"text-align: right;\">0.188433 </td><td style=\"text-align: right;\">  1.93065</td><td style=\"text-align: right;\">     0.382412 </td><td style=\"text-align: right;\">   0.0544758  </td><td style=\"text-align: right;\"> 58100.9        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.12837</td><td style=\"text-align: right;\"> 63385.1        </td></tr>\n",
       "<tr><td>train_boston_9655bbcc</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.545211 </td><td style=\"text-align: right;\">      0.347435 </td><td style=\"text-align: right;\">    192.53  </td><td style=\"text-align: right;\">0.0704904 </td><td style=\"text-align: right;\">0.45929  </td><td style=\"text-align: right;\">  1.44864</td><td style=\"text-align: right;\">     0.644256 </td><td style=\"text-align: right;\">   0.0537678  </td><td style=\"text-align: right;\">201968          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.69962</td><td style=\"text-align: right;\">218349          </td></tr>\n",
       "<tr><td>train_boston_9a6cbbe8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.491043 </td><td style=\"text-align: right;\">      0.247405 </td><td style=\"text-align: right;\">    177.323 </td><td style=\"text-align: right;\">0.0425409 </td><td style=\"text-align: right;\">0.689932 </td><td style=\"text-align: right;\">  1.84214</td><td style=\"text-align: right;\">     0.510027 </td><td style=\"text-align: right;\">   0.0342212  </td><td style=\"text-align: right;\">    23.0244     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.15732</td><td style=\"text-align: right;\">    23.1159     </td></tr>\n",
       "<tr><td>train_boston_9a950a58</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.682544 </td><td style=\"text-align: right;\">      0.246236 </td><td style=\"text-align: right;\">     89.8635</td><td style=\"text-align: right;\">0.0520941 </td><td style=\"text-align: right;\">0.551439 </td><td style=\"text-align: right;\">  1.76615</td><td style=\"text-align: right;\">     0.240776 </td><td style=\"text-align: right;\">   0.0249756  </td><td style=\"text-align: right;\">     4.01713    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.05543</td><td style=\"text-align: right;\">     4.6677     </td></tr>\n",
       "<tr><td>train_boston_9ab839f6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.800305 </td><td style=\"text-align: right;\">      0.408902 </td><td style=\"text-align: right;\">    246.36  </td><td style=\"text-align: right;\">0.00496684</td><td style=\"text-align: right;\">0.760581 </td><td style=\"text-align: right;\">  2.18419</td><td style=\"text-align: right;\">     0.446059 </td><td style=\"text-align: right;\">   0.0819502  </td><td style=\"text-align: right;\">   104.877      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.38637</td><td style=\"text-align: right;\">   109.937      </td></tr>\n",
       "<tr><td>train_boston_9d563b72</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.543483 </td><td style=\"text-align: right;\">      0.153491 </td><td style=\"text-align: right;\">    138.196 </td><td style=\"text-align: right;\">0.0608216 </td><td style=\"text-align: right;\">0.484869 </td><td style=\"text-align: right;\">  2.28706</td><td style=\"text-align: right;\">     0.555232 </td><td style=\"text-align: right;\">   0.0175784  </td><td style=\"text-align: right;\">310103          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.12966</td><td style=\"text-align: right;\">322547          </td></tr>\n",
       "<tr><td>train_boston_9d736dd2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.600616 </td><td style=\"text-align: right;\">      0.125398 </td><td style=\"text-align: right;\">    150.717 </td><td style=\"text-align: right;\">0.0481093 </td><td style=\"text-align: right;\">0.723921 </td><td style=\"text-align: right;\">  2.84673</td><td style=\"text-align: right;\">     0.686256 </td><td style=\"text-align: right;\">   0.0452479  </td><td style=\"text-align: right;\">     0.0176981  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.26565</td><td style=\"text-align: right;\">     0.0138061  </td></tr>\n",
       "<tr><td>train_boston_9d86d610</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.198855 </td><td style=\"text-align: right;\">      0.16489  </td><td style=\"text-align: right;\">    101.793 </td><td style=\"text-align: right;\">0.0763515 </td><td style=\"text-align: right;\">0.434287 </td><td style=\"text-align: right;\">  1.60977</td><td style=\"text-align: right;\">     0.281829 </td><td style=\"text-align: right;\">   0.0476293  </td><td style=\"text-align: right;\">     1.18582e+06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.98023</td><td style=\"text-align: right;\">     1.28823e+06</td></tr>\n",
       "<tr><td>train_boston_9dd334a6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.171393 </td><td style=\"text-align: right;\">      0.14411  </td><td style=\"text-align: right;\">    190.224 </td><td style=\"text-align: right;\">0.024465  </td><td style=\"text-align: right;\">0.634095 </td><td style=\"text-align: right;\">  1.84269</td><td style=\"text-align: right;\">     0.154571 </td><td style=\"text-align: right;\">   0.0576795  </td><td style=\"text-align: right;\">     7.60498    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.65187</td><td style=\"text-align: right;\">     7.63246    </td></tr>\n",
       "<tr><td>train_boston_9dec32d0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.247601 </td><td style=\"text-align: right;\">      0.189744 </td><td style=\"text-align: right;\">    168.904 </td><td style=\"text-align: right;\">0.0208652 </td><td style=\"text-align: right;\">0.700728 </td><td style=\"text-align: right;\">  1.87337</td><td style=\"text-align: right;\">     0.483494 </td><td style=\"text-align: right;\">   0.0664963  </td><td style=\"text-align: right;\">     9.77046    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.78631</td><td style=\"text-align: right;\">     9.27259    </td></tr>\n",
       "<tr><td>train_boston_a0c6bc50</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.460592 </td><td style=\"text-align: right;\">      0.308467 </td><td style=\"text-align: right;\">    219.438 </td><td style=\"text-align: right;\">0.0336784 </td><td style=\"text-align: right;\">0.579486 </td><td style=\"text-align: right;\">  1.65722</td><td style=\"text-align: right;\">     0.419804 </td><td style=\"text-align: right;\">   0.0424805  </td><td style=\"text-align: right;\">     2.54972    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.74073</td><td style=\"text-align: right;\">     1.78043    </td></tr>\n",
       "<tr><td>train_boston_a1479262</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.686723 </td><td style=\"text-align: right;\">      0.233829 </td><td style=\"text-align: right;\">    134.376 </td><td style=\"text-align: right;\">0.0325007 </td><td style=\"text-align: right;\">0.614192 </td><td style=\"text-align: right;\">  1.91777</td><td style=\"text-align: right;\">     0.5039   </td><td style=\"text-align: right;\">   0.0547438  </td><td style=\"text-align: right;\">     2.79652    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.64916</td><td style=\"text-align: right;\">     2.21008    </td></tr>\n",
       "<tr><td>train_boston_a619df52</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.31741  </td><td style=\"text-align: right;\">      0.177132 </td><td style=\"text-align: right;\">    127.643 </td><td style=\"text-align: right;\">0.0317783 </td><td style=\"text-align: right;\">0.415688 </td><td style=\"text-align: right;\">  1.69883</td><td style=\"text-align: right;\">     0.322531 </td><td style=\"text-align: right;\">   0.0617326  </td><td style=\"text-align: right;\">   395.5        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.41808</td><td style=\"text-align: right;\">   451.68       </td></tr>\n",
       "<tr><td>train_boston_a655edf8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.143659 </td><td style=\"text-align: right;\">      0.242156 </td><td style=\"text-align: right;\">    125.77  </td><td style=\"text-align: right;\">0.0272236 </td><td style=\"text-align: right;\">0.488714 </td><td style=\"text-align: right;\">  1.70491</td><td style=\"text-align: right;\">     0.379345 </td><td style=\"text-align: right;\">   0.043532   </td><td style=\"text-align: right;\">   276.365      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.16208</td><td style=\"text-align: right;\">   305.973      </td></tr>\n",
       "<tr><td>train_boston_a8169502</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.620664 </td><td style=\"text-align: right;\">      0.239108 </td><td style=\"text-align: right;\">    140.904 </td><td style=\"text-align: right;\">0.0430372 </td><td style=\"text-align: right;\">0.667116 </td><td style=\"text-align: right;\">  1.54519</td><td style=\"text-align: right;\">     0.146794 </td><td style=\"text-align: right;\">   0.0383176  </td><td style=\"text-align: right;\">    30.0217     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.50886</td><td style=\"text-align: right;\">    29.7656     </td></tr>\n",
       "<tr><td>train_boston_a87e46de</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.297687 </td><td style=\"text-align: right;\">      0.177826 </td><td style=\"text-align: right;\">    118.41  </td><td style=\"text-align: right;\">0.0477556 </td><td style=\"text-align: right;\">0.800832 </td><td style=\"text-align: right;\">  1.70228</td><td style=\"text-align: right;\">     0.262115 </td><td style=\"text-align: right;\">   0.0505323  </td><td style=\"text-align: right;\">    35.7362     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.36676</td><td style=\"text-align: right;\">    36.229      </td></tr>\n",
       "<tr><td>train_boston_a894b91e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.520757 </td><td style=\"text-align: right;\">      0.193417 </td><td style=\"text-align: right;\">    155.741 </td><td style=\"text-align: right;\">0.0471642 </td><td style=\"text-align: right;\">0.699423 </td><td style=\"text-align: right;\">  1.75887</td><td style=\"text-align: right;\">     0.387444 </td><td style=\"text-align: right;\">   0.0210444  </td><td style=\"text-align: right;\">     3.76307    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.3562 </td><td style=\"text-align: right;\">     3.83825    </td></tr>\n",
       "<tr><td>train_boston_a8dcb872</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.354969 </td><td style=\"text-align: right;\">      0.261372 </td><td style=\"text-align: right;\">    189.504 </td><td style=\"text-align: right;\">0.0374622 </td><td style=\"text-align: right;\">0.714425 </td><td style=\"text-align: right;\">  2.45151</td><td style=\"text-align: right;\">     0.267378 </td><td style=\"text-align: right;\">   0.0350583  </td><td style=\"text-align: right;\">     0.0498569  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         2.3976 </td><td style=\"text-align: right;\">     0.0425032  </td></tr>\n",
       "<tr><td>train_boston_a927a940</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.381451 </td><td style=\"text-align: right;\">      0.456116 </td><td style=\"text-align: right;\">     75.9657</td><td style=\"text-align: right;\">0.0339213 </td><td style=\"text-align: right;\">0.0262608</td><td style=\"text-align: right;\">  2.19612</td><td style=\"text-align: right;\">     0.0681097</td><td style=\"text-align: right;\">   0.0494103  </td><td style=\"text-align: right;\">    32.8022     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.25265</td><td style=\"text-align: right;\">    37.5393     </td></tr>\n",
       "<tr><td>train_boston_a98b1520</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.184063 </td><td style=\"text-align: right;\">      0.27652  </td><td style=\"text-align: right;\">    204.805 </td><td style=\"text-align: right;\">0.0412174 </td><td style=\"text-align: right;\">0.50092  </td><td style=\"text-align: right;\">  1.9418 </td><td style=\"text-align: right;\">     0.686055 </td><td style=\"text-align: right;\">   0.0279622  </td><td style=\"text-align: right;\">     2.03077    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.44768</td><td style=\"text-align: right;\">     1.70482    </td></tr>\n",
       "<tr><td>train_boston_ad291b28</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.839875 </td><td style=\"text-align: right;\">      0.254554 </td><td style=\"text-align: right;\">    214.978 </td><td style=\"text-align: right;\">0.0429358 </td><td style=\"text-align: right;\">0.485615 </td><td style=\"text-align: right;\">  1.56329</td><td style=\"text-align: right;\">     0.722377 </td><td style=\"text-align: right;\">   0.036167   </td><td style=\"text-align: right;\">271941          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66989</td><td style=\"text-align: right;\">297694          </td></tr>\n",
       "<tr><td>train_boston_ae7cce02</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.566836 </td><td style=\"text-align: right;\">      0.345332 </td><td style=\"text-align: right;\">    120.636 </td><td style=\"text-align: right;\">0.0319898 </td><td style=\"text-align: right;\">0.661581 </td><td style=\"text-align: right;\">  1.87137</td><td style=\"text-align: right;\">     0.524062 </td><td style=\"text-align: right;\">   0.0339464  </td><td style=\"text-align: right;\">     0.0245559  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.3261 </td><td style=\"text-align: right;\">     0.0329921  </td></tr>\n",
       "<tr><td>train_boston_aefce2c2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.499413 </td><td style=\"text-align: right;\">      0.269809 </td><td style=\"text-align: right;\">    139.778 </td><td style=\"text-align: right;\">0.0336974 </td><td style=\"text-align: right;\">0.635814 </td><td style=\"text-align: right;\">  2.05088</td><td style=\"text-align: right;\">     0.322634 </td><td style=\"text-align: right;\">   0.0339496  </td><td style=\"text-align: right;\">    26.9367     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.09039</td><td style=\"text-align: right;\">    26.06       </td></tr>\n",
       "<tr><td>train_boston_af1781c2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.331966 </td><td style=\"text-align: right;\">      0.312748 </td><td style=\"text-align: right;\">    152.254 </td><td style=\"text-align: right;\">0.0502742 </td><td style=\"text-align: right;\">0.70239  </td><td style=\"text-align: right;\">  1.48588</td><td style=\"text-align: right;\">     0.293042 </td><td style=\"text-align: right;\">   0.0527697  </td><td style=\"text-align: right;\">     0.939003   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.05728</td><td style=\"text-align: right;\">     1.38904    </td></tr>\n",
       "<tr><td>train_boston_af91bb40</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.62351  </td><td style=\"text-align: right;\">      0.380807 </td><td style=\"text-align: right;\">    152.723 </td><td style=\"text-align: right;\">0.0391377 </td><td style=\"text-align: right;\">0.651054 </td><td style=\"text-align: right;\">  2.29525</td><td style=\"text-align: right;\">     0.462491 </td><td style=\"text-align: right;\">   0.0828395  </td><td style=\"text-align: right;\">    30.6842     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.99455</td><td style=\"text-align: right;\">    28.3918     </td></tr>\n",
       "<tr><td>train_boston_b1aaa7a2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.432021 </td><td style=\"text-align: right;\">      0.286984 </td><td style=\"text-align: right;\">    107.074 </td><td style=\"text-align: right;\">0.0539774 </td><td style=\"text-align: right;\">0.645092 </td><td style=\"text-align: right;\">  1.82461</td><td style=\"text-align: right;\">     0.621021 </td><td style=\"text-align: right;\">   0.0370416  </td><td style=\"text-align: right;\">     2.38202    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.86653</td><td style=\"text-align: right;\">     2.16812    </td></tr>\n",
       "<tr><td>train_boston_b1c6d116</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.454935 </td><td style=\"text-align: right;\">      0.323986 </td><td style=\"text-align: right;\">    150.76  </td><td style=\"text-align: right;\">0.0367871 </td><td style=\"text-align: right;\">0.66585  </td><td style=\"text-align: right;\">  1.79458</td><td style=\"text-align: right;\">     0.379663 </td><td style=\"text-align: right;\">   0.0436873  </td><td style=\"text-align: right;\">    28.5797     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.68116</td><td style=\"text-align: right;\">    25.3525     </td></tr>\n",
       "<tr><td>train_boston_b1dad436</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.683908 </td><td style=\"text-align: right;\">      0.274684 </td><td style=\"text-align: right;\">    183.091 </td><td style=\"text-align: right;\">0.0617144 </td><td style=\"text-align: right;\">0.954308 </td><td style=\"text-align: right;\">  1.96979</td><td style=\"text-align: right;\">     0.351341 </td><td style=\"text-align: right;\">   0.0495231  </td><td style=\"text-align: right;\">    44.0935     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.99781</td><td style=\"text-align: right;\">    44.4035     </td></tr>\n",
       "<tr><td>train_boston_b1f53d6c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.542912 </td><td style=\"text-align: right;\">      0.349619 </td><td style=\"text-align: right;\">    152.015 </td><td style=\"text-align: right;\">0.0704049 </td><td style=\"text-align: right;\">0.803264 </td><td style=\"text-align: right;\">  2.59959</td><td style=\"text-align: right;\">     0.703883 </td><td style=\"text-align: right;\">   0.0394677  </td><td style=\"text-align: right;\">    37.2428     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.05938</td><td style=\"text-align: right;\">    35.8061     </td></tr>\n",
       "<tr><td>train_boston_b3512c0c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.444367 </td><td style=\"text-align: right;\">      0.255866 </td><td style=\"text-align: right;\">    191.087 </td><td style=\"text-align: right;\">0.0488875 </td><td style=\"text-align: right;\">0.721741 </td><td style=\"text-align: right;\">  1.52851</td><td style=\"text-align: right;\">     0.702947 </td><td style=\"text-align: right;\">   0.0656437  </td><td style=\"text-align: right;\">     2.25142    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.92993</td><td style=\"text-align: right;\">     2.22351    </td></tr>\n",
       "<tr><td>train_boston_b3680602</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.474213 </td><td style=\"text-align: right;\">      0.409848 </td><td style=\"text-align: right;\">    142.929 </td><td style=\"text-align: right;\">0.0395739 </td><td style=\"text-align: right;\">0.261121 </td><td style=\"text-align: right;\">  2.44284</td><td style=\"text-align: right;\">     0.851161 </td><td style=\"text-align: right;\">   0.000838513</td><td style=\"text-align: right;\">    99.5263     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.02009</td><td style=\"text-align: right;\">    91.7263     </td></tr>\n",
       "<tr><td>train_boston_b39d9312</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.547233 </td><td style=\"text-align: right;\">      0.153082 </td><td style=\"text-align: right;\">    144.683 </td><td style=\"text-align: right;\">0.0715696 </td><td style=\"text-align: right;\">0.994815 </td><td style=\"text-align: right;\">  1.94286</td><td style=\"text-align: right;\">     0.32218  </td><td style=\"text-align: right;\">   0.0365085  </td><td style=\"text-align: right;\">     0.032869   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         5.95907</td><td style=\"text-align: right;\">     0.0362143  </td></tr>\n",
       "<tr><td>train_boston_b3d5b6a2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.362208 </td><td style=\"text-align: right;\">      0.452299 </td><td style=\"text-align: right;\">     98.3715</td><td style=\"text-align: right;\">0.079424  </td><td style=\"text-align: right;\">0.502342 </td><td style=\"text-align: right;\">  2.22116</td><td style=\"text-align: right;\">     0.598859 </td><td style=\"text-align: right;\">   0.0184037  </td><td style=\"text-align: right;\">    63.6943     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.14704</td><td style=\"text-align: right;\">    64.7023     </td></tr>\n",
       "<tr><td>train_boston_b756bec0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.522346 </td><td style=\"text-align: right;\">      0.205115 </td><td style=\"text-align: right;\">    136.295 </td><td style=\"text-align: right;\">0.0350616 </td><td style=\"text-align: right;\">0.62359  </td><td style=\"text-align: right;\">  2.14651</td><td style=\"text-align: right;\">     0.454182 </td><td style=\"text-align: right;\">   0.0417697  </td><td style=\"text-align: right;\">     2.74257    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.4279 </td><td style=\"text-align: right;\">     3.08808    </td></tr>\n",
       "<tr><td>train_boston_b98e56ee</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.264312 </td><td style=\"text-align: right;\">      0.0927229</td><td style=\"text-align: right;\">    116.877 </td><td style=\"text-align: right;\">0.0521002 </td><td style=\"text-align: right;\">0.38644  </td><td style=\"text-align: right;\">  1.96333</td><td style=\"text-align: right;\">     0.627763 </td><td style=\"text-align: right;\">   0.00101418 </td><td style=\"text-align: right;\">  2123.65       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.21221</td><td style=\"text-align: right;\">  2000.92       </td></tr>\n",
       "<tr><td>train_boston_b9bad552</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.621262 </td><td style=\"text-align: right;\">      0.185881 </td><td style=\"text-align: right;\">    205.993 </td><td style=\"text-align: right;\">0.0426874 </td><td style=\"text-align: right;\">0.732505 </td><td style=\"text-align: right;\">  2.53372</td><td style=\"text-align: right;\">     0.408185 </td><td style=\"text-align: right;\">   0.0732777  </td><td style=\"text-align: right;\">    32.97       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.31749</td><td style=\"text-align: right;\">    35.4301     </td></tr>\n",
       "<tr><td>train_boston_b9ec290e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.400141 </td><td style=\"text-align: right;\">      0.284487 </td><td style=\"text-align: right;\">    178.964 </td><td style=\"text-align: right;\">0.0126345 </td><td style=\"text-align: right;\">0.443879 </td><td style=\"text-align: right;\">  2.09371</td><td style=\"text-align: right;\">     0.498973 </td><td style=\"text-align: right;\">   0.0497483  </td><td style=\"text-align: right;\">   111.945      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.52322</td><td style=\"text-align: right;\">   121.206      </td></tr>\n",
       "<tr><td>train_boston_ba0d39d2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.459795 </td><td style=\"text-align: right;\">      0.31571  </td><td style=\"text-align: right;\">    200.429 </td><td style=\"text-align: right;\">0.0674684 </td><td style=\"text-align: right;\">0.561107 </td><td style=\"text-align: right;\">  1.72424</td><td style=\"text-align: right;\">     0.579005 </td><td style=\"text-align: right;\">   0.0503761  </td><td style=\"text-align: right;\">    61.8991     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.98856</td><td style=\"text-align: right;\">    66.4907     </td></tr>\n",
       "<tr><td>train_boston_ba7b7a8c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.534113 </td><td style=\"text-align: right;\">      0.176892 </td><td style=\"text-align: right;\">    197.909 </td><td style=\"text-align: right;\">0.0389022 </td><td style=\"text-align: right;\">0.61196  </td><td style=\"text-align: right;\">  2.14611</td><td style=\"text-align: right;\">     0.498739 </td><td style=\"text-align: right;\">   0.0342538  </td><td style=\"text-align: right;\">     1.61462    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.75555</td><td style=\"text-align: right;\">     1.63826    </td></tr>\n",
       "<tr><td>train_boston_bd111810</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.422742 </td><td style=\"text-align: right;\">      0.234044 </td><td style=\"text-align: right;\">    171.781 </td><td style=\"text-align: right;\">0.0501579 </td><td style=\"text-align: right;\">0.550436 </td><td style=\"text-align: right;\">  2.01101</td><td style=\"text-align: right;\">     0.621373 </td><td style=\"text-align: right;\">   0.0430053  </td><td style=\"text-align: right;\">    38.8177     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.59874</td><td style=\"text-align: right;\">    35.1448     </td></tr>\n",
       "<tr><td>train_boston_bd264776</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.521283 </td><td style=\"text-align: right;\">      0.0791755</td><td style=\"text-align: right;\">    157.624 </td><td style=\"text-align: right;\">0.0549776 </td><td style=\"text-align: right;\">0.709891 </td><td style=\"text-align: right;\">  2.0782 </td><td style=\"text-align: right;\">     0.626014 </td><td style=\"text-align: right;\">   0.0525295  </td><td style=\"text-align: right;\">    25.5824     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.18277</td><td style=\"text-align: right;\">    29.7839     </td></tr>\n",
       "<tr><td>train_boston_bd56fbdc</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.246312 </td><td style=\"text-align: right;\">      0.205365 </td><td style=\"text-align: right;\">    146.841 </td><td style=\"text-align: right;\">0.0258175 </td><td style=\"text-align: right;\">0.675798 </td><td style=\"text-align: right;\">  2.00062</td><td style=\"text-align: right;\">     0.906868 </td><td style=\"text-align: right;\">   0.0542552  </td><td style=\"text-align: right;\">    72.1162     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.09905</td><td style=\"text-align: right;\">    79.3703     </td></tr>\n",
       "<tr><td>train_boston_c02201c2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.585283 </td><td style=\"text-align: right;\">      0.168031 </td><td style=\"text-align: right;\">    192.706 </td><td style=\"text-align: right;\">0.0580726 </td><td style=\"text-align: right;\">0.761556 </td><td style=\"text-align: right;\">  2.77035</td><td style=\"text-align: right;\">     0.622071 </td><td style=\"text-align: right;\">   0.0555236  </td><td style=\"text-align: right;\">    18.984      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.85861</td><td style=\"text-align: right;\">    17.1285     </td></tr>\n",
       "<tr><td>train_boston_c03a7626</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.55804  </td><td style=\"text-align: right;\">      0.420679 </td><td style=\"text-align: right;\">    133.881 </td><td style=\"text-align: right;\">0.00662518</td><td style=\"text-align: right;\">0.839039 </td><td style=\"text-align: right;\">  2.07005</td><td style=\"text-align: right;\">     0.669954 </td><td style=\"text-align: right;\">   0.012033   </td><td style=\"text-align: right;\">    51.886      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.63984</td><td style=\"text-align: right;\">    58.0368     </td></tr>\n",
       "<tr><td>train_boston_c0511ac0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.165168 </td><td style=\"text-align: right;\">      0.252135 </td><td style=\"text-align: right;\">    151.586 </td><td style=\"text-align: right;\">0.0169563 </td><td style=\"text-align: right;\">0.348973 </td><td style=\"text-align: right;\">  1.68868</td><td style=\"text-align: right;\">     0.13619  </td><td style=\"text-align: right;\">   0.0333214  </td><td style=\"text-align: right;\">   140.982      </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.30085</td><td style=\"text-align: right;\">   156.413      </td></tr>\n",
       "<tr><td>train_boston_c06a5f6c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.521259 </td><td style=\"text-align: right;\">      0.26567  </td><td style=\"text-align: right;\">    178.269 </td><td style=\"text-align: right;\">0.0110667 </td><td style=\"text-align: right;\">0.604324 </td><td style=\"text-align: right;\">  2.67193</td><td style=\"text-align: right;\">     0.116338 </td><td style=\"text-align: right;\">   0.0254044  </td><td style=\"text-align: right;\">    46.178      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.70345</td><td style=\"text-align: right;\">    49.0851     </td></tr>\n",
       "<tr><td>train_boston_c0882556</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.39573  </td><td style=\"text-align: right;\">      0.229489 </td><td style=\"text-align: right;\">    171.26  </td><td style=\"text-align: right;\">0.0407829 </td><td style=\"text-align: right;\">0.727558 </td><td style=\"text-align: right;\">  2.20729</td><td style=\"text-align: right;\">     0.373129 </td><td style=\"text-align: right;\">   0.0325459  </td><td style=\"text-align: right;\">    23.7245     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.20002</td><td style=\"text-align: right;\">    26.989      </td></tr>\n",
       "<tr><td>train_boston_c0ea4632</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.452183 </td><td style=\"text-align: right;\">      0.200138 </td><td style=\"text-align: right;\">    112.481 </td><td style=\"text-align: right;\">0.0605911 </td><td style=\"text-align: right;\">0.370068 </td><td style=\"text-align: right;\">  1.87888</td><td style=\"text-align: right;\">     0.69175  </td><td style=\"text-align: right;\">   0.0139369  </td><td style=\"text-align: right;\"> 78875.8        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.099  </td><td style=\"text-align: right;\"> 79633.1        </td></tr>\n",
       "<tr><td>train_boston_c2f9df5a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.429438 </td><td style=\"text-align: right;\">      0.240486 </td><td style=\"text-align: right;\">    167.28  </td><td style=\"text-align: right;\">0.0203529 </td><td style=\"text-align: right;\">0.547552 </td><td style=\"text-align: right;\">  2.79207</td><td style=\"text-align: right;\">     0.492813 </td><td style=\"text-align: right;\">   0.0387693  </td><td style=\"text-align: right;\">     2.65646    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.71651</td><td style=\"text-align: right;\">     3.5579     </td></tr>\n",
       "<tr><td>train_boston_c75c4c86</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.304298 </td><td style=\"text-align: right;\">      0.246762 </td><td style=\"text-align: right;\">    132.332 </td><td style=\"text-align: right;\">0.0355878 </td><td style=\"text-align: right;\">0.563097 </td><td style=\"text-align: right;\">  2.22637</td><td style=\"text-align: right;\">     0.312926 </td><td style=\"text-align: right;\">   0.0478637  </td><td style=\"text-align: right;\">     0.0196835  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11.6057 </td><td style=\"text-align: right;\">     0.0163426  </td></tr>\n",
       "<tr><td>train_boston_c77aef7e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.314747 </td><td style=\"text-align: right;\">      0.108231 </td><td style=\"text-align: right;\">    133.761 </td><td style=\"text-align: right;\">0.00115683</td><td style=\"text-align: right;\">0.55424  </td><td style=\"text-align: right;\">  2.66099</td><td style=\"text-align: right;\">     0.841634 </td><td style=\"text-align: right;\">   0.0792514  </td><td style=\"text-align: right;\">     3.17102    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.2096 </td><td style=\"text-align: right;\">     2.893      </td></tr>\n",
       "<tr><td>train_boston_c79444c4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.465198 </td><td style=\"text-align: right;\">      0.201419 </td><td style=\"text-align: right;\">    165.285 </td><td style=\"text-align: right;\">0.0453268 </td><td style=\"text-align: right;\">0.501317 </td><td style=\"text-align: right;\">  1.95597</td><td style=\"text-align: right;\">     0.833234 </td><td style=\"text-align: right;\">   0.0255069  </td><td style=\"text-align: right;\">     4.31006    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.33916</td><td style=\"text-align: right;\">     3.11828    </td></tr>\n",
       "<tr><td>train_boston_c7a4fe72</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.348503 </td><td style=\"text-align: right;\">      0.223727 </td><td style=\"text-align: right;\">    166.284 </td><td style=\"text-align: right;\">0.0400411 </td><td style=\"text-align: right;\">0.490048 </td><td style=\"text-align: right;\">  1.77199</td><td style=\"text-align: right;\">     0.287644 </td><td style=\"text-align: right;\">   0.0289654  </td><td style=\"text-align: right;\">  1597.54       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.0187 </td><td style=\"text-align: right;\">  1554.09       </td></tr>\n",
       "<tr><td>train_boston_c7c7a760</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.916014 </td><td style=\"text-align: right;\">      0.0366582</td><td style=\"text-align: right;\">    160.643 </td><td style=\"text-align: right;\">0.00548437</td><td style=\"text-align: right;\">0.861054 </td><td style=\"text-align: right;\">  2.84773</td><td style=\"text-align: right;\">     0.414485 </td><td style=\"text-align: right;\">   0.0562033  </td><td style=\"text-align: right;\">    90.023      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.02405</td><td style=\"text-align: right;\">    96.0119     </td></tr>\n",
       "<tr><td>train_boston_c7e683ec</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.633669 </td><td style=\"text-align: right;\">      0.145897 </td><td style=\"text-align: right;\">    141.157 </td><td style=\"text-align: right;\">0.0662082 </td><td style=\"text-align: right;\">0.610333 </td><td style=\"text-align: right;\">  2.08791</td><td style=\"text-align: right;\">     0.606378 </td><td style=\"text-align: right;\">   0.00271616 </td><td style=\"text-align: right;\">    32.9562     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.74446</td><td style=\"text-align: right;\">    35.9217     </td></tr>\n",
       "<tr><td>train_boston_c803a08a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.318625 </td><td style=\"text-align: right;\">      0.241181 </td><td style=\"text-align: right;\">    177.937 </td><td style=\"text-align: right;\">0.0354724 </td><td style=\"text-align: right;\">0.751875 </td><td style=\"text-align: right;\">  2.25168</td><td style=\"text-align: right;\">     0.399819 </td><td style=\"text-align: right;\">   0.015136   </td><td style=\"text-align: right;\">    30.0222     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.8441 </td><td style=\"text-align: right;\">    30.6679     </td></tr>\n",
       "<tr><td>train_boston_c82faafe</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.574652 </td><td style=\"text-align: right;\">      0.173798 </td><td style=\"text-align: right;\">    151.4   </td><td style=\"text-align: right;\">0.0488563 </td><td style=\"text-align: right;\">0.663589 </td><td style=\"text-align: right;\">  2.10754</td><td style=\"text-align: right;\">     0.524317 </td><td style=\"text-align: right;\">   0.0474275  </td><td style=\"text-align: right;\">    18.1133     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.78317</td><td style=\"text-align: right;\">    19.2242     </td></tr>\n",
       "<tr><td>train_boston_d2043d74</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.249628 </td><td style=\"text-align: right;\">      0.441918 </td><td style=\"text-align: right;\">    138.743 </td><td style=\"text-align: right;\">0.0388521 </td><td style=\"text-align: right;\">0.502709 </td><td style=\"text-align: right;\">  1.99702</td><td style=\"text-align: right;\">     0.256661 </td><td style=\"text-align: right;\">   0.0140234  </td><td style=\"text-align: right;\">     1.57186    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.07461</td><td style=\"text-align: right;\">     1.33482    </td></tr>\n",
       "<tr><td>train_boston_d22b0e54</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.328407 </td><td style=\"text-align: right;\">      0.240359 </td><td style=\"text-align: right;\">    248.086 </td><td style=\"text-align: right;\">0.0386214 </td><td style=\"text-align: right;\">0.268264 </td><td style=\"text-align: right;\">  2.42006</td><td style=\"text-align: right;\">     0.887825 </td><td style=\"text-align: right;\">   0.0482474  </td><td style=\"text-align: right;\"> 13102.9        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.90154</td><td style=\"text-align: right;\"> 14927.1        </td></tr>\n",
       "<tr><td>train_boston_d23b1bfa</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.632657 </td><td style=\"text-align: right;\">      0.30583  </td><td style=\"text-align: right;\">    199.382 </td><td style=\"text-align: right;\">0.0283715 </td><td style=\"text-align: right;\">0.645763 </td><td style=\"text-align: right;\">  1.64681</td><td style=\"text-align: right;\">     0.313485 </td><td style=\"text-align: right;\">   0.042413   </td><td style=\"text-align: right;\">     0.0126916  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         7.52154</td><td style=\"text-align: right;\">     0.0131606  </td></tr>\n",
       "<tr><td>train_boston_d24c24a4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.305587 </td><td style=\"text-align: right;\">      0.229061 </td><td style=\"text-align: right;\">    126.225 </td><td style=\"text-align: right;\">0.0309762 </td><td style=\"text-align: right;\">0.806676 </td><td style=\"text-align: right;\">  2.06026</td><td style=\"text-align: right;\">     0.40201  </td><td style=\"text-align: right;\">   0.0441451  </td><td style=\"text-align: right;\">     0.266559   </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         7.04138</td><td style=\"text-align: right;\">     0.249468   </td></tr>\n",
       "<tr><td>train_boston_d25ddeba</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.459255 </td><td style=\"text-align: right;\">      0.286607 </td><td style=\"text-align: right;\">    154.31  </td><td style=\"text-align: right;\">0.0472496 </td><td style=\"text-align: right;\">0.588179 </td><td style=\"text-align: right;\">  1.77063</td><td style=\"text-align: right;\">     0.363265 </td><td style=\"text-align: right;\">   0.0527194  </td><td style=\"text-align: right;\">    14.2407     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.87492</td><td style=\"text-align: right;\">    14.9833     </td></tr>\n",
       "<tr><td>train_boston_d26a772e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.219714 </td><td style=\"text-align: right;\">      0.242229 </td><td style=\"text-align: right;\">    191.99  </td><td style=\"text-align: right;\">0.0361168 </td><td style=\"text-align: right;\">0.791222 </td><td style=\"text-align: right;\">  2.09743</td><td style=\"text-align: right;\">     0.320401 </td><td style=\"text-align: right;\">   0.0485871  </td><td style=\"text-align: right;\">     1.10661    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.77722</td><td style=\"text-align: right;\">     1.64223    </td></tr>\n",
       "<tr><td>train_boston_d27bec2a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.424446 </td><td style=\"text-align: right;\">      0.229613 </td><td style=\"text-align: right;\">    181.607 </td><td style=\"text-align: right;\">0.0289984 </td><td style=\"text-align: right;\">0.763126 </td><td style=\"text-align: right;\">  1.73589</td><td style=\"text-align: right;\">     0.367082 </td><td style=\"text-align: right;\">   0.0374044  </td><td style=\"text-align: right;\">     0.0218996  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.83108</td><td style=\"text-align: right;\">     0.0223187  </td></tr>\n",
       "<tr><td>train_boston_d28eb92c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.512514 </td><td style=\"text-align: right;\">      0.122741 </td><td style=\"text-align: right;\">    163.015 </td><td style=\"text-align: right;\">0.0245732 </td><td style=\"text-align: right;\">0.538402 </td><td style=\"text-align: right;\">  1.71663</td><td style=\"text-align: right;\">     0.337302 </td><td style=\"text-align: right;\">   0.0520644  </td><td style=\"text-align: right;\">     1.84246    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.62671</td><td style=\"text-align: right;\">     1.2263     </td></tr>\n",
       "<tr><td>train_boston_d8dde582</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0895564</td><td style=\"text-align: right;\">      0.083594 </td><td style=\"text-align: right;\">    166.922 </td><td style=\"text-align: right;\">0.00616087</td><td style=\"text-align: right;\">0.36667  </td><td style=\"text-align: right;\">  2.39537</td><td style=\"text-align: right;\">     0.396986 </td><td style=\"text-align: right;\">   0.0378791  </td><td style=\"text-align: right;\">    38.9736     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.51631</td><td style=\"text-align: right;\">    42.8107     </td></tr>\n",
       "<tr><td>train_boston_d90204da</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.317111 </td><td style=\"text-align: right;\">      0.286415 </td><td style=\"text-align: right;\">    154.64  </td><td style=\"text-align: right;\">0.0463643 </td><td style=\"text-align: right;\">0.668853 </td><td style=\"text-align: right;\">  2.00882</td><td style=\"text-align: right;\">     0.410662 </td><td style=\"text-align: right;\">   0.0331191  </td><td style=\"text-align: right;\">     0.0124167  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         9.43326</td><td style=\"text-align: right;\">     0.0103137  </td></tr>\n",
       "<tr><td>train_boston_d9133264</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.575527 </td><td style=\"text-align: right;\">      0.157671 </td><td style=\"text-align: right;\">    163.32  </td><td style=\"text-align: right;\">0.0575224 </td><td style=\"text-align: right;\">0.703595 </td><td style=\"text-align: right;\">  1.90434</td><td style=\"text-align: right;\">     0.60033  </td><td style=\"text-align: right;\">   0.0494504  </td><td style=\"text-align: right;\">     1.65247    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.09904</td><td style=\"text-align: right;\">     1.57224    </td></tr>\n",
       "<tr><td>train_boston_d92939c4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.194536 </td><td style=\"text-align: right;\">      0.13331  </td><td style=\"text-align: right;\">    155.238 </td><td style=\"text-align: right;\">0.0280468 </td><td style=\"text-align: right;\">0.750081 </td><td style=\"text-align: right;\">  1.46877</td><td style=\"text-align: right;\">     0.322766 </td><td style=\"text-align: right;\">   0.041729   </td><td style=\"text-align: right;\">    26.207      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.35433</td><td style=\"text-align: right;\">    19.2075     </td></tr>\n",
       "<tr><td>train_boston_d9c93cda</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.422027 </td><td style=\"text-align: right;\">      0.233265 </td><td style=\"text-align: right;\">    156.662 </td><td style=\"text-align: right;\">0.0524353 </td><td style=\"text-align: right;\">0.577937 </td><td style=\"text-align: right;\">  2.00133</td><td style=\"text-align: right;\">     0.525987 </td><td style=\"text-align: right;\">   0.0361351  </td><td style=\"text-align: right;\">     1.86431    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.15027</td><td style=\"text-align: right;\">     1.94485    </td></tr>\n",
       "<tr><td>train_boston_da14be9e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.555371 </td><td style=\"text-align: right;\">      0.265831 </td><td style=\"text-align: right;\">    202.264 </td><td style=\"text-align: right;\">0.0415319 </td><td style=\"text-align: right;\">0.737418 </td><td style=\"text-align: right;\">  1.94238</td><td style=\"text-align: right;\">     0.558193 </td><td style=\"text-align: right;\">   0.0577774  </td><td style=\"text-align: right;\">    18.2792     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05429</td><td style=\"text-align: right;\">    18.2238     </td></tr>\n",
       "<tr><td>train_boston_da1f242e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.368043 </td><td style=\"text-align: right;\">      0.345821 </td><td style=\"text-align: right;\">    164.035 </td><td style=\"text-align: right;\">0.0633466 </td><td style=\"text-align: right;\">0.650749 </td><td style=\"text-align: right;\">  1.82483</td><td style=\"text-align: right;\">     0.360626 </td><td style=\"text-align: right;\">   0.0491105  </td><td style=\"text-align: right;\">    52.9638     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.14805</td><td style=\"text-align: right;\">    57.4363     </td></tr>\n",
       "<tr><td>train_boston_da85479a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.645727 </td><td style=\"text-align: right;\">      0.287729 </td><td style=\"text-align: right;\">    191.326 </td><td style=\"text-align: right;\">0.0444176 </td><td style=\"text-align: right;\">0.861246 </td><td style=\"text-align: right;\">  2.09999</td><td style=\"text-align: right;\">     0.810171 </td><td style=\"text-align: right;\">   0.0236152  </td><td style=\"text-align: right;\">    30.8208     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.52135</td><td style=\"text-align: right;\">    33.6125     </td></tr>\n",
       "<tr><td>train_boston_e00b6bfe</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.379436 </td><td style=\"text-align: right;\">      0.263814 </td><td style=\"text-align: right;\">    186.224 </td><td style=\"text-align: right;\">0.0413872 </td><td style=\"text-align: right;\">0.530491 </td><td style=\"text-align: right;\">  1.75214</td><td style=\"text-align: right;\">     0.636183 </td><td style=\"text-align: right;\">   0.0499047  </td><td style=\"text-align: right;\">    21.5233     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.74371</td><td style=\"text-align: right;\">    20.229      </td></tr>\n",
       "<tr><td>train_boston_e0273c76</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.65063  </td><td style=\"text-align: right;\">      0.291014 </td><td style=\"text-align: right;\">    144.552 </td><td style=\"text-align: right;\">0.0427627 </td><td style=\"text-align: right;\">0.646817 </td><td style=\"text-align: right;\">  1.94688</td><td style=\"text-align: right;\">     0.534796 </td><td style=\"text-align: right;\">   0.0542326  </td><td style=\"text-align: right;\">    18.1281     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.58959</td><td style=\"text-align: right;\">    21.1281     </td></tr>\n",
       "<tr><td>train_boston_e28845be</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.406783 </td><td style=\"text-align: right;\">      0.274347 </td><td style=\"text-align: right;\">    196.436 </td><td style=\"text-align: right;\">0.0473229 </td><td style=\"text-align: right;\">0.629718 </td><td style=\"text-align: right;\">  1.71301</td><td style=\"text-align: right;\">     0.504793 </td><td style=\"text-align: right;\">   0.0482668  </td><td style=\"text-align: right;\">     0.0356832  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        10.2368 </td><td style=\"text-align: right;\">     0.0294535  </td></tr>\n",
       "<tr><td>train_boston_e295abe6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.365921 </td><td style=\"text-align: right;\">      0.38996  </td><td style=\"text-align: right;\">    120.259 </td><td style=\"text-align: right;\">0.0834971 </td><td style=\"text-align: right;\">0.982896 </td><td style=\"text-align: right;\">  2.03265</td><td style=\"text-align: right;\">     0.288967 </td><td style=\"text-align: right;\">   0.0632007  </td><td style=\"text-align: right;\">    87.5721     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.70249</td><td style=\"text-align: right;\">    90.0207     </td></tr>\n",
       "<tr><td>train_boston_e31aa71a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.43888  </td><td style=\"text-align: right;\">      0.169569 </td><td style=\"text-align: right;\">    189.343 </td><td style=\"text-align: right;\">0.0156347 </td><td style=\"text-align: right;\">0.864789 </td><td style=\"text-align: right;\">  1.92329</td><td style=\"text-align: right;\">     0.344962 </td><td style=\"text-align: right;\">   0.0442503  </td><td style=\"text-align: right;\">   110.829      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.74922</td><td style=\"text-align: right;\">   121.601      </td></tr>\n",
       "<tr><td>train_boston_e333981a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.520433 </td><td style=\"text-align: right;\">      0.269347 </td><td style=\"text-align: right;\">    122.589 </td><td style=\"text-align: right;\">0.0107742 </td><td style=\"text-align: right;\">0.680391 </td><td style=\"text-align: right;\">  2.15362</td><td style=\"text-align: right;\">     0.381279 </td><td style=\"text-align: right;\">   0.0607642  </td><td style=\"text-align: right;\">    17.996      </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.45847</td><td style=\"text-align: right;\">    17.6699     </td></tr>\n",
       "<tr><td>train_boston_e344bd20</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.565389 </td><td style=\"text-align: right;\">      0.266531 </td><td style=\"text-align: right;\">    114.282 </td><td style=\"text-align: right;\">0.0463944 </td><td style=\"text-align: right;\">0.6675   </td><td style=\"text-align: right;\">  1.69079</td><td style=\"text-align: right;\">     0.452873 </td><td style=\"text-align: right;\">   0.0380224  </td><td style=\"text-align: right;\">     2.99149    </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.41928</td><td style=\"text-align: right;\">     3.63252    </td></tr>\n",
       "<tr><td>train_boston_e357fbba</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.512893 </td><td style=\"text-align: right;\">      0.29894  </td><td style=\"text-align: right;\">    134.835 </td><td style=\"text-align: right;\">0.0561337 </td><td style=\"text-align: right;\">0.44575  </td><td style=\"text-align: right;\">  2.01904</td><td style=\"text-align: right;\">     0.700637 </td><td style=\"text-align: right;\">   0.0419177  </td><td style=\"text-align: right;\">612281          </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.13054</td><td style=\"text-align: right;\">664845          </td></tr>\n",
       "<tr><td>train_boston_eb5fd9c2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.219405 </td><td style=\"text-align: right;\">      0.0754853</td><td style=\"text-align: right;\">    248.861 </td><td style=\"text-align: right;\">0.0718356 </td><td style=\"text-align: right;\">0.832539 </td><td style=\"text-align: right;\">  2.18882</td><td style=\"text-align: right;\">     0.631572 </td><td style=\"text-align: right;\">   0.0420215  </td><td style=\"text-align: right;\">    51.7854     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.72673</td><td style=\"text-align: right;\">    54.1755     </td></tr>\n",
       "<tr><td>train_boston_eb75d81c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.490105 </td><td style=\"text-align: right;\">      0.29266  </td><td style=\"text-align: right;\">    162.987 </td><td style=\"text-align: right;\">0.0567315 </td><td style=\"text-align: right;\">0.770855 </td><td style=\"text-align: right;\">  1.94192</td><td style=\"text-align: right;\">     0.584038 </td><td style=\"text-align: right;\">   0.0552273  </td><td style=\"text-align: right;\">    44.1423     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.83571</td><td style=\"text-align: right;\">    45.5313     </td></tr>\n",
       "<tr><td>train_boston_eb8a5760</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.345771 </td><td style=\"text-align: right;\">      0.207913 </td><td style=\"text-align: right;\">    170.16  </td><td style=\"text-align: right;\">0.0592772 </td><td style=\"text-align: right;\">0.742629 </td><td style=\"text-align: right;\">  1.63732</td><td style=\"text-align: right;\">     0.526789 </td><td style=\"text-align: right;\">   0.0451881  </td><td style=\"text-align: right;\">    46.5243     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.9102 </td><td style=\"text-align: right;\">    51.2263     </td></tr>\n",
       "<tr><td>train_boston_ebab14aa</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.179712 </td><td style=\"text-align: right;\">      0.195905 </td><td style=\"text-align: right;\">    157.811 </td><td style=\"text-align: right;\">0.0403169 </td><td style=\"text-align: right;\">0.687103 </td><td style=\"text-align: right;\">  1.81548</td><td style=\"text-align: right;\">     0.657711 </td><td style=\"text-align: right;\">   0.0423084  </td><td style=\"text-align: right;\">    16.4473     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.47929</td><td style=\"text-align: right;\">    19.0098     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "        \n",
    "    \n",
    "    \n",
    "#experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "experiment_metrics = dict(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 128\n",
    "    \n",
    "\n",
    "\n",
    "#[nn.ReLU, nn.Softmax(), nn.Tanh(),nn.Sigmoid() ]\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 1 if True else 2,\n",
    "    \"config\": {\n",
    "    \"steps\": 3,  # evaluation times\n",
    "     \"lr\":  tune.quniform(1e-10, 0.1,1e-10),\n",
    "    \"b1\": tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"b2\":tune.quniform(0.9, 1-1e-10,1e-10),\n",
    "        \"eps\": tune.uniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.quniform(1e-10, 0.1,1e-10),\n",
    "        \"sigmoid_func\":nn.ReLU()\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-10, 0.1),\n",
    "    \"b1\": tune.loguniform(0.9, 1-1e-10),\n",
    "        \"b2\":tune.loguniform(0.9, 1-1e-10),\n",
    "        \"eps\": tune.loguniform(1e-10, 0.1),\n",
    "         \"weight_decay\":tune.loguniform(1e-10, 0.1)\n",
    "    }\n",
    "}\n",
    "   \n",
    "#i is in [0;1]\n",
    "#We want all values between 0 and 1\n",
    "def get_sigmoid_func(i):\n",
    "    if(i<0.33):\n",
    "        return nn.ReLU()\n",
    "    elif(i<0.67):\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "    \n",
    "optimizer_is_adam = True   \n",
    "    \n",
    "f = get_sigmoid_func(3)\n",
    "print(f(torch.randn(2)))\n",
    "import random\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.loguniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.loguniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.loguniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1)\n",
    "    }\n",
    "}\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  # evaluation times\n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),#,1e-4), #*10 et 0\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.),#,1), #log de 32 à 256\n",
    "        \"n_layer\":tune.uniform(1,3),#,1), #from 1 to 3\n",
    "        \"droupout_prob\":tune.uniform(0,0.5),#,0.1), #0.x pour x allant de 0 à 5     \n",
    "        \"adam\":tune.uniform(0,1),\n",
    "        \"model\":tune.uniform(0,1),\n",
    "    }\n",
    "}\n",
    "config= {\n",
    "    \"steps\": ITERATIONS,  \n",
    "     \"lr\": 0.001# tune.uniform(1e-4, 0.1 ),\n",
    "     ,    \"embedding\": 400#tune.uniform(64, 1024),\n",
    "\n",
    "      ,   \"weight_decay\":tune.uniform(1e-4, 0.1),\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":256#tune.uniform(32.,256.),\n",
    "      ,  \"n_layer\":1 #tune.uniform(1,3),\n",
    "      ,  \"droupout_prob\":0.5#tune.uniform(0,0.5),     \n",
    "      ,  \"adam\":tune.uniform(0,1),\n",
    "        \"model\":0#tune.uniform(0,0.6),\n",
    "    }\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_TUNED if args.smoke_test else NUM_TUNED,\n",
    "    \"config\": {\n",
    "    \"steps\": ITERATIONS,  \n",
    "     \"lr\":  tune.uniform(1e-4, 0.1 ),\n",
    "    # ,    \"embedding\": 400#tune.uniform(64, 1024),\n",
    "\n",
    "         \"weight_decay\":tune.uniform(1e-4, 0.1),\n",
    "        \"sigmoid_func\":tune.uniform(0,1),\n",
    "        \"hidden_dim\":tune.uniform(32.,256.)\n",
    "      ,  \"n_layer\":tune.uniform(1,3)\n",
    "      ,  \"droupout_prob\":tune.uniform(0,0.5)     \n",
    "      ,  \"adam\":tune.uniform(0,1)\n",
    "      ,  \"model\":tune.uniform(0,1)\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#x_all = [train_IMDB,  train_TREC, train_boston, train_diabetes, train_mnist, train_fashion_mnist]\n",
    "\n",
    "#CMA(train_boston)\n",
    "#TwoPointsDE(train_boston)\n",
    "PSO(train_boston)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One shot\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "for i in range(0,1):\n",
    "    for j in range(0,1):\n",
    "        x = x_all[i]\n",
    "        f_HyperOpt(x)\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small budget\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_TUNED= 20\n",
    "\n",
    "\n",
    "model_all = [Net,ConvNet]\n",
    "optimizer_is_adam = True\n",
    "if(0==1):\n",
    "    for i in range(1,2):\n",
    "        x = train_TREC\n",
    "        f_BayesOpt(x)\n",
    "        f_AX(x)\n",
    "        f_NeverGrad(x)\n",
    "        f_BOHB(x)\n",
    "        f_Random(x)\n",
    "        f_ZOOpt(x)\n",
    "        print(\"all worked with \" + str(x)+  \" !\")\n",
    "    for i in range(1,1):\n",
    "        GAN_MNIST(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PSO in module nevergrad.optimization.optimizerlib:\n",
      "\n",
      "class PSO(nevergrad.optimization.base.Optimizer)\n",
      " |  PSO(parametrization: Union[int, nevergrad.parametrization.core.Parameter], budget: Union[int, NoneType] = None, num_workers: int = 1, transform: str = 'arctan', wide: bool = False, popsize: Union[int, NoneType] = None) -> None\n",
      " |  \n",
      " |  Algorithm framework with 3 main functions:\n",
      " |  \n",
      " |  - :code:`ask()` which provides a candidate on which to evaluate the function to optimize.\n",
      " |  - :code:`tell(candidate, value)` which lets you provide the values associated to points.\n",
      " |  - :code:`provide_recommendation()` which provides the best final candidate.\n",
      " |  \n",
      " |  Typically, one would call :code:`ask()` num_workers times, evaluate the\n",
      " |  function on these num_workers points in parallel, update with the fitness value when the\n",
      " |  evaluations is finished, and iterate until the budget is over. At the very end,\n",
      " |  one would call provide_recommendation for the estimated optimum.\n",
      " |  \n",
      " |  This class is abstract, it provides internal equivalents for the 3 main functions,\n",
      " |  among which at least :code:`_internal_ask_candidate` has to be overridden.\n",
      " |  \n",
      " |  Each optimizer instance should be used only once, with the initial provided budget\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  parametrization: int or Parameter\n",
      " |      either the dimension of the optimization space, or its parametrization\n",
      " |  budget: int/None\n",
      " |      number of allowed evaluations\n",
      " |  num_workers: int\n",
      " |      number of evaluations which will be run in parallel at once\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PSO\n",
      " |      nevergrad.optimization.base.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, parametrization: Union[int, nevergrad.parametrization.core.Parameter], budget: Union[int, NoneType] = None, num_workers: int = 1, transform: str = 'arctan', wide: bool = False, popsize: Union[int, NoneType] = None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nevergrad.optimization.base.Optimizer:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ask(self) -> nevergrad.parametrization.core.Parameter\n",
      " |      Provides a point to explore.\n",
      " |      This function can be called multiple times to explore several points in parallel\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p.Parameter:\n",
      " |          The candidate to try on the objective function. :code:`p.Parameter` have field :code:`args` and :code:`kwargs`\n",
      " |          which can be directly used on the function (:code:`objective_function(*candidate.args, **candidate.kwargs)`).\n",
      " |  \n",
      " |  dump(self, filepath: Union[str, pathlib.Path]) -> None\n",
      " |      Pickles the optimizer into a file.\n",
      " |  \n",
      " |  minimize(self, objective_function: Callable[..., float], executor: Union[nevergrad.common.ExecutorLike, NoneType] = None, batch_mode: bool = False, verbosity: int = 0) -> nevergrad.parametrization.core.Parameter\n",
      " |      Optimization (minimization) procedure\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      objective_function: callable\n",
      " |          A callable to optimize (minimize)\n",
      " |      executor: Executor\n",
      " |          An executor object, with method :code:`submit(callable, *args, **kwargs)` and returning a Future-like object\n",
      " |          with methods :code:`done() -> bool` and :code:`result() -> float`. The executor role is to dispatch the execution of\n",
      " |          the jobs locally/on a cluster/with multithreading depending on the implementation.\n",
      " |          Eg: :code:`concurrent.futures.ThreadPoolExecutor`\n",
      " |      batch_mode: bool\n",
      " |          when :code:`num_workers = n > 1`, whether jobs are executed by batch (:code:`n` function evaluations are launched,\n",
      " |          we wait for all results and relaunch n evals) or not (whenever an evaluation is finished, we launch\n",
      " |          another one)\n",
      " |      verbosity: int\n",
      " |          print information about the optimization (0: None, 1: fitness values, 2: fitness values and recommendation)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p.Parameter\n",
      " |          The candidate with minimal value. :code:`p.Parameters` have field :code:`args` and :code:`kwargs` which can be directly used\n",
      " |          on the function (:code:`objective_function(*candidate.args, **candidate.kwargs)`).\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      for evaluation purpose and with the current implementation, it is better to use batch_mode=True\n",
      " |  \n",
      " |  provide_recommendation(self) -> nevergrad.parametrization.core.Parameter\n",
      " |      Provides the best point to use as a minimum, given the budget that was used\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p.Parameter\n",
      " |          The candidate with minimal value. p.Parameters have field :code:`args` and :code:`kwargs` which can be directly used\n",
      " |          on the function (:code:`objective_function(*candidate.args, **candidate.kwargs)`).\n",
      " |  \n",
      " |  recommend(self) -> nevergrad.parametrization.core.Parameter\n",
      " |      Provides the best candidate to use as a minimum, given the budget that was used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p.Parameter\n",
      " |          The candidate with minimal value. :code:`p.Parameters` have field :code:`args` and :code:`kwargs` which can be directly used\n",
      " |          on the function (:code:`objective_function(*candidate.args, **candidate.kwargs)`).\n",
      " |  \n",
      " |  register_callback(self, name: str, callback: Union[Callable[[ForwardRef('Optimizer'), ForwardRef('p.Parameter'), float], NoneType], Callable[[ForwardRef('Optimizer')], NoneType]]) -> None\n",
      " |      Add a callback method called either when `tell` or `ask` are called, with the same\n",
      " |      arguments (including the optimizer / self). This can be useful for custom logging.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name: str\n",
      " |          name of the method to register the callback for (either :code:`ask` or :code:`tell`)\n",
      " |      callback: callable\n",
      " |          a callable taking the same parameters as the method it is registered upon (including self)\n",
      " |  \n",
      " |  remove_all_callbacks(self) -> None\n",
      " |      Removes all registered callables\n",
      " |  \n",
      " |  suggest(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Suggests a new point to ask.\n",
      " |      It will be asked at the next call (last in first out).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *args: Any\n",
      " |          positional arguments matching the parametrization pattern.\n",
      " |      *kwargs: Any\n",
      " |          keyword arguments matching the parametrization pattern.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      - This relies on optmizers implementing a way to deal with unasked candidate.\n",
      " |        Some optimizers may not support it and will raise a :code:`TellNotAskedNotSupportedError`\n",
      " |        at :code:`tell` time.\n",
      " |      - LIFO is used so as to be able to suggest and ask straightaway, as an alternative to\n",
      " |        creating a new candidate with :code:`optimizer.parametrization.spawn_child(new_value)`\n",
      " |  \n",
      " |  tell(self, candidate: nevergrad.parametrization.core.Parameter, value: float) -> None\n",
      " |      Provides the optimizer with the evaluation of a fitness value for a candidate.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x: np.ndarray\n",
      " |          point where the function was evaluated\n",
      " |      value: float\n",
      " |          value of the function\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      The candidate should generally be one provided by :code:`ask()`, but can be also\n",
      " |      a non-asked candidate. To create a p.Parameter instance from args and kwargs,\n",
      " |      you can use :code:`candidate = optimizer.parametrization.spawn_child(new_value=your_value)`:\n",
      " |      \n",
      " |      - for an :code:`Array(shape(2,))`: :code:`optimizer.parametrization.spawn_child(new_value=[12, 12])`\n",
      " |      \n",
      " |      - for an :code:`Instrumentation`: :code:`optimizer.parametrization.spawn_child(new_value=(args, kwargs))`\n",
      " |      \n",
      " |      Alternatively, you can provide a suggestion with :code:`optimizer.suggest(*args, **kwargs)`, the next :code:`ask`\n",
      " |      will use this suggestion.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from nevergrad.optimization.base.Optimizer:\n",
      " |  \n",
      " |  load(filepath: Union[str, pathlib.Path]) -> ~X from builtins.type\n",
      " |      Loads a pickle and checks that the class is correct.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nevergrad.optimization.base.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dimension\n",
      " |      int: Dimension of the optimization space.\n",
      " |  \n",
      " |  num_ask\n",
      " |      int: Number of time the `ask` method was called.\n",
      " |  \n",
      " |  num_tell\n",
      " |      int: Number of time the `tell` method was called.\n",
      " |  \n",
      " |  num_tell_not_asked\n",
      " |      int: Number of time the :code:`tell` method was called on candidates that were not asked for by the optimizer\n",
      " |      (or were suggested).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from nevergrad.optimization.base.Optimizer:\n",
      " |  \n",
      " |  hashed = False\n",
      " |  \n",
      " |  no_parallelization = False\n",
      " |  \n",
      " |  one_shot = False\n",
      " |  \n",
      " |  recast = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "help(ng.ptimizers.PSO\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMA(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.CMA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"CMA\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoPointsDE(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.TwoPointsDE\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"TwoPointsDE\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.TBPSA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"PSO\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.logger import *\n",
    "\n",
    "def f_HyperOpt(dataset):\n",
    "    class TestLogger(tune.logger.Logger):\n",
    "        def _init(self):\n",
    "            progress_file = os.path.join(\"/home/antoine/Projet/NovelTuning/\", \"hyper.csv\")\n",
    "            self._continuing = os.path.exists(progress_file)\n",
    "            self._file = open(progress_file, \"a\")\n",
    "            self._csv_out = None\n",
    "        def on_result(self, result):\n",
    "            tmp = result.copy()\n",
    "            #if \"done\" in tmp:\n",
    "             #   if(tmp[\"done\"] != True):\n",
    "                    \n",
    "            if \"config\" in tmp:\n",
    "                del tmp[\"config\"]\n",
    "            result = flatten_dict(tmp, delimiter=\"/\")\n",
    "            if self._csv_out is None:\n",
    "                self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "                if not self._continuing:\n",
    "                    self._csv_out.writeheader()\n",
    "            self._csv_out.writerow(\n",
    "                {k: v\n",
    "                 for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "            self._file.flush()\n",
    "        \n",
    "        \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = HyperOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler,  name=\"hyper\", search_alg=bayesopt,\n",
    "            loggers=[TestLogger] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.logger import *\n",
    "dir(ray.tune.logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BayesOpt(dataset):\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "    bayesopt = BayesOptSearch(**experiment_metrics)\n",
    "    tune.run(dataset, **tune_kwargs , scheduler = scheduler, name=\"bayes\",  search_alg=bayesopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_AX(dataset):\n",
    "    \n",
    "   \n",
    "    if __name__ == \"__main__\":\n",
    "                \n",
    "        algo = AxSearch(\n",
    "            max_concurrent=2, #was working with 2\n",
    "            **experiment_metrics\n",
    "        )\n",
    "        scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "        tune.run(\n",
    "            dataset,       name=\"ax\",\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            **tune_kwargs)\n",
    "\n",
    "        \n",
    "#        algo = AxSearch(\n",
    "#            **experiment_metrics\n",
    "#        )\n",
    "#        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        \n",
    "#        scheduler = AsyncHyperBandScheduler()\n",
    "#        tune.run(\n",
    "#            dataset,\n",
    "#            **experiment_metrics,\n",
    "#            search_alg=algo,\n",
    "#            scheduler=scheduler,\n",
    "#            **tune_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO We are interested in multiple Population based algorithms from nevergrad, and certainly not in OnePlusOne. \n",
    "def f_NeverGrad(dataset):\n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.CMA\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo, name=\"ng\",\n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BOHB(dataset):\n",
    "\n",
    "    bohb_hyperband = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=100,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    bohb_search = TuneBOHB(\n",
    "        # space=config_space, \n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "       # config=config, \n",
    "        scheduler=bohb_hyperband,name=\"bohb\",\n",
    "        search_alg=bohb_search,       \n",
    "         **tune_kwargs)\n",
    "        #num_samples=NUM_TUNED,\n",
    "       # stop={\"training_iteration\": 100})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Random(dataset):\n",
    "    \n",
    "    algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,\n",
    "    # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "    tune.run(\n",
    "        dataset,\n",
    "        **experiment_metrics,\n",
    "      #  name=\"nevergrad\",\n",
    "        search_alg=algo,   name=\"random\",    \n",
    "        scheduler=scheduler,\n",
    "        **tune_kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ZOOpt(dataset):\n",
    "\n",
    "    dim_dict = {\n",
    "        \"lr\": (ValueType.CONTINUOUS, [0, 1], 1e-2),\n",
    "        \"momentum\": (ValueType.CONTINUOUS, [0,1, 0.9], 1e-2)\n",
    "    }\n",
    "\n",
    "    zoopt_search_config = {\n",
    "        \"parallel_num\": 8,  # how many workers to parallel\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    zoopt_search = ZOOptSearch(\n",
    "    algo=\"Asracos\",  # only support Asracos currently\n",
    "    #dim_dict=dim_dict,\n",
    "    budget=ITERATIONS,\n",
    "    #dim_dict=dim_dict,\n",
    "   #     **zoopt_search_config,\n",
    "    **experiment_metrics)\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(**experiment_metrics)\n",
    "\n",
    "   \n",
    "    tune.run(dataset,\n",
    " #        config = config,\n",
    "    search_alg=zoopt_search,\n",
    "   # num_samples= ITERATIONS,\n",
    "    scheduler=scheduler,       \n",
    "    #         paralell_num=4,\n",
    "    name=\"zoopt_search\", \n",
    "              **tune_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import nevergrad as ng\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from zoopt import ValueType\n",
    "import torch\n",
    "\n",
    "def GAN_MNIST(SA):\n",
    "    import ray\n",
    "\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.utils.data\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "    import torchvision.utils as vutils\n",
    "    import numpy as np\n",
    "\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.trial import ExportFormat\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    import argparse\n",
    "    import os\n",
    "    from filelock import FileLock\n",
    "    import random\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.parallel\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data\n",
    "    import numpy as np\n",
    "    from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "    from ray.tune.suggest.ax import AxSearch\n",
    "\n",
    "\n",
    "\n",
    "    from torch.autograd import Variable\n",
    "    from torch.nn import functional as F\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "    # Training parameters\n",
    "    dataroot = ray.utils.get_user_temp_dir() + os.sep\n",
    "    workers = 2\n",
    "    batch_size = 64\n",
    "    image_size = 32\n",
    "\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 32\n",
    "\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 32\n",
    "\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # iterations of actual training in each Trainable _train\n",
    "    train_iterations_per_step = 5\n",
    "\n",
    "    MODEL_PATH = os.path.expanduser(\"~/.ray/models/mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "    def get_data_loader():\n",
    "        dataset = dset.MNIST(\n",
    "            root=dataroot,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, ), (0.5, )),\n",
    "            ]))\n",
    "\n",
    "        # Create the dataloader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "    # __GANmodel_begin__\n",
    "    # custom weights initialization called on netG and netD\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find(\"Conv\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find(\"BatchNorm\") != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "    # Generator Code\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "\n",
    "\n",
    "    # __GANmodel_end__\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_begin__\n",
    "    class Net(nn.Module):\n",
    "        \"\"\"\n",
    "        LeNet for MNist classification, used for inception_score\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def inception_score(imgs, mnist_model_ref, batch_size=32, splits=1):\n",
    "        N = len(imgs)\n",
    "        dtype = torch.FloatTensor\n",
    "        dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "        cm = ray.get(mnist_model_ref)  # Get the mnist model from Ray object store.\n",
    "        up = nn.Upsample(size=(28, 28), mode=\"bilinear\").type(dtype)\n",
    "\n",
    "        def get_pred(x):\n",
    "            x = up(x)\n",
    "            x = cm(x)\n",
    "            return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "        preds = np.zeros((N, 10))\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            batch = batch.type(dtype)\n",
    "            batchv = Variable(batch)\n",
    "            batch_size_i = batch.size()[0]\n",
    "            preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "        # Now compute the mean kl-div\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (N // splits):(k + 1) * (N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "        return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "    # __INCEPTION_SCORE_end__\n",
    "\n",
    "\n",
    "    def train(netD, netG, optimG, optimD, criterion, dataloader, iteration, device,\n",
    "              mnist_model_ref):\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            if i >= train_iterations_per_step:\n",
    "                break\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full(\n",
    "                (b_size, ), real_label, dtype=torch.float, device=device)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimG.step()\n",
    "\n",
    "            is_score, is_std = inception_score(fake, mnist_model_ref)\n",
    "\n",
    "            # Output training stats\n",
    "            if iteration % 10 == 0:\n",
    "                print(\"[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z))\"\n",
    "                      \": %.4f / %.4f \\tInception score: %.4f\" %\n",
    "                      (iteration, len(dataloader), errD.item(), errG.item(), D_x,\n",
    "                       D_G_z1, D_G_z2, is_score))\n",
    "\n",
    "        return errG.item(), errD.item(), is_score\n",
    "\n",
    "\n",
    "    def plot_images(dataloader):\n",
    "        # Plot some training images\n",
    "        real_batch = next(iter(dataloader))\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Original Images\")\n",
    "        plt.imshow(\n",
    "            np.transpose(\n",
    "                vutils.make_grid(real_batch[0][:64], padding=2,\n",
    "                                 normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def demo_gan(checkpoint_paths):\n",
    "        img_list = []\n",
    "        fixed_noise = torch.randn(64, nz, 1, 1)\n",
    "        for netG_path in checkpoint_paths:\n",
    "            loadedG = Generator()\n",
    "            loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])\n",
    "            with torch.no_grad():\n",
    "                fake = loadedG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.axis(\"off\")\n",
    "        ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)]\n",
    "               for i in img_list]\n",
    "        ani = animation.ArtistAnimation(\n",
    "            fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "        ani.save(\"./generated.gif\", writer=\"imagemagick\", dpi=72)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # __Trainable_begin__\n",
    "    class PytorchTrainable(tune.Trainable):\n",
    "        def setup(self, config):\n",
    "            use_cuda = config.get(\"use_gpu\") and torch.cuda.is_available()\n",
    "            self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            self.netD = Discriminator().to(self.device)\n",
    "            self.netD.apply(weights_init)\n",
    "            self.netG = Generator().to(self.device)\n",
    "            self.netG.apply(weights_init)\n",
    "            self.criterion = nn.BCELoss()\n",
    "            self.optimizerD = optim.Adam(\n",
    "                self.netD.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            self.optimizerG = optim.Adam(\n",
    "                self.netG.parameters(),\n",
    "                lr=config.get(\"lr\", 0.01),\n",
    "                betas=(beta1, 0.999))\n",
    "            with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "                self.dataloader = get_data_loader()\n",
    "            self.mnist_model_ref = c[\"mnist_model_ref\"]\n",
    "\n",
    "        def step(self):\n",
    "            lossG, lossD, is_score = train(self.netD, self.netG, self.optimizerG,\n",
    "                                           self.optimizerD, self.criterion,\n",
    "                                           self.dataloader, self._iteration,\n",
    "                                           self.device, self.mnist_model_ref)\n",
    "            return {\"lossg\": lossG, \"lossd\": lossD, \"is_score\": is_score}\n",
    "\n",
    "        def save_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\n",
    "                \"netDmodel\": self.netD.state_dict(),\n",
    "                \"netGmodel\": self.netG.state_dict(),\n",
    "                \"optimD\": self.optimizerD.state_dict(),\n",
    "                \"optimG\": self.optimizerG.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "            return checkpoint_dir\n",
    "\n",
    "        def load_checkpoint(self, checkpoint_dir):\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            checkpoint = torch.load(path)\n",
    "            self.netD.load_state_dict(checkpoint[\"netDmodel\"])\n",
    "            self.netG.load_state_dict(checkpoint[\"netGmodel\"])\n",
    "            self.optimizerD.load_state_dict(checkpoint[\"optimD\"])\n",
    "            self.optimizerG.load_state_dict(checkpoint[\"optimG\"])\n",
    "\n",
    "        def reset_config(self, new_config):\n",
    "            if \"netD_lr\" in new_config:\n",
    "                for param_group in self.optimizerD.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netD_lr\"]\n",
    "            if \"netG_lr\" in new_config:\n",
    "                for param_group in self.optimizerG.param_groups:\n",
    "                    param_group[\"lr\"] = new_config[\"netG_lr\"]\n",
    "\n",
    "            self.config = new_config\n",
    "            return True\n",
    "\n",
    "        def _export_model(self, export_formats, export_dir):\n",
    "            if export_formats == [ExportFormat.MODEL]:\n",
    "                path = os.path.join(export_dir, \"exported_models\")\n",
    "                torch.save({\n",
    "                    \"netDmodel\": self.netD.state_dict(),\n",
    "                    \"netGmodel\": self.netG.state_dict()\n",
    "                }, path)\n",
    "                return {ExportFormat.MODEL: path}\n",
    "            else:\n",
    "                raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "\n",
    "\n",
    "    import urllib.request\n",
    "    # Download a pre-trained MNIST model for inception score calculation.\n",
    "    # This is a tiny model (<100kb).\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"downloading model\")\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://github.com/ray-project/ray/raw/master/python/ray/tune/\"\n",
    "            \"examples/pbt_dcgan_mnist/mnist_cnn.pt\", MODEL_PATH)\n",
    "\n",
    "    dataloader = get_data_loader()\n",
    "    if not args.smoke_test:\n",
    "        plot_images(dataloader)\n",
    "\n",
    "    # load the pretrained mnist classification model for inception_score\n",
    "    mnist_cnn = Net()\n",
    "    mnist_cnn.load_state_dict(torch.load(MODEL_PATH))\n",
    "    mnist_cnn.eval()\n",
    "    mnist_model_ref = ray.put(mnist_cnn)\n",
    "\n",
    "    # __tune_begin__\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"is_score\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"netG_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "            \"netD_lr\": lambda: np.random.uniform(1e-2, 1e-5),\n",
    "        })\n",
    "\n",
    "\n",
    "    experiment_metrics= dict(metric=\"is_score\",\n",
    "        mode=\"max\")\n",
    "\n",
    "   \n",
    "    dim_dict = {\n",
    "        \"netG_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2),\n",
    "        \"netD_lr\": (ValueType.CONTINUOUS, [0, 0.1], 1e-2)\n",
    "    }\n",
    "\n",
    "    config =     {\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "           \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        }\n",
    "\n",
    "    algo = NevergradSearch(optimizer=ng.optimizers.OnePlusOne)\n",
    "\n",
    "    \n",
    "    tune_iter = 5 if args.smoke_test else 1\n",
    "    c={\"mnist_model_ref\" : mnist_model_ref}\n",
    "    \n",
    "    \n",
    "    if(SA==1):\n",
    "        algo =   BayesOptSearch(**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "    if(SA==2):\n",
    "        algo =  AxSearch(\n",
    "            **experiment_metrics) \n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-3, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-3, 0.1)\n",
    "\n",
    "\n",
    "        })\n",
    "        \n",
    "        \n",
    "    if(SA==3):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.OnePlusOne,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "     \n",
    "    if(SA==4):\n",
    "        bohb_hyperband = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=100,\n",
    "            reduction_factor=4,\n",
    "            **experiment_metrics)\n",
    "\n",
    "        bohb_search = TuneBOHB(\n",
    "            # space=config_space, \n",
    "            max_concurrent=4,\n",
    "            **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=bohb_hyperband,\n",
    "        reuse_actors=True,\n",
    "        search_alg=bohb_search,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "    if(SA==5):\n",
    "        algo =   NevergradSearch(\n",
    "    optimizer=ng.optimizers.RandomSearch,**experiment_metrics) \n",
    "        analysis = tune.run(\n",
    "        PytorchTrainable,\n",
    "        name=\"pbt_dcgan_mnist\",\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        search_alg=algo,\n",
    "        verbose=1,\n",
    "        checkpoint_at_end=True,\n",
    "        stop={\n",
    "            \"training_iteration\": tune_iter,\n",
    "        },\n",
    "        num_samples=8,\n",
    "        export_formats=[ExportFormat.MODEL],\n",
    "        config={\n",
    "            \"netG_lr\": tune.loguniform(1e-10, 0.1),\n",
    "            \"netD_lr\": tune.loguniform(1e-10, 0.1)\n",
    "        })\n",
    "        \n",
    "\n",
    "        \n",
    "    if(SA==6):\n",
    "        algo=  ZOOptSearch(\n",
    "                algo=\"Asracos\",  # only support Asracos currently\n",
    "                dim_dict=dim_dict,\n",
    "                budget=10,\n",
    "                #dim_dict=dim_dict,\n",
    "                **experiment_metrics)\n",
    "        analysis = tune.run(\n",
    "            PytorchTrainable,\n",
    "            name=\"pbt_dcgan_mnist\",\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            search_alg=algo,\n",
    "            verbose=1,\n",
    "            checkpoint_at_end=True,\n",
    "            stop={\n",
    "                \"training_iteration\": tune_iter,\n",
    "            },\n",
    "            num_samples=8,\n",
    "            export_formats=[ExportFormat.MODEL],\n",
    "            config=dim_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#Classification:\n",
    "\n",
    "metrics.accuracy_score()\n",
    "metrics.f1_score\n",
    "metrics.log_loss\n",
    "metrics.precision_score\n",
    "metrics.recall_score\n",
    "\n",
    "#Regression\n",
    "mean_absolute_error\n",
    "mean_squared_error\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
